{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 中国人寿及其同业数据，补充八分类和倾向性结果\n",
    "> - 七月及以前数据：db_docinfo_backup、db_docinfo_text_backup\n",
    "> - 七月之后数据：db_docinfo_trade、db_docinfo_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:51:27.407829Z",
     "start_time": "2018-11-06T08:51:27.381828Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import requests,json\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:51:27.418830Z",
     "start_time": "2018-11-06T08:51:27.410830Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from toolkits.setup.date_time import get_day_list\n",
    "from toolkits.setup import specific_func\n",
    "specific_func.set_ch_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T08:32:07.899502Z",
     "start_time": "2018-11-07T08:32:07.790495Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'signal' has no attribute 'SIGALRM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-13ad6ca49241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mseconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\site-packages\\timeout_decorator\\timeout_decorator.py\u001b[0m in \u001b[0;36mnew_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mnew_seconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnew_seconds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                     \u001b[0mold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIGALRM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                     \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetitimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITIMER_REAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'signal' has no attribute 'SIGALRM'"
     ]
    }
   ],
   "source": [
    "from timeout_decorator import timeout\n",
    "import time\n",
    "seconds = 3\n",
    "timeout(seconds)(lambda x:time.sleep(x))(1)\n",
    "timeout(seconds)(lambda x:time.sleep(x))(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:51:37.582411Z",
     "start_time": "2018-11-06T08:51:27.423830Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = specific_func.get_engine('cbirc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:51:37.602413Z",
     "start_time": "2018-11-06T08:51:37.589412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '监管',\n",
       " 2: '行业',\n",
       " 3: '产品销售',\n",
       " 4: '资本市场',\n",
       " 5: '公司内部管理',\n",
       " 6: '消费服务',\n",
       " 7: '其他相关报道',\n",
       " 8: '噪音'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic={'监管':1,'行业':2,'产品销售':3,'资本市场':4,'公司内部管理':5,'消费服务':6,'其他相关报道':7,'噪音':8}\n",
    "class_name_dict = {v: k for k, v in label_dic.items()}\n",
    "class_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七月及以前数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:51:56.833513Z",
     "start_time": "2018-11-06T08:51:56.823512Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_port = '47.93.183.157:10000'\n",
    "\n",
    "headers={'content-type':'application/json'}\n",
    "url_cor = \"http://%s/judge_correlation_i\"%ip_port\n",
    "url_tend = \"http://%s/tendency_analysis_i\"%ip_port\n",
    "url_warn = \"http://%s/early_warning_i\"%ip_port\n",
    "\n",
    "file_list_1 = ['raw/人寿 7月.xlsx', 'raw/同业  七月.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T09:42:44.725842Z",
     "start_time": "2018-11-06T08:51:58.198591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/人寿 7月.xlsx   ----------------\n",
      "总量： (8053, 12)\n",
      "缺失值数量： (8028, 12)\n",
      "无缺失值数量： (25, 12)\n",
      "id_list:  7426\n",
      "id_list_sel:  0 100\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  9.00\n",
      "    0  title_id:  (100, 2)\n",
      "    0  content_id:  (100, 2)\n",
      "    0  title_content:  (100, 3)\n",
      "    0  title_content_com:  (100, 3)\n",
      "id_list_sel:  100 200\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  8.00\n",
      "    1  title_id:  (100, 2)\n",
      "    1  content_id:  (100, 2)\n",
      "    1  title_content:  (100, 3)\n",
      "    1  title_content_com:  (200, 3)\n",
      "id_list_sel:  200 300\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  8.00\n",
      "    2  title_id:  (100, 2)\n",
      "    2  content_id:  (100, 2)\n",
      "    2  title_content:  (100, 3)\n",
      "    2  title_content_com:  (300, 3)\n",
      "id_list_sel:  300 400\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  9.00\n",
      "    3  title_id:  (100, 2)\n",
      "    3  content_id:  (100, 2)\n",
      "    3  title_content:  (100, 3)\n",
      "    3  title_content_com:  (400, 3)\n",
      "id_list_sel:  400 500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    4  title_id:  (100, 2)\n",
      "    4  content_id:  (100, 2)\n",
      "    4  title_content:  (100, 3)\n",
      "    4  title_content_com:  (500, 3)\n",
      "id_list_sel:  500 600\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    5  title_id:  (100, 2)\n",
      "    5  content_id:  (100, 2)\n",
      "    5  title_content:  (100, 3)\n",
      "    5  title_content_com:  (600, 3)\n",
      "id_list_sel:  600 700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  15.00\n",
      "    6  title_id:  (100, 2)\n",
      "    6  content_id:  (100, 2)\n",
      "    6  title_content:  (100, 3)\n",
      "    6  title_content_com:  (700, 3)\n",
      "id_list_sel:  700 800\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  18.00\n",
      "    7  title_id:  (100, 2)\n",
      "    7  content_id:  (100, 2)\n",
      "    7  title_content:  (100, 3)\n",
      "    7  title_content_com:  (800, 3)\n",
      "id_list_sel:  800 900\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  14.00\n",
      "    8  title_id:  (100, 2)\n",
      "    8  content_id:  (100, 2)\n",
      "    8  title_content:  (100, 3)\n",
      "    8  title_content_com:  (900, 3)\n",
      "id_list_sel:  900 1000\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  12.00\n",
      "    9  title_id:  (100, 2)\n",
      "    9  content_id:  (100, 2)\n",
      "    9  title_content:  (100, 3)\n",
      "    9  title_content_com:  (1000, 3)\n",
      "id_list_sel:  1000 1100\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  11.00\n",
      "    10  title_id:  (100, 2)\n",
      "    10  content_id:  (100, 2)\n",
      "    10  title_content:  (100, 3)\n",
      "    10  title_content_com:  (1100, 3)\n",
      "id_list_sel:  1100 1200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  15.00\n",
      "    11  title_id:  (100, 2)\n",
      "    11  content_id:  (100, 2)\n",
      "    11  title_content:  (100, 3)\n",
      "    11  title_content_com:  (1200, 3)\n",
      "id_list_sel:  1200 1300\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  10.00\n",
      "    12  title_id:  (100, 2)\n",
      "    12  content_id:  (100, 2)\n",
      "    12  title_content:  (100, 3)\n",
      "    12  title_content_com:  (1300, 3)\n",
      "id_list_sel:  1300 1400\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  13.00\n",
      "    13  title_id:  (100, 2)\n",
      "    13  content_id:  (100, 2)\n",
      "    13  title_content:  (100, 3)\n",
      "    13  title_content_com:  (1400, 3)\n",
      "id_list_sel:  1400 1500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  10.00\n",
      "    14  title_id:  (100, 2)\n",
      "    14  content_id:  (100, 2)\n",
      "    14  title_content:  (100, 3)\n",
      "    14  title_content_com:  (1500, 3)\n",
      "id_list_sel:  1500 1600\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  11.00\n",
      "    15  title_id:  (100, 2)\n",
      "    15  content_id:  (100, 2)\n",
      "    15  title_content:  (100, 3)\n",
      "    15  title_content_com:  (1600, 3)\n",
      "id_list_sel:  1600 1700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  14.00\n",
      "    16  title_id:  (100, 2)\n",
      "    16  content_id:  (100, 2)\n",
      "    16  title_content:  (100, 3)\n",
      "    16  title_content_com:  (1700, 3)\n",
      "id_list_sel:  1700 1800\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  33.00\n",
      "    17  title_id:  (100, 2)\n",
      "    17  content_id:  (100, 2)\n",
      "    17  title_content:  (100, 3)\n",
      "    17  title_content_com:  (1800, 3)\n",
      "id_list_sel:  1800 1900\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  16.00\n",
      "    18  title_id:  (100, 2)\n",
      "    18  content_id:  (100, 2)\n",
      "    18  title_content:  (100, 3)\n",
      "    18  title_content_com:  (1900, 3)\n",
      "id_list_sel:  1900 2000\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  21.00\n",
      "    19  title_id:  (100, 2)\n",
      "    19  content_id:  (100, 2)\n",
      "    19  title_content:  (100, 3)\n",
      "    19  title_content_com:  (2000, 3)\n",
      "id_list_sel:  2000 2100\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  20.00\n",
      "    20  title_id:  (100, 2)\n",
      "    20  content_id:  (100, 2)\n",
      "    20  title_content:  (100, 3)\n",
      "    20  title_content_com:  (2100, 3)\n",
      "id_list_sel:  2100 2200\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  9.00\n",
      "    21  title_id:  (100, 2)\n",
      "    21  content_id:  (100, 2)\n",
      "    21  title_content:  (100, 3)\n",
      "    21  title_content_com:  (2200, 3)\n",
      "id_list_sel:  2200 2300\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  10.00\n",
      "    22  title_id:  (100, 2)\n",
      "    22  content_id:  (100, 2)\n",
      "    22  title_content:  (100, 3)\n",
      "    22  title_content_com:  (2300, 3)\n",
      "id_list_sel:  2300 2400\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  35.00\n",
      "    23  title_id:  (100, 2)\n",
      "    23  content_id:  (100, 2)\n",
      "    23  title_content:  (100, 3)\n",
      "    23  title_content_com:  (2400, 3)\n",
      "id_list_sel:  2400 2500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 54974461, 'title': '中国人寿“康宁终身”（至尊版）产品详解+案例+病种明细', 'content': nan}\n",
      "    24  title_id:  (100, 2)\n",
      "    24  content_id:  (99, 2)\n",
      "    24  title_content:  (100, 3)\n",
      "    24  title_content_com:  (2500, 3)\n",
      "id_list_sel:  2500 2600\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  14.00\n",
      "    25  title_id:  (100, 2)\n",
      "    25  content_id:  (100, 2)\n",
      "    25  title_content:  (100, 3)\n",
      "    25  title_content_com:  (2600, 3)\n",
      "id_list_sel:  2600 2700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  18.00\n",
      "    26  title_id:  (100, 2)\n",
      "    26  content_id:  (100, 2)\n",
      "    26  title_content:  (100, 3)\n",
      "    26  title_content_com:  (2700, 3)\n",
      "id_list_sel:  2700 2800\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  39.00\n",
      "    27  title_id:  (100, 2)\n",
      "    27  content_id:  (100, 2)\n",
      "    27  title_content:  (100, 3)\n",
      "    27  title_content_com:  (2800, 3)\n",
      "id_list_sel:  2800 2900\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  14.00\n",
      "    28  title_id:  (100, 2)\n",
      "    28  content_id:  (100, 2)\n",
      "    28  title_content:  (100, 3)\n",
      "    28  title_content_com:  (2900, 3)\n",
      "id_list_sel:  2900 3000\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    29  title_id:  (100, 2)\n",
      "    29  content_id:  (100, 2)\n",
      "    29  title_content:  (100, 3)\n",
      "    29  title_content_com:  (3000, 3)\n",
      "id_list_sel:  3000 3100\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  12.00\n",
      "    30  title_id:  (100, 2)\n",
      "    30  content_id:  (100, 2)\n",
      "    30  title_content:  (100, 3)\n",
      "    30  title_content_com:  (3100, 3)\n",
      "id_list_sel:  3100 3200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    31  title_id:  (100, 2)\n",
      "    31  content_id:  (100, 2)\n",
      "    31  title_content:  (100, 3)\n",
      "    31  title_content_com:  (3200, 3)\n",
      "id_list_sel:  3200 3300\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  12.00\n",
      "    32  title_id:  (100, 2)\n",
      "    32  content_id:  (100, 2)\n",
      "    32  title_content:  (100, 3)\n",
      "    32  title_content_com:  (3300, 3)\n",
      "id_list_sel:  3300 3400\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  14.00\n",
      "    33  title_id:  (100, 2)\n",
      "    33  content_id:  (100, 2)\n",
      "    33  title_content:  (100, 3)\n",
      "    33  title_content_com:  (3400, 3)\n",
      "id_list_sel:  3400 3500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  16.00\n",
      "    34  title_id:  (100, 2)\n",
      "    34  content_id:  (100, 2)\n",
      "    34  title_content:  (100, 3)\n",
      "    34  title_content_com:  (3500, 3)\n",
      "id_list_sel:  3500 3600\n",
      "count:  [100]\n",
      "cor elapsed_time:  19.00\n",
      "tend elapsed_time:  31.00\n",
      "    35  title_id:  (100, 2)\n",
      "    35  content_id:  (100, 2)\n",
      "    35  title_content:  (100, 3)\n",
      "    35  title_content_com:  (3600, 3)\n",
      "id_list_sel:  3600 3700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 56172180, 'title': '成绩单和路线图：中国人寿召开2018上半年经营形势分析会', 'content': nan}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    36  title_id:  (100, 2)\n",
      "    36  content_id:  (99, 2)\n",
      "    36  title_content:  (100, 3)\n",
      "    36  title_content_com:  (3700, 3)\n",
      "id_list_sel:  3700 3800\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  19.00\n",
      "    37  title_id:  (100, 2)\n",
      "    37  content_id:  (100, 2)\n",
      "    37  title_content:  (100, 3)\n",
      "    37  title_content_com:  (3800, 3)\n",
      "id_list_sel:  3800 3900\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  10.00\n",
      "    38  title_id:  (100, 2)\n",
      "    38  content_id:  (100, 2)\n",
      "    38  title_content:  (100, 3)\n",
      "    38  title_content_com:  (3900, 3)\n",
      "id_list_sel:  3900 4000\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    39  title_id:  (100, 2)\n",
      "    39  content_id:  (100, 2)\n",
      "    39  title_content:  (100, 3)\n",
      "    39  title_content_com:  (4000, 3)\n",
      "id_list_sel:  4000 4100\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    40  title_id:  (100, 2)\n",
      "    40  content_id:  (100, 2)\n",
      "    40  title_content:  (100, 3)\n",
      "    40  title_content_com:  (4100, 3)\n",
      "id_list_sel:  4100 4200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  11.00\n",
      "    41  title_id:  (100, 2)\n",
      "    41  content_id:  (100, 2)\n",
      "    41  title_content:  (100, 3)\n",
      "    41  title_content_com:  (4200, 3)\n",
      "id_list_sel:  4200 4300\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  25.00\n",
      "    42  title_id:  (100, 2)\n",
      "    42  content_id:  (100, 2)\n",
      "    42  title_content:  (100, 3)\n",
      "    42  title_content_com:  (4300, 3)\n",
      "id_list_sel:  4300 4400\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 56621944, 'title': '成绩单和路线图：中国人寿召开2018上半年经营形势分析会', 'content': nan}\n",
      "    43  title_id:  (100, 2)\n",
      "    43  content_id:  (99, 2)\n",
      "    43  title_content:  (100, 3)\n",
      "    43  title_content_com:  (4400, 3)\n",
      "id_list_sel:  4400 4500\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  14.00\n",
      "    44  title_id:  (100, 2)\n",
      "    44  content_id:  (100, 2)\n",
      "    44  title_content:  (100, 3)\n",
      "    44  title_content_com:  (4500, 3)\n",
      "id_list_sel:  4500 4600\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  14.00\n",
      "    45  title_id:  (100, 2)\n",
      "    45  content_id:  (100, 2)\n",
      "    45  title_content:  (100, 3)\n",
      "    45  title_content_com:  (4600, 3)\n",
      "id_list_sel:  4600 4700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 56809476, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "    46  title_id:  (100, 2)\n",
      "    46  content_id:  (99, 2)\n",
      "    46  title_content:  (100, 3)\n",
      "    46  title_content_com:  (4700, 3)\n",
      "id_list_sel:  4700 4800\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57054771, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "    47  title_id:  (100, 2)\n",
      "    47  content_id:  (99, 2)\n",
      "    47  title_content:  (100, 3)\n",
      "    47  title_content_com:  (4800, 3)\n",
      "id_list_sel:  4800 4900\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57276170, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57350380, 'title': '成绩单和路线图：中国人寿召开2018上半年经营形势分析会', 'content': nan}\n",
      "    48  title_id:  (100, 2)\n",
      "    48  content_id:  (98, 2)\n",
      "    48  title_content:  (100, 3)\n",
      "    48  title_content_com:  (4900, 3)\n",
      "id_list_sel:  4900 5000\n",
      "count:  [100]\n",
      "cor elapsed_time:  12.00\n",
      "tend elapsed_time:  12.00\n",
      "    49  title_id:  (100, 2)\n",
      "    49  content_id:  (100, 2)\n",
      "    49  title_content:  (100, 3)\n",
      "    49  title_content_com:  (5000, 3)\n",
      "id_list_sel:  5000 5100\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57782081, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "    50  title_id:  (100, 2)\n",
      "    50  content_id:  (99, 2)\n",
      "    50  title_content:  (100, 3)\n",
      "    50  title_content_com:  (5100, 3)\n",
      "id_list_sel:  5100 5200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    51  title_id:  (100, 2)\n",
      "    51  content_id:  (100, 2)\n",
      "    51  title_content:  (100, 3)\n",
      "    51  title_content_com:  (5200, 3)\n",
      "id_list_sel:  5200 5300\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  16.00\n",
      "    52  title_id:  (100, 2)\n",
      "    52  content_id:  (100, 2)\n",
      "    52  title_content:  (100, 3)\n",
      "    52  title_content_com:  (5300, 3)\n",
      "id_list_sel:  5300 5400\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 58375027, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "    53  title_id:  (100, 2)\n",
      "    53  content_id:  (99, 2)\n",
      "    53  title_content:  (100, 3)\n",
      "    53  title_content_com:  (5400, 3)\n",
      "id_list_sel:  5400 5500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    54  title_id:  (100, 2)\n",
      "    54  content_id:  (100, 2)\n",
      "    54  title_content:  (100, 3)\n",
      "    54  title_content_com:  (5500, 3)\n",
      "id_list_sel:  5500 5600\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 58559635, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "    55  title_id:  (100, 2)\n",
      "    55  content_id:  (99, 2)\n",
      "    55  title_content:  (100, 3)\n",
      "    55  title_content_com:  (5600, 3)\n",
      "id_list_sel:  5600 5700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 58921649, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "    56  title_id:  (100, 2)\n",
      "    56  content_id:  (99, 2)\n",
      "    56  title_content:  (100, 3)\n",
      "    56  title_content_com:  (5700, 3)\n",
      "id_list_sel:  5700 5800\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    57  title_id:  (100, 2)\n",
      "    57  content_id:  (100, 2)\n",
      "    57  title_content:  (100, 3)\n",
      "    57  title_content_com:  (5800, 3)\n",
      "id_list_sel:  5800 5900\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  11.00\n",
      "    58  title_id:  (100, 2)\n",
      "    58  content_id:  (100, 2)\n",
      "    58  title_content:  (100, 3)\n",
      "    58  title_content_com:  (5900, 3)\n",
      "id_list_sel:  5900 6000\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  16.00\n",
      "    59  title_id:  (100, 2)\n",
      "    59  content_id:  (100, 2)\n",
      "    59  title_content:  (100, 3)\n",
      "    59  title_content_com:  (6000, 3)\n",
      "id_list_sel:  6000 6100\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  14.00\n",
      "    60  title_id:  (100, 2)\n",
      "    60  content_id:  (100, 2)\n",
      "    60  title_content:  (100, 3)\n",
      "    60  title_content_com:  (6100, 3)\n",
      "id_list_sel:  6100 6200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    61  title_id:  (100, 2)\n",
      "    61  content_id:  (100, 2)\n",
      "    61  title_content:  (100, 3)\n",
      "    61  title_content_com:  (6200, 3)\n",
      "id_list_sel:  6200 6300\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  14.00\n",
      "    62  title_id:  (100, 2)\n",
      "    62  content_id:  (100, 2)\n",
      "    62  title_content:  (100, 3)\n",
      "    62  title_content_com:  (6300, 3)\n",
      "id_list_sel:  6300 6400\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  16.00\n",
      "    63  title_id:  (100, 2)\n",
      "    63  content_id:  (100, 2)\n",
      "    63  title_content:  (100, 3)\n",
      "    63  title_content_com:  (6400, 3)\n",
      "id_list_sel:  6400 6500\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  16.00\n",
      "    64  title_id:  (100, 2)\n",
      "    64  content_id:  (100, 2)\n",
      "    64  title_content:  (100, 3)\n",
      "    64  title_content_com:  (6500, 3)\n",
      "id_list_sel:  6500 6600\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  16.00\n",
      "    65  title_id:  (100, 2)\n",
      "    65  content_id:  (100, 2)\n",
      "    65  title_content:  (100, 3)\n",
      "    65  title_content_com:  (6600, 3)\n",
      "id_list_sel:  6600 6700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    66  title_id:  (100, 2)\n",
      "    66  content_id:  (100, 2)\n",
      "    66  title_content:  (100, 3)\n",
      "    66  title_content_com:  (6700, 3)\n",
      "id_list_sel:  6700 6800\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  9.00\n",
      "    67  title_id:  (100, 2)\n",
      "    67  content_id:  (100, 2)\n",
      "    67  title_content:  (100, 3)\n",
      "    67  title_content_com:  (6800, 3)\n",
      "id_list_sel:  6800 6900\n",
      "count:  [100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  9.00\n",
      "    68  title_id:  (100, 2)\n",
      "    68  content_id:  (100, 2)\n",
      "    68  title_content:  (100, 3)\n",
      "    68  title_content_com:  (6900, 3)\n",
      "id_list_sel:  6900 7000\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  11.00\n",
      "    69  title_id:  (100, 2)\n",
      "    69  content_id:  (100, 2)\n",
      "    69  title_content:  (100, 3)\n",
      "    69  title_content_com:  (7000, 3)\n",
      "id_list_sel:  7000 7100\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  11.00\n",
      "    70  title_id:  (100, 2)\n",
      "    70  content_id:  (100, 2)\n",
      "    70  title_content:  (100, 3)\n",
      "    70  title_content_com:  (7100, 3)\n",
      "id_list_sel:  7100 7200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  8.00\n",
      "    71  title_id:  (100, 2)\n",
      "    71  content_id:  (100, 2)\n",
      "    71  title_content:  (100, 3)\n",
      "    71  title_content_com:  (7200, 3)\n",
      "id_list_sel:  7200 7300\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    72  title_id:  (100, 2)\n",
      "    72  content_id:  (100, 2)\n",
      "    72  title_content:  (100, 3)\n",
      "    72  title_content_com:  (7300, 3)\n",
      "id_list_sel:  7300 7400\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  14.00\n",
      "    73  title_id:  (100, 2)\n",
      "    73  content_id:  (100, 2)\n",
      "    73  title_content:  (100, 3)\n",
      "    73  title_content_com:  (7400, 3)\n",
      "id_list_sel:  7400 7500\n",
      "count:  [26]\n",
      "cor elapsed_time:  0.00\n",
      "tend elapsed_time:  3.00\n",
      "    74  title_id:  (26, 2)\n",
      "    74  content_id:  (26, 2)\n",
      "    74  title_content:  (26, 3)\n",
      "    74  title_content_com:  (7426, 3)\n",
      "data_null:  (8028, 10)\n",
      "combined_data:  (8028, 12)\n",
      "data_null_still:  (0, 12)\n",
      "update_data:  (8053, 13)\n"
     ]
    }
   ],
   "source": [
    "for filename in ['raw/人寿 7月.xlsx',]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo_backup t1, db_docinfo_text_backup t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "        \n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])           \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)        \n",
    "        title_content_com.index = range(title_content_com.shape[0])\n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "\n",
    "    data_null['八大分险类型'] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null['文章倾向性'] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T09:45:05.506894Z",
     "start_time": "2018-11-06T09:45:02.691733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_null:  (8028, 11)\n",
      "combined_data:  (8028, 13)\n",
      "data_null_still:  (0, 13)\n",
      "update_data:  (8053, 13)\n"
     ]
    }
   ],
   "source": [
    "    title_content_com.index = range(title_content_com.shape[0])\n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "    data_null['八大分险类型'] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null['文章倾向性'] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in ['raw/同业  七月.xlsx', ]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo_backup t1, db_docinfo_text_backup t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "        \n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])           \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)        \n",
    "        title_content_com.index = range(title_content_com.shape[0])\n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "\n",
    "    data_null[''] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null[''] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七月之后数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:58:34.263341Z",
     "start_time": "2018-11-06T03:52:53.495Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list_2 = ['raw/人寿  8-10月.xlsx', 'raw/同业  8-10月.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in ['raw/人寿  8-10月.xlsx',]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo t1, db_docinfo_text t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "\n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])            \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)                \n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    title_content_com.index = range(title_content_com.shape[0])\n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "    data_null[''] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null[''] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in ['raw/同业  8-10月.xlsx', ]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo t1, db_docinfo_text t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "\n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])            \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)                \n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    title_content_com.index = range(title_content_com.shape[0])\n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "    data_null[''] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null[''] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

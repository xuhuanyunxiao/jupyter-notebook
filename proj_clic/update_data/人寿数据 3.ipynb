{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 中国人寿及其同业数据，补充八分类和倾向性结果\n",
    "> - 七月及以前数据：db_docinfo_backup、db_docinfo_text_backup\n",
    "> - 七月之后数据：db_docinfo_trade、db_docinfo_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:52:56.564929Z",
     "start_time": "2018-11-06T08:52:55.465866Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import requests,json\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:52:56.580930Z",
     "start_time": "2018-11-06T08:52:56.566929Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from toolkits.setup.date_time import get_day_list\n",
    "from toolkits.setup import specific_func\n",
    "specific_func.set_ch_pd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:53:06.943523Z",
     "start_time": "2018-11-06T08:52:56.585930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = specific_func.get_engine('cbirc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:53:06.960524Z",
     "start_time": "2018-11-06T08:53:06.946523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '监管',\n",
       " 2: '行业',\n",
       " 3: '产品销售',\n",
       " 4: '资本市场',\n",
       " 5: '公司内部管理',\n",
       " 6: '消费服务',\n",
       " 7: '其他相关报道',\n",
       " 8: '噪音'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic={'监管':1,'行业':2,'产品销售':3,'资本市场':4,'公司内部管理':5,'消费服务':6,'其他相关报道':7,'噪音':8}\n",
    "class_name_dict = {v: k for k, v in label_dic.items()}\n",
    "class_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七月及以前数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:53:30.838889Z",
     "start_time": "2018-11-06T08:53:30.827889Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_port = '47.93.183.157:10000'\n",
    "\n",
    "headers={'content-type':'application/json'}\n",
    "url_cor = \"http://%s/judge_correlation_i\"%ip_port\n",
    "url_tend = \"http://%s/tendency_analysis_i\"%ip_port\n",
    "url_warn = \"http://%s/early_warning_i\"%ip_port\n",
    "\n",
    "file_list_1 = ['raw/人寿 7月.xlsx', 'raw/同业  七月.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:42:58.220706Z",
     "start_time": "2018-11-06T07:58:52.476378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/人寿 7月.xlsx   ----------------\n",
      "总量： (8053, 12)\n",
      "缺失值数量： (8028, 12)\n",
      "无缺失值数量： (25, 12)\n",
      "id_list:  7426\n",
      "id_list_sel:  0 1000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  32.00\n",
      "tend elapsed_time:  111.00\n",
      "    0  title_id:  (1000, 2)\n",
      "    0  content_id:  (1000, 2)\n",
      "    0  title_content:  (1000, 3)\n",
      "    0  title_content_com:  (1000, 3)\n",
      "id_list_sel:  1000 2000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  22.00\n",
      "tend elapsed_time:  129.00\n",
      "    1  title_id:  (1000, 2)\n",
      "    1  content_id:  (1000, 2)\n",
      "    1  title_content:  (1000, 3)\n",
      "    1  title_content_com:  (2000, 3)\n",
      "id_list_sel:  2000 3000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  25.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 54974461, 'title': '中国人寿“康宁终身”（至尊版）产品详解+案例+病种明细', 'content': nan}\n",
      "error again...     ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "{'id': 55510647, 'title': '国寿新防癌：保100多种癌症+160%健康保障，6重给付！', 'content': '“ 每个中国人都必须有一张国寿防癌险（优享版），人人都买得起的生命防线！ 四大亮点值得拥有 1 性价比高：保费超低，保障超高 一天几块钱，保障几十万，绝对的惠民产品！ 2 保障全：多重保障、全面呵护\\xa0 从轻症到癌症，从治疗到康复，关爱始终陪伴！高达100多种癌症保障！ 3 确诊有豁免：保费豁免，爱不停歇 癌症确诊免交后期保费， 减轻家庭二次伤害！ 4 满期保费返：满期给付，双重关怀 有病管病，无病养老！ 国寿防癌险升级前后图 国寿防癌第一险，出生满28天的健康宝宝——60岁都能投保。投入小、保障高、满期返，优享版更安心！ 投保案例 30岁男性，每年缴费3700（平均每天10块钱，也就是少抽半包烟），20年交，保障20万，保障到80岁，可获得如下保障 1 癌症确诊保障：200000元； 确诊即给付100%保额，专注治疗和康复，如果是在缴费期间之内，则豁免后期保费。 2 特定癌症保障：260000元； 六大特定高发癌症（前列腺癌、肺癌、乳腺癌、宫颈癌、白血病、骨癌），针对不同人群高发病种，额外给付30%保额，彰显中国人寿人性化。 3 轻症癌症保障：60000元； 确诊给付30%保额，补充医疗，合同继续有效。 4 康复金：40000元/年*5年； 最高持续给付5年，康复期给您更多呵护，让您安心养病，缓解压力。 5 满期金：80岁返74000元； 平平安安到80，给付所交保费，有病看病，无病养老！ 6 身故保障：不少于所交保费； ■ 主险保障： 18岁后给付主险所交保费105%与现价较大者； ■ 附加两全保障： 41岁前给付附加险所交保费160%与现价较大者； 61岁前给付附加险所交保费140%与现价较大者； 61岁后给付附加险所交保费120%与现价较大者； 辛辛苦苦几十年，一病回到解放前！但是有保险，一切都将不一样！国寿防癌险（优享版），给你全面的防癌保障，构建健康新防线！'}\n",
      "    2  title_id:  (1000, 2)\n",
      "    2  content_id:  (999, 2)\n",
      "    2  title_content:  (1000, 3)\n",
      "    2  title_content_com:  (3000, 3)\n",
      "id_list_sel:  3000 4000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  19.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 56172180, 'title': '成绩单和路线图：中国人寿召开2018上半年经营形势分析会', 'content': nan}\n",
      "error again...     ('Connection aborted.', TimeoutError(10060, '由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。', None, 10060, None))\n",
      "{'id': 56436969, 'title': '上市险企保费收入增速超预期 收入的增势乐观', 'content': '四大上市企险近日陆续公布上半年保费数据。上半年中国平安实现原保费收入共计4080亿元，同比增20%，其中，产险1188.78亿元，寿险2745.25亿元，养老险128.44亿元，健康险17.51亿元。中国人寿上半年原保费收入达3602亿元，同比增长4%。新华保险上半年原保费收入678.70亿元，同比增11%。太保寿险、产险业务收入分别为1300.23亿元、610.97亿元，合计实现1911.20亿元，同比增17%。\\n\\r券商研究人士认为，上市险企保费数据好于预期，在资管新规、税延养老平台等政策支持下，保险行业长期整体向好已成共识，下半年保费收入改善可期。\\n\\r上半年保费收入增速好于预期\\n\\r从6月份的数据来看，上市险企保费增长普遍较好。其中，国寿、新华、平安寿险分别实现原保费收入531亿元、132亿元、307亿元，同比分别增长11%、20%、24%。其中，平安寿险6月实现单月个险新单为111亿元，同比增长20%。\\n\\r安信证券研究中心副总经理、金融行业首席分析师赵湘怀认为，6月保险业整体数据好于预期，一定程度得益于去年同期基数较低。国寿、新华、平安寿险单月保费同比增速均较5月数据改善。另外，平安寿险6月份个险新单同比增速达20%，好于此前市场预期，也受益于去年公司较早停售快返产品造成的低基数所致。\\n\\r赵湘怀表示，业务结构优化也是数据好于预期的原因。今年行业开门红明显遇冷的情况之下，各保险公司在第二季度加大聚焦保障型产品的力度，加大费用投放并调整佣金分配，与优化培训机制提升代理人素质等措施，推动行业上半年原保费与个险新单降幅进一步缩窄。\\n\\r“面对保险业转型期，扩大期交业务，加大保障功能，是险企一致的改革方向。”东方金诚首席金融分析师徐承远认为，从盈利模式来看，中国平安持续巩固全牌照下的发展优势，新单保费优于行业，同时以科技金融构建核心竞争力，处于行业龙头地位。新华保险价值转型起步最晚，但已取得阶段性成果，2018年以来聚焦于健康险、附加险和代理人队伍增长。\\n\\r徐承远同时称，国寿业务稳步推进银保渠道转型，加强内部控制和代理人队伍管理，并与京东、百度等科技巨头开展合作，加强自身科技建设。中国太保总新保单保费增长有所恢复，负增长幅度收窄，其长期保障型保险新单保费4月以来持续实现正增长。同期，中国太保非车险增速较高，车险增速高于行业，前五个月综合成本率同比略有优化、赔付率和费用率小幅优化，承保盈利优于行业水平。\\n\\r申万宏源分析师马鲲鹏表示，平安寿险在新单结构持续改善的背景下，预计上半年价值继续改善，NBV预期可实现同比小幅正增长。新华保险二季度健康险新单销量实现高增长，加上&ldquo;多倍保&rdquo;系列7月16日起全国销售，预计将在&ldquo;以量补价&rdquo;的方式下推动完成全年148亿元健康险新单目标。\\n\\r保费收入增势乐观\\n\\r&ldquo;中期来看，保险行业有望增加行业规模保费5个百分点。&rdquo;招商证券分析师郑积沙称，展望下半年，预计相关费用政策将延续至下半年，保费基本面改善可持续。\\n\\r第二季度各家公司调整费用政策效果显著，健康险销售增速明显回升。不过，新单和NBV缺口犹存，保险公司压力依旧较大。\\n\\r郑积沙指出，下半年新产品将陆续上线。平安推出&ldquo;爱满分&rdquo;，后期还将上线&ldquo;福满分&rdquo;，丰富长期保障类产品线；新华7月16日大力推多倍保；太保7月1日上线&ldquo;金诺人生&rdquo;等。费用投入政策持续、新产品的陆续上线，同时新单增速逐步改善后有望稳住代理人队伍，叠加下半年基数下降，健康险同比和占比有望持续提升，推动价值进一步朝目标靠拢。\\n\\r郑积沙表示，长期来看，政策助力，寿险行业长期增长无忧。一是资管新规力图打破刚性兑付，而保险产品属于表内负债，并不在资管新规监管范畴，因此，年金、分红等刚兑型产品的竞争对象仅剩存款一类，而保险产品预定收益率碾压存款利率，将极大受益资金挤出效应，有望推动负债端高速增长。二是税延养老平台正式运行，首批获准经营税延养老产品的12家险企中，已有6家险企销售获批。'}\n",
      "error again...     ('Connection aborted.', TimeoutError(10060, '由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。', None, 10060, None))\n",
      "{'id': 56450183, 'title': '4大上市险企交双面成绩单：半年原保费超万亿、股价一路下行', 'content': '具体来看，集团上市的中国平安，今年上半年实现原保险保费收入4079.98亿元，其中，平安人寿实现2745.25亿元原保费收入，平安财险实现1188.78亿元原保费收入，平安养老实现原保费128.44亿元原保费收入，平安健康实现17.75亿元原保费收入。\\n\\r(图片)\\n\\r单独寿险上市的中国人寿，同样在体量上表现突出，上半年，中国人寿原保费收入达3602亿元，看来，素有“一哥之争”的中国人寿同平安人寿两家企业，在2018年上半年，中国人寿体量上依然“略胜一筹”。与此同时，蓝鲸保险了解到，7月18日，中国人寿召开上半年经营形势分析会，会上透露未上市保险业务数据，上半年，未经审计数据，中国人寿财险公司实现保费收入352亿元。\\n\\r中国太保方面，上半年实现原保费收入1911.2亿元，其中，太保寿险在前6月实现1300.23亿元原保险保费收入，太保财险原保险保费收入达到610.97亿元。新华保险则在上半年实现了原保险保费678.7亿元，并第一家公布半年报预告，新华保险表示，预计今年半年度实现归属母公司股东净利润、扣除非经常性损益后归属于母公司股东净利润两项指标，与上年同期相比，均将增加80%左右。\\n\\r增速有差异：寿险中国人寿垫底，健康险、养老险有突破\\n\\r从业务增速来看，寿险业务中，增长最快的仍是中国平安，今年上半年平安人寿原保费收入同比增速达21.19%;紧随其后的，是太保寿险，上半年原保费收入同比增速达17.61%;值得关注的是，转型收官的新华保险，在甩掉趸交“包袱”后，增速实现“节节高”，上半年，原保费收入同比增长10.83%;而排在四大上市险企增速末位的，则是中国人寿，增速为个位数，仅为4.04%。\\n\\r从业务增长支撑点来看，平安人寿及太保寿险都为个险业务，公告数据显示，平安人寿个人业务在今年上半年实现2780.95亿元原保费收入，占比达到96.19%，其中新业务保费收入为925.41亿元，续期业务原保费为1855.54亿元。太保寿险个人业务原保费收入为1251.03亿元，占比达到96.22%。\\n\\r财险方面，太保财险上半年同比增长16.25%，平安财险同比增幅为14.92%，从两家公司的财险业务构成来看，两家公司车险业务占比均超70%，太保财险车险业务占比达71.76%，平安财险车险业务占比达71.26%。对比去年前半年数据，两家财险公司车险业务占比均有所下滑，分别下滑3.3个百分点及6.3个百分点。\\n\\r此外，健康、养老业务方面，平安健康上半年增幅最为明显，今年前6月实现17.75亿元原保费收入，同比增长78.31%。平安养老同期原保险保费收入为128.44万元，同比增幅达到25.82%。与此同时，今年上半年，保险业助力养老体系实现突破，税延养老保险试点落地，四大上市险企积极响应，并纷纷签单“落地”。\\n\\r业绩、股价“两重天”，上半年保险板块表现逊于大盘\\n\\r值得关注的是，与四大上市险企前六月保费收入同比均上涨不同的是，业务向好并未在市场表现上得到体现。开年至今，整体来看，四家险企股价均呈震荡下滑态势。\\n\\r(图片)\\n\\r细分来看，同花顺数据显示，中国平安1月2日以71.39元开盘，随后小幅攀升，在1月末达到80.08元阶段高点后，一路下挫，截止6月底，中国平安以58.58元的价格收官上半年，区间跌幅达17.94%。\\n\\r中国人寿、中国太保跌幅则均超过两成，中国人寿今年以30.09元开盘，横向震荡一个月后，于2月初跳空低走，连续两日日均跌幅超5%，随后一路下探，截至6月底，股价跌至22.52元，区间跌幅达25.16%。中国太保则以41.91元的股价开启今年走势，随后区间震荡下跌、横向调整，截至6月末，股价以31.85元收官上半年，区间跌幅累积达24%。\\n\\rA股四大上市险企中，跌幅最大的，是新华保险。今年开年，新华保险以70.9元股价开盘，随后一路下跌，在4月20日探至区间低点40.51元后，区间箱体震荡，截至6月末，新华保险股价为42.88元，区间跌幅达39.52%。\\n\\r查看同期上证指数、沪深300指数走势，分别下跌14.96%、14.10%，对比来看，今年上半年，四大上市险企表现均逊色于大盘。但进入下半年，受经济基本面及大盘走势影响，保险股仍处低迷，未实现反转行情。\\n\\r掣肘因素仍有小影响，机构称板块极具安全边际\\n\\r“目前来看，短期市场对于二级市场悲观情绪可能会压制投资者对保险股的风险偏好，但板块整体向好已经是较为明显的趋势”，万联证券缴文超表示，其判断，上市险企整体都将在二季度以及下半年有一定程度的业绩改善”。\\n\\r缴文超建议称，总体来看，保险板块短线博弈的意义不大，但从长期价值来看，仍坚持内含价值和剩余边际全年将保持中高速增长的判断，“目前中国人寿、中国太保、新华保险PEV均在1倍左右，极具安全边际，板块具有相当的配置机会”。\\n\\r尽管业绩不错，但在分析机构眼中，上市险企今年也确实存在掣肘因素。“目前制约股价上涨的核心因素，在于新单保费增速为负，及新业务价值增速下降”，安信金融团队分析称。\\n\\r长江非银团队亦持有同样观点，其指出，从基本面来看，保险行业受监管政策收紧和利率水平较高影响，负债端保费增速普遍下滑，“新单保费出现较大负增长”。同时，利率的下滑，也给投资端带来压力。\\n\\r此外，安信金融团队同时指出，保险营销方面，上半年营销员人数下滑带来的压力或也将延续，“人均件数带动产能提升难度较大”。\\n\\r行业政策方面，此前银保监会下发《关于组织开展人身保险产品专项检查清理工作的通知》，已于7月开始专项检查，据华泰金融团队调研所得，四大上市险企已对受影响的产品进行调整且基本整改到位，影响相对有限。\\n\\r“监管规范和引导下预计全年价值成长稳健”，华泰金融预计2018年中国平安、中国太保、新华保险、中国人寿的P/EV分别为1.09、0.91、0.79和0.77，估值优势显著，“对板块维持增持评级”。（蓝鲸保险 李丹萍 石雨）\\n\\r保费\\n\\r股价\\n\\r保险\\n\\r(图片)\\n\\r掌握50万亿的机构，他们在买什么股\\n\\r声明：该文观点仅代表作者本人，搜狐号系信息发布平台，搜狐仅提供信息存储空间服务。'}\n",
      "    3  title_id:  (1000, 2)\n",
      "    3  content_id:  (999, 2)\n",
      "    3  title_content:  (1000, 3)\n",
      "    3  title_content_com:  (4000, 3)\n",
      "id_list_sel:  4000 5000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  21.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 56621944, 'title': '成绩单和路线图：中国人寿召开2018上半年经营形势分析会', 'content': nan}\n",
      "error again...     ('Connection aborted.', TimeoutError(10060, '由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。', None, 10060, None))\n",
      "{'id': 56641252, 'title': '情系中国人寿    文/吴明芝', 'content': '有一天，我从中国人寿太白支公司经过，左上角那个醒目的绿色标志深深地吸引了我，它为的是让每一个珍爱它的人，都能扬起生命的风帆。于是，我把一颗心留在了这里…… “ 把爱 心送给每一个人，把真情送给每一个人……”第一次参加早会时，我把自己也融入了这首最动听的《司歌》…… 十四年前，走出班主任蔡宝堂老师的91期新人培训班，我试着去销售为“生命筑起一道绿色屏障”的《国寿康宁终身保险》，那位个头高高的男客户，不仅以居高临下的气势审视一脸纯朴和热情的我，还用刻薄的话语把我拒之于冷冷的门外边……“拒绝千遍不厌倦，拒绝的感觉像春天”，擦干眼泪，我在心中哼了一遍又一遍，吟唱着一份爱的执着，依然在没有航标的寿险河床上，寻找流向…… 十四年前，我用朴实而诚恳的话语与一位0岁孩子的妈妈促膝而谈，递给她一张“伴我成长”的计划书，建议她给孩子购买《国寿英才少儿保险（豁免功能）》，她拿出了家中仅有的894元抚尉我：你在践行一个成己为人，成人达己的理念。 十四年前，我背着凉开水和方便面，行走在山路十八湾，因为是公司在县电台的关注下，交给客户13多万元的理赔款，让我看到了中国人寿保险业明媚的春天！ 忘不了，行销中遭受的挫折;忘不了，亲朋好友期盼鼓励的目光;更忘不了，在中国人寿营销10年“诚信”演讲比赛中，社会各界人士，三百多名诚信企业家，市领导以及其它选手认可的笑声和热烈的掌声。 有多少次，我回望这条寿险之路，它曲曲折折;有多少次，我在心里默默问自己：“选择了中国人寿，就选择了辉煌的人生舞台”这句誓言是否永驻心间？ 也许，十四年的寿险之旅不算太长，但在通过中国人寿把爱献给更多需要关爱的善良的人们的路上，留下了我艰难跋涉的脚印，也留下了我串串散乱的诗句…… 2009年7月25日，公司推荐我参加中国人寿宝鸡分公司通讯员培训班。是保险通讯会上，《国寿客户报》的主编杨文博老师课程的激励性，幽默感和双向互动性启迪了我的心灵，我领悟到了：我们只有学会了先做人，以平和心态为客户提供最好的服务，每天去做帮助老百姓的事，把自己真正融入到民族寿险业中去，才能写出有真情实感的东西。我也深刻的体会到了，成长的快乐和做一名通讯员的自信。 2010年1月，我恪守着“今天我与客户有个约定”，靠着一个“没有比腿更长的路，没有比人更高的山”的信念，走出了风雪的重围，又一次来到离县城较远的一个闭塞的村庄。45岁的某先生，取出了一张满期的9万元存单，交给我，并用颤抖的手在保单上签了字。他的妻子泣不成声的说：“一年多了，我们都沉浸在悲痛中，因一次意外事故，我们失去了唯一的儿子。但这些天以来，是你的好多个暖心电话，你的几次上门讲解，才使我们对保险有了认识。我们相信你，也支持你，更是象你所说的，这张特别的保单能让我们没事当存钱，有事当保险！” 2015年7月22日，在中国人寿宝鸡分公司“国寿职业经理人”训练营阶段，我被授予“新人育成荣誉称号”;10月23日被送往一代伟人的故乡，接受红色教育;24日下午5时，我站在了橘子洲头，领略了学生时代毛泽东诗中的情怀，按捺不住内心的激动，在落日余晖铺就的金光闪闪湘江边，感慨万千，高声吟诵“独立寒秋，湘江北去，橘子洲头，看万山红遍，层林尽染……问宝鸡国寿，谁主沉浮?”尽然出乎意料的PK过了那位年轻优秀的男讲解员导游;11月20日荣获太白支公司“2015新主管荣誉称号”;在2018年保学堂“诚信315，我为保险代言”活动中荣获“诚信保险大使称号”，在收到鲜红本本的那一刻，我突然泪眼婆娑，以为看错了什么。 今天，我重新去凝视这轮冉冉东升的旭日，终于有机会向它表达真实的心意。我不知道这个品牌价值3253、72亿元的“中国人寿”是否将永久地照耀着我生命的航程，但我早已把它融进了我热血滚滚的心，只为一颗赤诚于它（要更用心地去为它抒写美好未来）的心，不再迷惘中漂泊沉沦！ \\xa0 \\xa0 作者简介：吴明芝，陕西省太白县人，宝鸡职工作家协会会员，太白县人寿保险公司职员。中学时代开始发表小诗。在中国第一界现代诗，在全国新诗词创作大奖赛，在陕西省农民工诗歌大赛中多次获奖。多年来，对文字仍然充满了热爱，时常在一个人的世界里，习惯用文字与黑夜对语。 作者的往期作品： 秋恋 另一种思念\\xa0 诚信与生命同在（演讲稿）\\xa0 故乡的山城\\xa0 失 去 的 诗\\xa0 郑重申明： 本文由作者提供，并对此负责。图片除作者照片外，其他均来自网络。 主办单位：太白县作家协会 本期编辑：\\xa0闫永强 \\xa0李敏 \\xa0吴明芝 本期责任编辑：闫永强 欢迎您踊跃投稿！（有在本公众号已推送过的文章,来稿时请在文章底部附上往期文章标题）\\xa0 邮箱： tbxmcwy@163.com \\xa0 \\xa0 微信：TaiBai2015\\xa0 感谢您一路的陪伴、支持，欢迎您在下面评论、点赞、转发！'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error again...     ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "{'id': 56688549, 'title': '【发改动态】我委召开金融形势分析座谈会', 'content': '2018年7月19日，南京市发改委蓝军副主任在玉兰路8号国资大厦主持召开金融形势分析座谈会。人民银行南京分行营业管理部胡亮亮副处长，工商银行南京分行陈金华副总经理，紫金农商行总行王清国副行长，华泰联合证券债务融资部刘江峰主管，中国人寿南京分公司综合管理部钱旭强副经理，江苏省信用再担保集团南京分公司杨喜荣总经理，南京紫金投资集团陈玲总会计师，南京新工投资集团姚兆年总会计师，南京江宁科技创业投资集团叶桂副总经理，大公国际资信评估有限公司华东一区孙高升总经理，江苏国衡土地房地产资产评估咨询有限公司刘清军董事长，南京市发改委综合处余其刚处长，财金处胡耀调研员参加座谈。会议由南京紫金投资集团承办。 \\xa0 \\xa0 与会人员就2018年上半年经济和金融运行情况，特别是融资情况，以及下半年宏观经济和金融走势及对全市经济运行的影响等方面进行了充分交流和讨论，对做好下半年工作提出了相关意见和建议。 今年以来，全市认真落实中央和省市决策部署，坚持稳中求进工作总基调，认真践行新发展理念和高质量发展要求，全面对标找差、创新实干，聚力聚焦创新名城建设，统筹推进稳增长、调结构、促改革、惠民生、防风险等各项工作，经济运行呈现出“运行平稳、稳中向好、稳中提质”的良好态势。 来源：委财金处'}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 56809476, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57054771, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57276170, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57350380, 'title': '成绩单和路线图：中国人寿召开2018上半年经营形势分析会', 'content': nan}\n",
      "    4  title_id:  (1000, 2)\n",
      "    4  content_id:  (995, 2)\n",
      "    4  title_content:  (1000, 3)\n",
      "    4  title_content_com:  (5000, 3)\n",
      "id_list_sel:  5000 6000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  24.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 57782081, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 58375027, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 58559635, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 58921649, 'title': '突破！中国人寿进入《财富》“世界500强”前50位', 'content': nan}\n",
      "    5  title_id:  (1000, 2)\n",
      "    5  content_id:  (996, 2)\n",
      "    5  title_content:  (1000, 3)\n",
      "    5  title_content_com:  (6000, 3)\n",
      "id_list_sel:  6000 7000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  21.00\n",
      "tend elapsed_time:  121.00\n",
      "    6  title_id:  (1000, 2)\n",
      "    6  content_id:  (1000, 2)\n",
      "    6  title_content:  (1000, 3)\n",
      "    6  title_content_com:  (7000, 3)\n",
      "id_list_sel:  7000 8000\n",
      "count:  [426]\n",
      "cor elapsed_time:  9.00\n",
      "tend elapsed_time:  60.00\n",
      "    7  title_id:  (426, 2)\n",
      "    7  content_id:  (426, 2)\n",
      "    7  title_content:  (426, 3)\n",
      "    7  title_content_com:  (7426, 3)\n",
      "data_null:  (8028, 10)\n",
      "combined_data:  (8028, 12)\n",
      "data_null_still:  (0, 12)\n",
      "update_data:  (8053, 12)\n",
      "raw/同业  七月.xlsx   ----------------\n",
      "总量： (11680, 12)\n",
      "缺失值数量： (11631, 12)\n",
      "无缺失值数量： (49, 12)\n",
      "id_list:  4774\n",
      "id_list_sel:  0 1000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  23.00\n",
      "tend elapsed_time:  121.00\n",
      "    0  title_id:  (1000, 2)\n",
      "    0  content_id:  (1000, 2)\n",
      "    0  title_content:  (1000, 3)\n",
      "    0  title_content_com:  (1000, 3)\n",
      "id_list_sel:  1000 2000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  28.00\n",
      "tend elapsed_time:  152.00\n",
      "    1  title_id:  (1000, 2)\n",
      "    1  content_id:  (1000, 2)\n",
      "    1  title_content:  (1000, 3)\n",
      "    1  title_content_com:  (2000, 3)\n",
      "id_list_sel:  2000 3000\n",
      "count:  [1000]\n",
      "cor elapsed_time:  23.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-8a083f733f5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mtry\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             result = requests.post(url_tend, data = json.dumps(data),\n\u001b[1;32m---> 45\u001b[1;33m                                    headers=headers, allow_redirects=True)\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mjson_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mtend_elapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'elapsed_time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \"\"\"\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    511\u001b[0m         }\n\u001b[0;32m    512\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for filename in ['raw/人寿 7月.xlsx',]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo_backup t1, db_docinfo_text_backup t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "        \n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])           \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)        \n",
    "        title_content_com.index = range(title_content_com.shape[0])\n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "\n",
    "    data_null[''] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null[''] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in ['raw/同业  七月.xlsx', ]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo_backup t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo_backup t1, db_docinfo_text_backup t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "        \n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])           \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)        \n",
    "        title_content_com.index = range(title_content_com.shape[0])\n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "\n",
    "    data_null[''] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null[''] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七月之后数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T08:53:41.017472Z",
     "start_time": "2018-11-06T08:53:41.012471Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list_2 = ['raw/人寿  8-10月.xlsx', 'raw/同业  8-10月.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T09:35:07.906713Z",
     "start_time": "2018-11-06T08:53:42.174538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/人寿  8-10月.xlsx   ----------------\n",
      "总量： (16316, 12)\n",
      "缺失值数量： (8598, 12)\n",
      "无缺失值数量： (7718, 12)\n",
      "id_list:  7472\n",
      "id_list_sel:  0 100\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  15.00\n",
      "    0  title_id:  (100, 2)\n",
      "    0  content_id:  (100, 2)\n",
      "    0  title_content:  (100, 3)\n",
      "    0  title_content_com:  (100, 3)\n",
      "id_list_sel:  100 200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "error:  Expecting value: line 1 column 1 (char 0)\n",
      "error again...     Expecting value: line 1 column 1 (char 0)\n",
      "{'id': 63620278, 'title': '反洗钱不到位 中国人寿被处罚70万元', 'content': nan}\n",
      "    1  title_id:  (100, 2)\n",
      "    1  content_id:  (99, 2)\n",
      "    1  title_content:  (100, 3)\n",
      "    1  title_content_com:  (200, 3)\n",
      "id_list_sel:  200 300\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  23.00\n",
      "    2  title_id:  (100, 2)\n",
      "    2  content_id:  (100, 2)\n",
      "    2  title_content:  (100, 3)\n",
      "    2  title_content_com:  (300, 3)\n",
      "id_list_sel:  300 400\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  14.00\n",
      "    3  title_id:  (100, 2)\n",
      "    3  content_id:  (100, 2)\n",
      "    3  title_content:  (100, 3)\n",
      "    3  title_content_com:  (400, 3)\n",
      "id_list_sel:  400 500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  30.00\n",
      "    4  title_id:  (100, 2)\n",
      "    4  content_id:  (100, 2)\n",
      "    4  title_content:  (100, 3)\n",
      "    4  title_content_com:  (500, 3)\n",
      "id_list_sel:  500 600\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  16.00\n",
      "    5  title_id:  (100, 2)\n",
      "    5  content_id:  (100, 2)\n",
      "    5  title_content:  (100, 3)\n",
      "    5  title_content_com:  (600, 3)\n",
      "id_list_sel:  600 700\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  11.00\n",
      "    6  title_id:  (100, 2)\n",
      "    6  content_id:  (100, 2)\n",
      "    6  title_content:  (100, 3)\n",
      "    6  title_content_com:  (700, 3)\n",
      "id_list_sel:  700 800\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  32.00\n",
      "    7  title_id:  (100, 2)\n",
      "    7  content_id:  (100, 2)\n",
      "    7  title_content:  (100, 3)\n",
      "    7  title_content_com:  (800, 3)\n",
      "id_list_sel:  800 900\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  17.00\n",
      "    8  title_id:  (100, 2)\n",
      "    8  content_id:  (100, 2)\n",
      "    8  title_content:  (100, 3)\n",
      "    8  title_content_com:  (900, 3)\n",
      "id_list_sel:  900 1000\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  27.00\n",
      "    9  title_id:  (100, 2)\n",
      "    9  content_id:  (100, 2)\n",
      "    9  title_content:  (100, 3)\n",
      "    9  title_content_com:  (1000, 3)\n",
      "id_list_sel:  1000 1100\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  62.00\n",
      "    10  title_id:  (100, 2)\n",
      "    10  content_id:  (100, 2)\n",
      "    10  title_content:  (100, 3)\n",
      "    10  title_content_com:  (1100, 3)\n",
      "id_list_sel:  1100 1200\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  21.00\n",
      "    11  title_id:  (100, 2)\n",
      "    11  content_id:  (100, 2)\n",
      "    11  title_content:  (100, 3)\n",
      "    11  title_content_com:  (1200, 3)\n",
      "id_list_sel:  1200 1300\n",
      "count:  [100]\n",
      "cor elapsed_time:  16.00\n",
      "tend elapsed_time:  73.00\n",
      "    12  title_id:  (100, 2)\n",
      "    12  content_id:  (100, 2)\n",
      "    12  title_content:  (100, 3)\n",
      "    12  title_content_com:  (1300, 3)\n",
      "id_list_sel:  1300 1400\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  24.00\n",
      "    13  title_id:  (100, 2)\n",
      "    13  content_id:  (100, 2)\n",
      "    13  title_content:  (100, 3)\n",
      "    13  title_content_com:  (1400, 3)\n",
      "id_list_sel:  1400 1500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  30.00\n",
      "    14  title_id:  (100, 2)\n",
      "    14  content_id:  (100, 2)\n",
      "    14  title_content:  (100, 3)\n",
      "    14  title_content_com:  (1500, 3)\n",
      "id_list_sel:  1500 1600\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  37.00\n",
      "    15  title_id:  (100, 2)\n",
      "    15  content_id:  (100, 2)\n",
      "    15  title_content:  (100, 3)\n",
      "    15  title_content_com:  (1600, 3)\n",
      "id_list_sel:  1600 1700\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  23.00\n",
      "    16  title_id:  (100, 2)\n",
      "    16  content_id:  (100, 2)\n",
      "    16  title_content:  (100, 3)\n",
      "    16  title_content_com:  (1700, 3)\n",
      "id_list_sel:  1700 1800\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  16.00\n",
      "    17  title_id:  (100, 2)\n",
      "    17  content_id:  (100, 2)\n",
      "    17  title_content:  (100, 3)\n",
      "    17  title_content_com:  (1800, 3)\n",
      "id_list_sel:  1800 1900\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  18.00\n",
      "    18  title_id:  (100, 2)\n",
      "    18  content_id:  (100, 2)\n",
      "    18  title_content:  (100, 3)\n",
      "    18  title_content_com:  (1900, 3)\n",
      "id_list_sel:  1900 2000\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  14.00\n",
      "    19  title_id:  (100, 2)\n",
      "    19  content_id:  (100, 2)\n",
      "    19  title_content:  (100, 3)\n",
      "    19  title_content_com:  (2000, 3)\n",
      "id_list_sel:  2000 2100\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    20  title_id:  (100, 2)\n",
      "    20  content_id:  (100, 2)\n",
      "    20  title_content:  (100, 3)\n",
      "    20  title_content_com:  (2100, 3)\n",
      "id_list_sel:  2100 2200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    21  title_id:  (100, 2)\n",
      "    21  content_id:  (100, 2)\n",
      "    21  title_content:  (100, 3)\n",
      "    21  title_content_com:  (2200, 3)\n",
      "id_list_sel:  2200 2300\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    22  title_id:  (100, 2)\n",
      "    22  content_id:  (100, 2)\n",
      "    22  title_content:  (100, 3)\n",
      "    22  title_content_com:  (2300, 3)\n",
      "id_list_sel:  2300 2400\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  45.00\n",
      "    23  title_id:  (100, 2)\n",
      "    23  content_id:  (100, 2)\n",
      "    23  title_content:  (100, 3)\n",
      "    23  title_content_com:  (2400, 3)\n",
      "id_list_sel:  2400 2500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  16.00\n",
      "    24  title_id:  (100, 2)\n",
      "    24  content_id:  (100, 2)\n",
      "    24  title_content:  (100, 3)\n",
      "    24  title_content_com:  (2500, 3)\n",
      "id_list_sel:  2500 2600\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  16.00\n",
      "    25  title_id:  (100, 2)\n",
      "    25  content_id:  (100, 2)\n",
      "    25  title_content:  (100, 3)\n",
      "    25  title_content_com:  (2600, 3)\n",
      "id_list_sel:  2600 2700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  27.00\n",
      "    26  title_id:  (100, 2)\n",
      "    26  content_id:  (100, 2)\n",
      "    26  title_content:  (100, 3)\n",
      "    26  title_content_com:  (2700, 3)\n",
      "id_list_sel:  2700 2800\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  47.00\n",
      "    27  title_id:  (100, 2)\n",
      "    27  content_id:  (100, 2)\n",
      "    27  title_content:  (100, 3)\n",
      "    27  title_content_com:  (2800, 3)\n",
      "id_list_sel:  2800 2900\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    28  title_id:  (100, 2)\n",
      "    28  content_id:  (100, 2)\n",
      "    28  title_content:  (100, 3)\n",
      "    28  title_content_com:  (2900, 3)\n",
      "id_list_sel:  2900 3000\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  14.00\n",
      "    29  title_id:  (100, 2)\n",
      "    29  content_id:  (100, 2)\n",
      "    29  title_content:  (100, 3)\n",
      "    29  title_content_com:  (3000, 3)\n",
      "id_list_sel:  3000 3100\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  22.00\n",
      "    30  title_id:  (100, 2)\n",
      "    30  content_id:  (100, 2)\n",
      "    30  title_content:  (100, 3)\n",
      "    30  title_content_com:  (3100, 3)\n",
      "id_list_sel:  3100 3200\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  16.00\n",
      "    31  title_id:  (100, 2)\n",
      "    31  content_id:  (100, 2)\n",
      "    31  title_content:  (100, 3)\n",
      "    31  title_content_com:  (3200, 3)\n",
      "id_list_sel:  3200 3300\n",
      "count:  [100]\n",
      "cor elapsed_time:  5.00\n",
      "tend elapsed_time:  26.00\n",
      "    32  title_id:  (100, 2)\n",
      "    32  content_id:  (100, 2)\n",
      "    32  title_content:  (100, 3)\n",
      "    32  title_content_com:  (3300, 3)\n",
      "id_list_sel:  3300 3400\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  19.00\n",
      "    33  title_id:  (100, 2)\n",
      "    33  content_id:  (100, 2)\n",
      "    33  title_content:  (100, 3)\n",
      "    33  title_content_com:  (3400, 3)\n",
      "id_list_sel:  3400 3500\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  18.00\n",
      "    34  title_id:  (100, 2)\n",
      "    34  content_id:  (100, 2)\n",
      "    34  title_content:  (100, 3)\n",
      "    34  title_content_com:  (3500, 3)\n",
      "id_list_sel:  3500 3600\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  17.00\n",
      "    35  title_id:  (100, 2)\n",
      "    35  content_id:  (100, 2)\n",
      "    35  title_content:  (100, 3)\n",
      "    35  title_content_com:  (3600, 3)\n",
      "id_list_sel:  3600 3700\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  18.00\n",
      "    36  title_id:  (100, 2)\n",
      "    36  content_id:  (100, 2)\n",
      "    36  title_content:  (100, 3)\n",
      "    36  title_content_com:  (3700, 3)\n",
      "id_list_sel:  3700 3800\n",
      "count:  [100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  19.00\n",
      "    37  title_id:  (100, 2)\n",
      "    37  content_id:  (100, 2)\n",
      "    37  title_content:  (100, 3)\n",
      "    37  title_content_com:  (3800, 3)\n",
      "id_list_sel:  3800 3900\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  22.00\n",
      "    38  title_id:  (100, 2)\n",
      "    38  content_id:  (100, 2)\n",
      "    38  title_content:  (100, 3)\n",
      "    38  title_content_com:  (3900, 3)\n",
      "id_list_sel:  3900 4000\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  27.00\n",
      "    39  title_id:  (100, 2)\n",
      "    39  content_id:  (100, 2)\n",
      "    39  title_content:  (100, 3)\n",
      "    39  title_content_com:  (4000, 3)\n",
      "id_list_sel:  4000 4100\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  22.00\n",
      "    40  title_id:  (100, 2)\n",
      "    40  content_id:  (100, 2)\n",
      "    40  title_content:  (100, 3)\n",
      "    40  title_content_com:  (4100, 3)\n",
      "id_list_sel:  4100 4200\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  18.00\n",
      "    41  title_id:  (100, 2)\n",
      "    41  content_id:  (100, 2)\n",
      "    41  title_content:  (100, 3)\n",
      "    41  title_content_com:  (4200, 3)\n",
      "id_list_sel:  4200 4300\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  17.00\n",
      "    42  title_id:  (100, 2)\n",
      "    42  content_id:  (100, 2)\n",
      "    42  title_content:  (100, 3)\n",
      "    42  title_content_com:  (4300, 3)\n",
      "id_list_sel:  4300 4400\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  22.00\n",
      "    43  title_id:  (100, 2)\n",
      "    43  content_id:  (100, 2)\n",
      "    43  title_content:  (100, 3)\n",
      "    43  title_content_com:  (4400, 3)\n",
      "id_list_sel:  4400 4500\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  20.00\n",
      "    44  title_id:  (100, 2)\n",
      "    44  content_id:  (100, 2)\n",
      "    44  title_content:  (100, 3)\n",
      "    44  title_content_com:  (4500, 3)\n",
      "id_list_sel:  4500 4600\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  14.00\n",
      "    45  title_id:  (100, 2)\n",
      "    45  content_id:  (100, 2)\n",
      "    45  title_content:  (100, 3)\n",
      "    45  title_content_com:  (4600, 3)\n",
      "id_list_sel:  4600 4700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  24.00\n",
      "    46  title_id:  (100, 2)\n",
      "    46  content_id:  (100, 2)\n",
      "    46  title_content:  (100, 3)\n",
      "    46  title_content_com:  (4700, 3)\n",
      "id_list_sel:  4700 4800\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  18.00\n",
      "    47  title_id:  (100, 2)\n",
      "    47  content_id:  (100, 2)\n",
      "    47  title_content:  (100, 3)\n",
      "    47  title_content_com:  (4800, 3)\n",
      "id_list_sel:  4800 4900\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  16.00\n",
      "    48  title_id:  (100, 2)\n",
      "    48  content_id:  (100, 2)\n",
      "    48  title_content:  (100, 3)\n",
      "    48  title_content_com:  (4900, 3)\n",
      "id_list_sel:  4900 5000\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  13.00\n",
      "    49  title_id:  (100, 2)\n",
      "    49  content_id:  (100, 2)\n",
      "    49  title_content:  (100, 3)\n",
      "    49  title_content_com:  (5000, 3)\n",
      "id_list_sel:  5000 5100\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  14.00\n",
      "    50  title_id:  (100, 2)\n",
      "    50  content_id:  (100, 2)\n",
      "    50  title_content:  (100, 3)\n",
      "    50  title_content_com:  (5100, 3)\n",
      "id_list_sel:  5100 5200\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    51  title_id:  (100, 2)\n",
      "    51  content_id:  (100, 2)\n",
      "    51  title_content:  (100, 3)\n",
      "    51  title_content_com:  (5200, 3)\n",
      "id_list_sel:  5200 5300\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  18.00\n",
      "    52  title_id:  (100, 2)\n",
      "    52  content_id:  (100, 2)\n",
      "    52  title_content:  (100, 3)\n",
      "    52  title_content_com:  (5300, 3)\n",
      "id_list_sel:  5300 5400\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    53  title_id:  (100, 2)\n",
      "    53  content_id:  (100, 2)\n",
      "    53  title_content:  (100, 3)\n",
      "    53  title_content_com:  (5400, 3)\n",
      "id_list_sel:  5400 5500\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  21.00\n",
      "    54  title_id:  (100, 2)\n",
      "    54  content_id:  (100, 2)\n",
      "    54  title_content:  (100, 3)\n",
      "    54  title_content_com:  (5500, 3)\n",
      "id_list_sel:  5500 5600\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  20.00\n",
      "    55  title_id:  (100, 2)\n",
      "    55  content_id:  (100, 2)\n",
      "    55  title_content:  (100, 3)\n",
      "    55  title_content_com:  (5600, 3)\n",
      "id_list_sel:  5600 5700\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  32.00\n",
      "    56  title_id:  (100, 2)\n",
      "    56  content_id:  (100, 2)\n",
      "    56  title_content:  (100, 3)\n",
      "    56  title_content_com:  (5700, 3)\n",
      "id_list_sel:  5700 5800\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  16.00\n",
      "    57  title_id:  (100, 2)\n",
      "    57  content_id:  (100, 2)\n",
      "    57  title_content:  (100, 3)\n",
      "    57  title_content_com:  (5800, 3)\n",
      "id_list_sel:  5800 5900\n",
      "count:  [100]\n",
      "cor elapsed_time:  5.00\n",
      "tend elapsed_time:  23.00\n",
      "    58  title_id:  (100, 2)\n",
      "    58  content_id:  (100, 2)\n",
      "    58  title_content:  (100, 3)\n",
      "    58  title_content_com:  (5900, 3)\n",
      "id_list_sel:  5900 6000\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  13.00\n",
      "    59  title_id:  (100, 2)\n",
      "    59  content_id:  (100, 2)\n",
      "    59  title_content:  (100, 3)\n",
      "    59  title_content_com:  (6000, 3)\n",
      "id_list_sel:  6000 6100\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  12.00\n",
      "    60  title_id:  (100, 2)\n",
      "    60  content_id:  (100, 2)\n",
      "    60  title_content:  (100, 3)\n",
      "    60  title_content_com:  (6100, 3)\n",
      "id_list_sel:  6100 6200\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  9.00\n",
      "    61  title_id:  (100, 2)\n",
      "    61  content_id:  (100, 2)\n",
      "    61  title_content:  (100, 3)\n",
      "    61  title_content_com:  (6200, 3)\n",
      "id_list_sel:  6200 6300\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  7.00\n",
      "    62  title_id:  (100, 2)\n",
      "    62  content_id:  (100, 2)\n",
      "    62  title_content:  (100, 3)\n",
      "    62  title_content_com:  (6300, 3)\n",
      "id_list_sel:  6300 6400\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  14.00\n",
      "    63  title_id:  (100, 2)\n",
      "    63  content_id:  (100, 2)\n",
      "    63  title_content:  (100, 3)\n",
      "    63  title_content_com:  (6400, 3)\n",
      "id_list_sel:  6400 6500\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  10.00\n",
      "    64  title_id:  (100, 2)\n",
      "    64  content_id:  (100, 2)\n",
      "    64  title_content:  (100, 3)\n",
      "    64  title_content_com:  (6500, 3)\n",
      "id_list_sel:  6500 6600\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  9.00\n",
      "    65  title_id:  (100, 2)\n",
      "    65  content_id:  (100, 2)\n",
      "    65  title_content:  (100, 3)\n",
      "    65  title_content_com:  (6600, 3)\n",
      "id_list_sel:  6600 6700\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  9.00\n",
      "    66  title_id:  (100, 2)\n",
      "    66  content_id:  (100, 2)\n",
      "    66  title_content:  (100, 3)\n",
      "    66  title_content_com:  (6700, 3)\n",
      "id_list_sel:  6700 6800\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  9.00\n",
      "    67  title_id:  (100, 2)\n",
      "    67  content_id:  (100, 2)\n",
      "    67  title_content:  (100, 3)\n",
      "    67  title_content_com:  (6800, 3)\n",
      "id_list_sel:  6800 6900\n",
      "count:  [100]\n",
      "cor elapsed_time:  1.00\n",
      "tend elapsed_time:  10.00\n",
      "    68  title_id:  (100, 2)\n",
      "    68  content_id:  (100, 2)\n",
      "    68  title_content:  (100, 3)\n",
      "    68  title_content_com:  (6900, 3)\n",
      "id_list_sel:  6900 7000\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  14.00\n",
      "    69  title_id:  (100, 2)\n",
      "    69  content_id:  (100, 2)\n",
      "    69  title_content:  (100, 3)\n",
      "    69  title_content_com:  (7000, 3)\n",
      "id_list_sel:  7000 7100\n",
      "count:  [100]\n",
      "cor elapsed_time:  2.00\n",
      "tend elapsed_time:  12.00\n",
      "    70  title_id:  (100, 2)\n",
      "    70  content_id:  (100, 2)\n",
      "    70  title_content:  (100, 3)\n",
      "    70  title_content_com:  (7100, 3)\n",
      "id_list_sel:  7100 7200\n",
      "count:  [100]\n",
      "cor elapsed_time:  4.00\n",
      "tend elapsed_time:  21.00\n",
      "    71  title_id:  (100, 2)\n",
      "    71  content_id:  (100, 2)\n",
      "    71  title_content:  (100, 3)\n",
      "    71  title_content_com:  (7200, 3)\n",
      "id_list_sel:  7200 7300\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  13.00\n",
      "    72  title_id:  (100, 2)\n",
      "    72  content_id:  (100, 2)\n",
      "    72  title_content:  (100, 3)\n",
      "    72  title_content_com:  (7300, 3)\n",
      "id_list_sel:  7300 7400\n",
      "count:  [100]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  19.00\n",
      "    73  title_id:  (100, 2)\n",
      "    73  content_id:  (100, 2)\n",
      "    73  title_content:  (100, 3)\n",
      "    73  title_content_com:  (7400, 3)\n",
      "id_list_sel:  7400 7500\n",
      "count:  [72]\n",
      "cor elapsed_time:  3.00\n",
      "tend elapsed_time:  17.00\n",
      "    74  title_id:  (72, 2)\n",
      "    74  content_id:  (72, 2)\n",
      "    74  title_content:  (72, 3)\n",
      "    74  title_content_com:  (7472, 3)\n",
      "data_null:  (8598, 10)\n",
      "combined_data:  (8598, 12)\n",
      "data_null_still:  (0, 12)\n",
      "update_data:  (16316, 13)\n"
     ]
    }
   ],
   "source": [
    "for filename in ['raw/人寿  8-10月.xlsx',]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo t1, db_docinfo_text t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "\n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])            \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)                \n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    title_content_com.index = range(title_content_com.shape[0])\n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "    data_null['八大分险类型'] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null['文章倾向性'] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T09:44:33.428059Z",
     "start_time": "2018-11-06T09:44:27.807738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_null:  (8598, 11)\n",
      "combined_data:  (8598, 13)\n",
      "data_null_still:  (0, 13)\n",
      "update_data:  (16316, 13)\n"
     ]
    }
   ],
   "source": [
    "    title_content_com.index = range(title_content_com.shape[0])\n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "    data_null['八大分险类型'] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null['文章倾向性'] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in ['raw/同业  8-10月.xlsx', ]:\n",
    "    print(filename, '  ----------------')\n",
    "    data = pd.read_excel(filename)\n",
    "    data_null = data[data['八大分险类型'].isnull() | data['文章倾向性'].isnull()]\n",
    "    data_full = data[data['八大分险类型'].notnull() & data['文章倾向性'].notnull()]\n",
    "    print('总量：', data.shape)\n",
    "    print('缺失值数量：', data_null.shape)\n",
    "    print('无缺失值数量：', data_full.shape)\n",
    "\n",
    "    id_list = tuple(data_null['id'].unique().tolist())\n",
    "    print('id_list: ', len(id_list))\n",
    "    \n",
    "    chunksize = 100\n",
    "    loop = int(len(id_list) / chunksize) + 1\n",
    "    title_content_com = pd.DataFrame()\n",
    "    for i in range(loop):\n",
    "        print('id_list_sel: ', 0 + i * chunksize, chunksize + i * chunksize)\n",
    "        id_list_sel = id_list[0 + i * chunksize:chunksize + i * chunksize]\n",
    "        sql_count = \"select count(t1.id) \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "        count = pd.read_sql_query(sql_count, engine)\n",
    "        print('count: ', list(count.values)[0])\n",
    "\n",
    "        sql_title = \"select t1.id, t1.title \\\n",
    "                            from db_docinfo t1 \\\n",
    "                                where t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        sql_content = \"select t1.id, t2.text as content \\\n",
    "                            from db_docinfo t1, db_docinfo_text t2 \\\n",
    "                                where t1.urlhash = t2.urlhash \\\n",
    "                                    and t1.id in {0}\".format(id_list_sel)\n",
    "\n",
    "        title_id = pd.read_sql_query(sql_title, engine)\n",
    "        content_id = pd.read_sql_query(sql_content, engine)\n",
    "        title_content = pd.merge(title_id, content_id, on = 'id', how = 'left')\n",
    "        \n",
    "        data = {\"types\":3, \"record\":title_content.iloc[:,[0, 1, 2]].to_dict(orient = 'records')}\n",
    "\n",
    "        # 相关性模型\n",
    "        result = requests.post(url_cor, data = json.dumps(data),\n",
    "                               headers=headers, allow_redirects=True)\n",
    "        json_data = json.loads(result.text)\n",
    "        cor_elapsed_time = json_data['elapsed_time']\n",
    "        print('cor elapsed_time: ', cor_elapsed_time)\n",
    "        cor_list = [[j['cor'], j['id']] for j in json_data['docs']]\n",
    "        cor_list = pd.DataFrame(cor_list, columns = ['八大分险类型', 'id'])\n",
    "\n",
    "        # 倾向性模型\n",
    "        try :\n",
    "            result = requests.post(url_tend, data = json.dumps(data),\n",
    "                                   headers=headers, allow_redirects=True)\n",
    "            json_data = json.loads(result.text)\n",
    "            tend_elapsed_time = json_data['elapsed_time'] \n",
    "            print('tend elapsed_time: ', tend_elapsed_time)\n",
    "            tendency_list = [[j['tendency'], j['id']] for j in json_data['docs']]            \n",
    "        except Exception as e:\n",
    "            print('error: ', e)\n",
    "            tendency_list = []\n",
    "            for index in range(len(data['record'])):\n",
    "#                 print(index, '.................')\n",
    "                data_sel = {\"types\":3, \"record\":[data['record'][index]]}\n",
    "#                 print('data_sel: ', data_sel)\n",
    "                try :\n",
    "                    result = requests.post(url_tend, data = json.dumps(data_sel),\n",
    "                                           headers=headers, allow_redirects=True)\n",
    "                    json_data = json.loads(result.text) \n",
    "                    tendency_list.append([json_data['docs'][0]['tendency'], json_data['docs'][0]['id']])\n",
    "                except Exception as e1:\n",
    "                    print('error again...    ', e1)\n",
    "                    print(data['record'][index])\n",
    "                    tendency_list.append([0, data['record'][index]['id']])            \n",
    "\n",
    "        tendency_list = pd.DataFrame(tendency_list, columns = ['文章倾向性', 'id'])\n",
    "\n",
    "        cor_tend = pd.merge(cor_list, tendency_list, on = 'id', how = 'inner')\n",
    "        title_content_com = pd.concat([title_content_com, cor_tend], axis = 0)                \n",
    "        print('    %s  title_id: '%i, title_id.shape)\n",
    "        print('    %s  content_id: '%i, content_id.shape)\n",
    "        print('    %s  title_content: '%i, title_content.shape)\n",
    "        print('    %s  title_content_com: '%i, title_content_com.shape)\n",
    "    \n",
    "    title_content_com.index = range(title_content_com.shape[0])\n",
    "    data_null = data_null.drop(['八大分险类型', '文章倾向性'], axis = 1)\n",
    "    print('data_null: ', data_null.shape) \n",
    "    data_null = pd.merge(data_null, title_content_com, on = 'id', how = 'left')\n",
    "    print('combined_data: ', data_null.shape)    \n",
    "    data_null_still = data_null[data_null['八大分险类型'].isnull() | data_null['文章倾向性'].isnull()]\n",
    "    print('data_null_still: ', data_null_still.shape) \n",
    "    data_null[''] = data_null['八大分险类型'].apply(lambda x: class_name_dict[x])\n",
    "    data_null[''] = data_null['文章倾向性'].apply(lambda x: '非负' if x == 0 else '负面')\n",
    "    \n",
    "    update_data = pd.concat([data_null, data_full], axis = 0)\n",
    "    print('update_data: ', update_data.shape) \n",
    "    \n",
    "    writer = pd.ExcelWriter('result/{0}'.format(filename.split('/')[1]),\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False})\n",
    "\n",
    "    update_data.to_excel(writer, sheet_name='Sheet1', index = False)\n",
    "    writer.save()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本文件说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优浪公司项目\n",
    "- 预处理及特征值计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T06:24:17.679771Z",
     "start_time": "2018-08-13T06:24:17.673771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['youlang_model.pkl.z']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "file_name = os.listdir(r\"model/\")\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:48.397301Z",
     "start_time": "2018-08-13T05:58:42.899987Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import joblib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_cor\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from jieba import analyse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:48.423303Z",
     "start_time": "2018-08-13T05:58:48.400301Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getkeywords(X, N = 1000):\n",
    "    '''\n",
    "    训练时生成，合并所有记录，取N个关键词\n",
    "    '''\n",
    "    textrank = analyse.textrank\n",
    "\n",
    "    text_combined = ' '.join(X)\n",
    "    keywords = textrank(text_combined, topK = N)\n",
    "    print('keywords num: ', len(keywords))\n",
    "    if len(keywords) < N : \n",
    "        N  = len(keywords)\n",
    "\n",
    "    if keywords:\n",
    "        f = open(\"corpus/keywords.txt\",\"w+\", encoding='UTF-8')\n",
    "        for content in keywords:\n",
    "            content = content.strip()\n",
    "            if content != ':AB:':\n",
    "                f.write(content + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:48.591312Z",
     "start_time": "2018-08-13T05:58:48.426303Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Statskeywords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, topk = 100):\n",
    "        self.topk = topk\n",
    "#         print(self.topk)\n",
    "        self.keywords = set()\n",
    "        f = open(\"corpus/keywords.txt\",\"r+\", encoding='UTF-8')\n",
    "        num = 0\n",
    "        for content in f:\n",
    "            if num < topk:\n",
    "                self.keywords.add(content.strip().replace('\\n', ''))\n",
    "            num += 1\n",
    "        f.close() \n",
    "        \n",
    "        #初始化字典liwc\n",
    "        self.liwc = {} \n",
    "        f2 = open(\"corpus/scliwc.txt\",'r', encoding = 'gb18030')\n",
    "        for ii in f2:     #ii在scliwc.txt中循环\n",
    "            i = ii.strip().split() \n",
    "            self.liwc[i[0]] = i[1:len(i)]\n",
    "        f2.close      \n",
    "        \n",
    "        self.category = set()\n",
    "        for i in list(self.liwc.values()):\n",
    "            for j in i:\n",
    "                self.category.add(j)        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本中关键词的词频\n",
    "        '''                        \n",
    "        data = []\n",
    "        for x in X:\n",
    "            words = x.split()\n",
    "            word_tf = []\n",
    "            keycnt = 0\n",
    "            for kw in self.keywords:\n",
    "                word_tf.append(words.count(kw)) # 各个关键词的词频\n",
    "                if kw in words:keycnt+=1\n",
    "            word_tf.append(keycnt) # 关键词的个数\n",
    "            \n",
    "            psy = []\n",
    "            for w in words:\n",
    "                if w in self.liwc: #是否liwc字典包含分词结果列表words的哪些分词\n",
    "                    psy += self.liwc[w]  \n",
    "            cat_tf = []\n",
    "            for cat in self.category:\n",
    "                cat_tf.append(psy.count(cat))                \n",
    "                \n",
    "            data.append(word_tf + cat_tf)            \n",
    "        return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:48.725320Z",
     "start_time": "2018-08-13T05:58:48.599313Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StatsFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neg = set()\n",
    "        f = open(\"corpus/neg_words.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.neg.add(content)\n",
    "        f.close()       \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def getcnt(self,x): \n",
    "        '''词个数'''\n",
    "        return len(list(set(x.split())))\n",
    "\n",
    "    def getnegcnt(self,x):\n",
    "        '''负面词个数'''\n",
    "        negcnt = 0\n",
    "        words = x.split()\n",
    "        for w in words:\n",
    "            if w in self.neg:\n",
    "                negcnt = negcnt+1\n",
    "        return negcnt\n",
    "\n",
    "    def getrepcnt(self,x):\n",
    "        '''重复词个数'''\n",
    "        repcnt =0\n",
    "        words = x.split()        \n",
    "        for w in list(set(words)):\n",
    "            if words.count(w)>1: # 记录重复词汇（词频大于1）\n",
    "                repcnt += 1\n",
    "        return repcnt\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本长度、词个数、词比例、\n",
    "        负面词个数、负面词比例、\n",
    "        重复词个数、重复词比例\n",
    "        '''\n",
    "        data = []\n",
    "        for x in X:\n",
    "            if len(x) == 0:\n",
    "                length  = 1\n",
    "            else :\n",
    "                length = len(x)\n",
    "            data.append([len(x),self.getcnt(x),self.getcnt(x)/length,\n",
    "                         self.getnegcnt(x),self.getnegcnt(x)/length,\n",
    "                         self.getrepcnt(x),self.getrepcnt(x)/length])            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理后数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:48.922331Z",
     "start_time": "2018-08-13T05:58:48.729320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 诈骗电话\n",
    "corpus_pos = []\n",
    "label_pos = []\n",
    "\n",
    "filename = 'data/pos_pre_20180723.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_pos.append(f)\n",
    "    label_pos.append(1)\n",
    "fid.close()\n",
    "print(len(corpus_pos))\n",
    "print(len(label_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:49.025337Z",
     "start_time": "2018-08-13T05:58:48.926331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 非诈骗电话\n",
    "corpus_neg = []\n",
    "label_neg = []\n",
    "\n",
    "filename = 'data/neg_pre_20180723.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_neg.append(f)\n",
    "    label_neg.append(0)\n",
    "fid.close()\n",
    "print(len(corpus_neg))\n",
    "print(len(label_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:49.423360Z",
     "start_time": "2018-08-13T05:58:49.028337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111\n",
      "6111\n"
     ]
    }
   ],
   "source": [
    "folder = '20180703'\n",
    "\n",
    "# 相关数据\n",
    "corpus_cor = []\n",
    "label_cor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_cor_0703.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:    \n",
    "    corpus_cor.append(f)\n",
    "    label_cor.append(1)\n",
    "fid.close()\n",
    "print(len(corpus_cor))\n",
    "print(len(label_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:49.790381Z",
     "start_time": "2018-08-13T05:58:49.428360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8949\n",
      "8949\n"
     ]
    }
   ],
   "source": [
    "# 不相关数据\n",
    "corpus_uncor = []\n",
    "label_uncor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_uncor_0703.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_uncor.append(f)\n",
    "    label_uncor.append(0)\n",
    "fid.close()\n",
    "print(len(corpus_uncor))\n",
    "print(len(label_uncor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:58:49.839384Z",
     "start_time": "2018-08-13T05:58:49.795381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集： 1800\n",
      "训练集-各类数量： Counter({1: 901, 0: 899})\n",
      "测试集： 200\n",
      "测试集-各类数量： Counter({0: 101, 1: 99})\n"
     ]
    }
   ],
   "source": [
    "# corpus = corpus_pos + corpus_neg\n",
    "# label = label_pos + label_neg\n",
    "corpus = corpus_cor[:1000] + corpus_uncor[:1000]\n",
    "label = label_cor[:1000] + label_uncor[:1000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, label, test_size=0.1, random_state=42)\n",
    "print('训练集：',len(y_train))\n",
    "print('训练集-各类数量：',Counter(y_train))\n",
    "print('测试集：',len(y_test))\n",
    "print('测试集-各类数量：',Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T05:34:56.904850Z",
     "start_time": "2018-07-25T05:34:56.891421Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0: getkeywords(corpus, N = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型:xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:59:02.216092Z",
     "start_time": "2018-08-13T05:59:01.742064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_features = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=200))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = 100)),\n",
    "                                    ('len_stats', StatsFeatures())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:59:44.801527Z",
     "start_time": "2018-08-13T05:59:03.589170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9872222222222222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0...tate=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', XGBClassifier(nthread = 4,# cpu 线程数\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline\n",
    "# clf_xgb = GridSearchCV(pipeline, param_grid=param_grid, verbose=10, cv = 10)\n",
    "# clf_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T05:59:46.598630Z",
     "start_time": "2018-08-13T05:59:45.364560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.965\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.97       101\n",
      "          1       0.99      0.94      0.96        99\n",
      "\n",
      "avg / total       0.97      0.96      0.96       200\n",
      "\n",
      "confusion_matrix: \n",
      "[[100   1]\n",
      " [  6  93]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T06:00:37.816560Z",
     "start_time": "2018-08-13T06:00:35.793444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/youlang_model.pkl.z']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"model/youlang_model.pkl.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T07:16:27.728093Z",
     "start_time": "2018-07-25T07:16:27.712938Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_1_best_para = {}\n",
    "score_2_best_para = {}\n",
    "cv = 5\n",
    "score_1 = 'roc_auc'\n",
    "score_2 = 'recall_macro'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topk、chi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T07:16:29.607880Z",
     "start_time": "2018-07-25T07:16:29.543192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf_idf__chi__k': [100, 200], 'features__tf__topk': [50, 100, 200]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0,\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [100, 200],# [2000, 5000, 20000, 40000],\n",
    "                   features__tf__topk=[50,100, 200]) \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T07:27:44.756833Z",
     "start_time": "2018-07-25T07:16:30.411289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9499219620958752, total= 1.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9600000000000001, total= 1.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9567607973421925, total= 1.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9584274640088594, total= 1.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9482552954292085, total= 1.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9566666666666667, total= 1.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9600000000000001, total= 1.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9584274640088594, total= 1.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9499219620958752, total= 1.3min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9482552954292085, total= 1.3min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9566666666666667, total= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  6.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9567607973421925, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9584274640088594, total= 1.3min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:  8.2min remaining:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9600000000000001, total= 1.3min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9499219620958752, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:  8.3min remaining:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9567607973421925, total= 1.3min\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9482552954292085, total= 1.1min\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9566666666666667, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_chi__k:  100\n",
      "score_1_topk:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.95612, std: 0.00443, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 100},\n",
       "  mean: 0.95390, std: 0.00399, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 200},\n",
       "  mean: 0.95612, std: 0.00443, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 100},\n",
       "  mean: 0.95390, std: 0.00399, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 200},\n",
       "  mean: 0.95612, std: 0.00443, params: {'features__tf__topk': 200, 'features__tf_idf__chi__k': 100},\n",
       "  mean: 0.95390, std: 0.00399, params: {'features__tf__topk': 200, 'features__tf_idf__chi__k': 200}],\n",
       " {'features__tf__topk': 50, 'features__tf_idf__chi__k': 100},\n",
       " 0.9561212006470854)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_1_best_para['topk'] = clf.best_params_['features__tf__topk']\n",
    "print('score_1_chi__k: ', score_1_best_para['chi__k'])\n",
    "print('score_1_topk: ', score_1_best_para['topk'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T07:27:44.829706Z",
     "start_time": "2018-07-25T07:27:44.760751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf_idf__chi__k': [100, 200, 300],\n",
       " 'features__tf__topk': [50, 100, 200]}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [100, 200, 300],# [2000, 5000, 20000, 40000],\n",
    "                   features__tf__topk=[50,100, 200]) \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T08:29:22.480874Z",
     "start_time": "2018-07-25T07:27:44.867507Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.95, total= 1.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9833333333333334, total= 1.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9611111111111111, total= 1.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9559218559218559, total= 1.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9388888888888889, total= 1.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  5.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.961111111111111, total= 1.6min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=100 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9722222222222222, total= 1.6min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9722222222222223, total= 1.6min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9722222222222222, total= 1.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=100, score=0.9609862671660424, total= 1.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  8.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9503663003663003, total= 1.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9833333333333334, total= 1.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9555555555555556, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.95, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9666666666666667, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9333333333333333, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=200 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9777777777777777, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 13.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.9666666666666667, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.95, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=200, score=0.949812734082397, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.9559218559218559, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.9611111111111111, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.9444444444444444, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.9833333333333334, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 15.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.961111111111111, total= 1.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.9722222222222223, total= 1.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=300 .............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.95, total= 1.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.9722222222222222, total= 1.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.949812734082397, total= 1.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9559218559218559, total= 1.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=300, score=0.9555555555555555, total= 1.7min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9833333333333334, total= 1.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.95, total= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 23.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9611111111111111, total= 1.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9722222222222223, total= 1.7min\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9388888888888889, total= 1.7min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9722222222222222, total= 1.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.961111111111111, total= 1.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9609862671660424, total= 1.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=100, score=0.9722222222222222, total= 1.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9503663003663003, total= 1.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9833333333333334, total= 1.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 29.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9555555555555556, total= 1.6min\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.95, total= 1.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9666666666666667, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9333333333333333, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.95, total= 1.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9777777777777777, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.949812734082397, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=200, score=0.9666666666666667, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.9559218559218559, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.9833333333333334, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.9611111111111111, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 36.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.9444444444444444, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.9722222222222223, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.95, total= 1.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.9722222222222222, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.961111111111111, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.9555555555555555, total= 1.5min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=300, score=0.949812734082397, total= 1.5min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9559218559218559, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9833333333333334, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.95, total= 1.8min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9611111111111111, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 42.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9722222222222223, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9388888888888889, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=100 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9722222222222222, total= 1.6min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.961111111111111, total= 1.6min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9609862671660424, total= 1.6min\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=100, score=0.9722222222222222, total= 1.5min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9503663003663003, total= 1.6min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9833333333333334, total= 1.6min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.95, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9555555555555556, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9666666666666667, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9333333333333333, total= 1.7min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=200 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.95, total= 1.3min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 53.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9666666666666667, total= 1.3min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.9777777777777777, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=200, score=0.949812734082397, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.9833333333333334, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.9559218559218559, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.9611111111111111, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.9444444444444444, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.95, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.9722222222222223, total= 1.4min\n",
      "[CV] features__tf__topk=200, features__tf_idf__chi__k=300 ............\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.9722222222222222, total= 1.4min\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.961111111111111, total= 1.4min\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.9555555555555555, total= 1.2min\n",
      "[CV]  features__tf__topk=200, features__tf_idf__chi__k=300, score=0.949812734082397, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 60.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_2_chi__k:  100\n",
      "score_2_topk:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.96280, std: 0.01216, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 100},\n",
       "  mean: 0.95835, std: 0.01432, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 200},\n",
       "  mean: 0.96058, std: 0.01151, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 300},\n",
       "  mean: 0.96280, std: 0.01216, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 100},\n",
       "  mean: 0.95835, std: 0.01432, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 200},\n",
       "  mean: 0.96058, std: 0.01151, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 300},\n",
       "  mean: 0.96280, std: 0.01216, params: {'features__tf__topk': 200, 'features__tf_idf__chi__k': 100},\n",
       "  mean: 0.95835, std: 0.01432, params: {'features__tf__topk': 200, 'features__tf_idf__chi__k': 200},\n",
       "  mean: 0.96058, std: 0.01151, params: {'features__tf__topk': 200, 'features__tf_idf__chi__k': 300}],\n",
       " {'features__tf__topk': 50, 'features__tf_idf__chi__k': 100},\n",
       " 0.9627991098580987)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_2_best_para['topk'] = clf.best_params_['features__tf__topk']\n",
    "print('score_2_chi__k: ', score_2_best_para['chi__k'])\n",
    "print('score_2_topk: ', score_2_best_para['topk'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T08:29:22.558003Z",
     "start_time": "2018-07-25T08:29:22.492936Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_features_1 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_1_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = score_1_best_para['topk'])),\n",
    "                                    ('len_stats', StatsFeatures())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T08:29:22.650948Z",
     "start_time": "2018-07-25T08:29:22.560993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_features_2 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_2_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = score_2_best_para['topk'])),\n",
    "                                    ('len_stats', StatsFeatures())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### n_estimators、learning_rate\n",
    "- 选择对应于此学习速率的理想决策树数量。\n",
    "- learning_rate\n",
    "    - 通过减少每一步的权重，可以提高模型的鲁棒性。 取值范围为：[0,1]。缺省值为0.3。\n",
    "    - 选择对应于此学习速率的理想决策树数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T08:29:22.672205Z",
     "start_time": "2018-07-25T08:29:22.654107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(10, 71, 10),\n",
       " 'classifier__learning_rate': [0.01, 0.1, 0.3]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(10,71,10), \n",
    "                    classifier__learning_rate=[0.01, 0.1, 0.3]) # 理想的学习速率有时候会在0.05到0.3之间波动\n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:41.369Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "score_1_best_para['learning_rate'] = clf.best_params_['classifier__learning_rate']\n",
    "print('score_1_n_estimators: ', score_1_best_para['n_estimators'])\n",
    "print('score_1_learning_rate: ', score_1_best_para['learning_rate'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:41.922Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(10,71,10), # 迭代次数/分类器个数\n",
    "                   classifier__learning_rate=[0.01, 0.1, 0.3]) \n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:42.589Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "score_2_best_para['learning_rate'] = clf.best_params_['classifier__learning_rate']\n",
    "print('score_2_n_estimators: ', score_2_best_para['n_estimators'])\n",
    "print('score_2_learning_rate: ', score_2_best_para['learning_rate'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### max_depth、min_child_weight\n",
    "- max_depth：树的最大深度\n",
    "    - 用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 \n",
    "    - 需要使用CV函数来进行调优。 典型值：3-10。起始值在4-6之间都是不错的选择。取值范围为：[1,∞]\n",
    "    - 树的深度越大，则对数据的拟合程度越高（过拟合程度也越高）。即该参数也是控制过拟合\n",
    "- min_child_weight：最小样本权重的和\n",
    "    - 用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。\n",
    "    - 这个参数需要使用CV来调整。取值范围为: [0,∞]\n",
    "    - 如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。该成熟越大算法越conservative。即调大这个参数能够控制过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:43.570Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_child_weight=[4, 5, 6]) \n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:44.246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "score_1_best_para['min_child_weight'] = clf.best_params_['classifier__min_child_weight']\n",
    "print('score_1_max_depth: ', score_1_best_para['max_depth'])\n",
    "print('score_1_min_child_weight: ', score_1_best_para['min_child_weight'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:44.808Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_2_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_2_best_para['learning_rate'],\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_child_weight=[4, 5, 6]) \n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:45.329Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "score_2_best_para['min_child_weight'] = clf.best_params_['classifier__min_child_weight']\n",
    "print('score_2_max_depth: ', score_2_best_para['max_depth'])\n",
    "print('score_2_min_child_weight: ', score_2_best_para['min_child_weight'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma\n",
    "- 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。\n",
    "- range: [0,∞]\n",
    "- 模型在默认情况下，对于一个节点的划分只有在其loss function 得到结果大于0的情况下才进行，而gamma 给定了所需的最低loss function的值gamma值使得算法更conservation，且其值依赖于loss function ，在模型中应该进行调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:46.398Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],\n",
    "                                                  max_depth = score_1_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_1_best_para['min_child_weight'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_2 = dict(classifier__gamma=[i/10.0 for i in range(0,5)]) \n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:47.015Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['gamma'] = clf.best_params_['classifier__gamma']\n",
    "print('score_1_gamma: ', score_1_best_para['gamma'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:47.622Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_2_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_2_best_para['learning_rate'],\n",
    "                                                  max_depth = score_2_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_2_best_para['min_child_weight'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_2 = dict(classifier__gamma=[i/10.0 for i in range(0,5)]) \n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:48.199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['gamma'] = clf.best_params_['classifier__gamma']\n",
    "print('score_2_gamma: ', score_2_best_para['gamma'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsample、colsample_bytree\n",
    "- subsample\n",
    "    - 控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1\n",
    "    - 如果设置为0.5则意味着XGBoost将随机的从整个样本集合中抽取出50%的子样本建立树模型，这能够防止过拟合。\n",
    "- colsample_bytree\n",
    "    - 和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 \n",
    "    - 典型值：0.5-1。取值范围为：(0,1]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:49.274Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],\n",
    "                                                  max_depth = score_1_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_1_best_para['min_child_weight'],      \n",
    "                                                  gamma = score_1_best_para['gamma'],\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_3 = dict(classifier__subsample=[0.6,0.8,1.0], \n",
    "                    classifier__colsample_bytree=[0.6,0.8,1.0]) # 内部节点再划分所需最小样本数)\n",
    "param_grid_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:49.807Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_3, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['subsample'] = clf.best_params_['classifier__subsample']\n",
    "score_1_best_para['colsample_bytree'] = clf.best_params_['classifier__colsample_bytree']\n",
    "print('score_1_subsample: ', score_1_best_para['subsample'])\n",
    "print('score_1_colsample_bytree: ', score_1_best_para['colsample_bytree'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:50.334Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_2_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_2_best_para['learning_rate'],\n",
    "                                                  max_depth = score_2_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_2_best_para['min_child_weight'],      \n",
    "                                                  gamma = score_2_best_para['gamma'],\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_3 = dict(classifier__subsample=[0.6,0.8,1.0], \n",
    "                    classifier__colsample_bytree=[0.6,0.8,1.0]) # 内部节点再划分所需最小样本数)\n",
    "param_grid_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:50.877Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_3, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['subsample'] = clf.best_params_['classifier__subsample']\n",
    "score_2_best_para['colsample_bytree'] = clf.best_params_['classifier__colsample_bytree']\n",
    "print('score_2_subsample: ', score_2_best_para['subsample'])\n",
    "print('score_2_colsample_bytree: ', score_2_best_para['colsample_bytree'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:51.844Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_1_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:52.018Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_2_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:52.224Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],\n",
    "                                                  max_depth = score_1_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_1_best_para['min_child_weight'],      \n",
    "                                                  gamma = score_1_best_para['gamma'],\n",
    "                                                  subsample = score_1_best_para['subsample'],      \n",
    "                                                  colsample_bytree = score_1_best_para['colsample_bytree'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:52.396Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:52.593Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_2_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_2_best_para['learning_rate'],\n",
    "                                                  max_depth = score_2_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_2_best_para['min_child_weight'],      \n",
    "                                                  gamma = score_2_best_para['gamma'],\n",
    "                                                  subsample = score_2_best_para['subsample'],      \n",
    "                                                  colsample_bytree = score_2_best_para['colsample_bytree'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-25T07:16:52.781Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本文件说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优浪公司项目\n",
    "- 预处理及特征值计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:29.756132Z",
     "start_time": "2018-07-25T09:27:29.746332Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##load packages, needed\n",
    "# encoding=utf-8\n",
    "\n",
    "import jieba\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import joblib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_cor\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from jieba import analyse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:30.188745Z",
     "start_time": "2018-07-25T09:27:30.182504Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getkeywords(X, N = 1000):\n",
    "    '''\n",
    "    训练时生成，合并所有记录，取N个关键词\n",
    "    '''\n",
    "    textrank = analyse.textrank\n",
    "\n",
    "    text_combined = ' '.join(X)\n",
    "    keywords = textrank(text_combined, topK = N)\n",
    "    print('keywords num: ', len(keywords))\n",
    "    if len(keywords) < N : \n",
    "        N  = len(keywords)\n",
    "\n",
    "    if keywords:\n",
    "        f = open(\"corpus/keywords.txt\",\"w+\", encoding='UTF-8')\n",
    "        for content in keywords:\n",
    "            content = content.strip()\n",
    "            if content != ':AB:':\n",
    "                f.write(content + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:30.663804Z",
     "start_time": "2018-07-25T09:27:30.655266Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Statskeywords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, topk = 100):\n",
    "        self.topk = topk\n",
    "#         print(self.topk)\n",
    "        self.keywords = set()\n",
    "        f = open(\"corpus/keywords.txt\",\"r+\", encoding='UTF-8')\n",
    "        num = 0\n",
    "        for content in f:\n",
    "            if num < topk:\n",
    "                self.keywords.add(content.strip().replace('\\n', ''))\n",
    "            num += 1\n",
    "        f.close() \n",
    "        \n",
    "        #初始化字典liwc\n",
    "        self.liwc = {} \n",
    "        f2 = open(\"corpus/scliwc.txt\",'r', encoding = 'gb18030')\n",
    "        for ii in f2:     #ii在scliwc.txt中循环\n",
    "            i = ii.strip().split() \n",
    "            self.liwc[i[0]] = i[1:len(i)]\n",
    "        f2.close      \n",
    "        \n",
    "        self.category = set()\n",
    "        for i in list(self.liwc.values()):\n",
    "            for j in i:\n",
    "                self.category.add(j)        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本中关键词的词频\n",
    "        '''                        \n",
    "        data = []\n",
    "        for x in X:\n",
    "            words = x.split()\n",
    "            word_tf = []\n",
    "            keycnt = 0\n",
    "            for kw in self.keywords:\n",
    "                word_tf.append(words.count(kw)) # 各个关键词的词频\n",
    "                if kw in words:keycnt+=1\n",
    "            word_tf.append(keycnt) # 关键词的个数\n",
    "            \n",
    "            psy = []\n",
    "            for w in words:\n",
    "                if w in self.liwc: #是否liwc字典包含分词结果列表words的哪些分词\n",
    "                    psy += self.liwc[w]  \n",
    "            cat_tf = []\n",
    "            for cat in self.category:\n",
    "                cat_tf.append(psy.count(cat))                \n",
    "                \n",
    "            data.append(word_tf + cat_tf)            \n",
    "        return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:31.175153Z",
     "start_time": "2018-07-25T09:27:31.166615Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StatsFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neg = set()\n",
    "        f = open(\"corpus/neg_words.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.neg.add(content)\n",
    "        f.close()       \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def getcnt(self,x): \n",
    "        '''词个数'''\n",
    "        return len(list(set(x.split())))\n",
    "\n",
    "    def getnegcnt(self,x):\n",
    "        '''负面词个数'''\n",
    "        negcnt = 0\n",
    "        words = x.split()\n",
    "        for w in words:\n",
    "            if w in self.neg:\n",
    "                negcnt = negcnt+1\n",
    "        return negcnt\n",
    "\n",
    "    def getrepcnt(self,x):\n",
    "        '''重复词个数'''\n",
    "        repcnt =0\n",
    "        words = x.split()        \n",
    "        for w in list(set(words)):\n",
    "            if words.count(w)>1: # 记录重复词汇（词频大于1）\n",
    "                repcnt += 1\n",
    "        return repcnt\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本长度、词个数、词比例、\n",
    "        负面词个数、负面词比例、\n",
    "        重复词个数、重复词比例\n",
    "        '''\n",
    "        data = []\n",
    "        for x in X:\n",
    "            if len(x) == 0:\n",
    "                length  = 1\n",
    "            else :\n",
    "                length = len(x)\n",
    "            data.append([len(x),self.getcnt(x),self.getcnt(x)/length,\n",
    "                         self.getnegcnt(x),self.getnegcnt(x)/length,\n",
    "                         self.getrepcnt(x),self.getrepcnt(x)/length])            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理后数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:32.890858Z",
     "start_time": "2018-07-25T09:27:32.883125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 诈骗电话\n",
    "corpus_pos = []\n",
    "label_pos = []\n",
    "\n",
    "filename = 'data/pos_pre_20180723.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_pos.append(f)\n",
    "    label_pos.append(1)\n",
    "fid.close()\n",
    "print(len(corpus_pos))\n",
    "print(len(label_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:33.490140Z",
     "start_time": "2018-07-25T09:27:33.482743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 非诈骗电话\n",
    "corpus_neg = []\n",
    "label_neg = []\n",
    "\n",
    "filename = 'data/neg_pre_20180723.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_neg.append(f)\n",
    "    label_neg.append(0)\n",
    "fid.close()\n",
    "print(len(corpus_neg))\n",
    "print(len(label_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:35.053133Z",
     "start_time": "2018-07-25T09:27:34.967605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111\n",
      "6111\n"
     ]
    }
   ],
   "source": [
    "folder = '20180703'\n",
    "\n",
    "# 相关数据\n",
    "corpus_cor = []\n",
    "label_cor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_cor_0703.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:    \n",
    "    corpus_cor.append(f)\n",
    "    label_cor.append(1)\n",
    "fid.close()\n",
    "print(len(corpus_cor))\n",
    "print(len(label_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:35.540084Z",
     "start_time": "2018-07-25T09:27:35.421721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8949\n",
      "8949\n"
     ]
    }
   ],
   "source": [
    "# 不相关数据\n",
    "corpus_uncor = []\n",
    "label_uncor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_uncor_0703.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_uncor.append(f)\n",
    "    label_uncor.append(0)\n",
    "fid.close()\n",
    "print(len(corpus_uncor))\n",
    "print(len(label_uncor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:27:59.874364Z",
     "start_time": "2018-07-25T09:27:59.866109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集： 180\n",
      "训练集-各类数量： Counter({0: 90, 1: 90})\n",
      "测试集： 20\n",
      "测试集-各类数量： Counter({1: 10, 0: 10})\n"
     ]
    }
   ],
   "source": [
    "# corpus = corpus_pos + corpus_neg\n",
    "# label = label_pos + label_neg\n",
    "corpus = corpus_cor[:100] + corpus_uncor[:100]\n",
    "label = label_cor[:100] + label_uncor[:100]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, label, test_size=0.1, random_state=42)\n",
    "print('训练集：',len(y_train))\n",
    "print('训练集-各类数量：',Counter(y_train))\n",
    "print('测试集：',len(y_test))\n",
    "print('测试集-各类数量：',Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:28:04.796863Z",
     "start_time": "2018-07-25T09:28:04.793406Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0: getkeywords(corpus, N = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型:RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:28:54.335783Z",
     "start_time": "2018-07-25T09:28:54.304498Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_features = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=20000))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = 100)),\n",
    "                                    ('len_stats', StatsFeatures())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:28:58.639517Z",
     "start_time": "2018-07-25T09:28:55.029949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0...         presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "              warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline\n",
    "# clf_xgb = GridSearchCV(pipeline, param_grid=param_grid, verbose=10, cv = 10)\n",
    "# clf_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:28:58.875513Z",
     "start_time": "2018-07-25T09:28:58.642966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.9\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.90      0.90        10\n",
      "          1       0.90      0.90      0.90        10\n",
      "\n",
      "avg / total       0.90      0.90      0.90        20\n",
      "\n",
      "confusion_matrix: \n",
      "[[9 1]\n",
      " [1 9]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:37:23.324882Z",
     "start_time": "2018-07-25T09:37:23.320262Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_1_best_para = {}\n",
    "score_2_best_para = {}\n",
    "cv = 5\n",
    "score_1 = 'roc_auc'\n",
    "score_2 = 'recall_macro'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topk、chi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:38:24.176899Z",
     "start_time": "2018-07-25T09:38:24.169413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf__topk': [50, 100, 500, 1000]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [2000, 5000, 20000, 40000],\n",
    "                   features__tf__topk=[50,100, 500, 1000]) \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:39:04.795161Z",
     "start_time": "2018-07-25T09:38:24.722664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] features__tf__topk=50 ...........................................\n",
      "[CV] features__tf__topk=50 ...........................................\n",
      "[CV] features__tf__topk=50 ...........................................\n",
      "[CV] features__tf__topk=50 ...........................................\n",
      "[CV] .. features__tf__topk=50, score=0.9660493827160493, total=   4.0s\n",
      "[CV] features__tf__topk=50 ...........................................\n",
      "[CV] .. features__tf__topk=50, score=0.8117283950617283, total=   4.6s\n",
      "[CV] features__tf__topk=100 ..........................................\n",
      "[CV] .. features__tf__topk=50, score=0.8209876543209876, total=   3.7s\n",
      "[CV] features__tf__topk=100 ..........................................\n",
      "[CV] .. features__tf__topk=50, score=0.9012345679012346, total=   5.2s\n",
      "[CV] features__tf__topk=100 ..........................................\n",
      "[CV] .. features__tf__topk=50, score=0.9629629629629629, total=   3.6s\n",
      "[CV] features__tf__topk=100 ..........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   12.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . features__tf__topk=100, score=0.9012345679012346, total=   4.1s\n",
      "[CV] features__tf__topk=100 ..........................................\n",
      "[CV] . features__tf__topk=100, score=0.9660493827160493, total=   4.8s\n",
      "[CV] features__tf__topk=500 ..........................................\n",
      "[CV] . features__tf__topk=100, score=0.8117283950617283, total=   4.7s\n",
      "[CV] features__tf__topk=500 ..........................................\n",
      "[CV] . features__tf__topk=100, score=0.8209876543209876, total=   4.3s\n",
      "[CV] features__tf__topk=500 ..........................................\n",
      "[CV] . features__tf__topk=100, score=0.9629629629629629, total=   3.6s\n",
      "[CV] features__tf__topk=500 ..........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   21.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . features__tf__topk=500, score=0.9012345679012346, total=   4.0s\n",
      "[CV] features__tf__topk=500 ..........................................\n",
      "[CV] . features__tf__topk=500, score=0.9660493827160493, total=   5.9s\n",
      "[CV] features__tf__topk=1000 .........................................\n",
      "[CV] . features__tf__topk=500, score=0.8117283950617283, total=   4.0s\n",
      "[CV] features__tf__topk=1000 .........................................\n",
      "[CV] . features__tf__topk=500, score=0.8209876543209876, total=   5.5s\n",
      "[CV] features__tf__topk=1000 .........................................\n",
      "[CV] . features__tf__topk=500, score=0.9629629629629629, total=   4.4s\n",
      "[CV] features__tf__topk=1000 .........................................\n",
      "[CV]  features__tf__topk=1000, score=0.9012345679012346, total=   3.6s\n",
      "[CV] features__tf__topk=1000 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:   31.8s remaining:    8.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=1000, score=0.8117283950617283, total=   3.6s\n",
      "[CV]  features__tf__topk=1000, score=0.9660493827160493, total=   5.1s\n",
      "[CV]  features__tf__topk=1000, score=0.9629629629629629, total=   3.4s\n",
      "[CV]  features__tf__topk=1000, score=0.8209876543209876, total=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   37.8s finished\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'features__tf_idf__chi__k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-06cece9211bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                    cv = cv, scoring=score_1, n_jobs=-1, verbose=10)\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore_1_best_para\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chi__k'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features__tf_idf__chi__k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscore_1_best_para\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topk'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features__tf__topk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score_1_chi__k: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_1_best_para\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chi__k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'features__tf_idf__chi__k'"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_1_best_para['topk'] = clf.best_params_['features__tf__topk']\n",
    "print('score_1_chi__k: ', score_1_best_para['chi__k'])\n",
    "print('score_1_topk: ', score_1_best_para['topk'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:39:11.771057Z",
     "start_time": "2018-07-25T09:39:11.629246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.9\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.90      0.90        10\n",
      "          1       0.90      0.90      0.90        10\n",
      "\n",
      "avg / total       0.90      0.90      0.90        20\n",
      "\n",
      "confusion_matrix: \n",
      "[[9 1]\n",
      " [1 9]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T11:47:22.719962Z",
     "start_time": "2018-07-24T11:47:22.682681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf_idf__chi__k': [2000, 5000, 20000, 40000],\n",
       " 'features__tf__topk': [50, 100, 500, 1000]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', GradientBoostingClassifier(random_state=0))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [2000, 5000, 20000, 40000],\n",
    "                   features__tf__topk=[50,100, 500, 1000]) \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T21:22:07.041369Z",
     "start_time": "2018-07-24T11:47:22.722871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8627427274966499, total= 7.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8601406028613202, total= 7.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8703773589169377, total= 7.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8562142532704515, total= 7.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8606705188204906, total= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 28.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8612106037426304, total= 8.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8706733887278175, total= 8.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8597631741196098, total= 8.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8622384216443622, total= 7.9min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8682769375838684, total= 8.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 44.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8562142532704515, total= 8.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8749217554793762, total= 8.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8474734646560744, total= 7.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8615845956029404, total= 6.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.86145465885389, total= 7.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8517997934570632, total= 7.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8786300373087952, total= 7.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 72.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8600102455300476, total= 7.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8431799223878431, total= 7.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8624854930548, total= 7.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8649448729218594, total= 8.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8400049262976161, total= 7.9min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8298879380280799, total= 8.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8555182442494514, total= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 88.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8552380328254126, total= 7.6min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8514517889465631, total= 7.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.856100134908242, total= 7.8min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8422891052841548, total= 8.0min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8493181735755994, total= 7.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8667210517871388, total= 6.9min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8437911701764654, total= 7.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8439234033293439, total= 7.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8395168160750967, total= 7.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 130.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8495456733322335, total= 7.6min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8488135079984542, total= 7.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8574763345634012, total= 7.5min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8432250979045157, total= 7.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8551844195408552, total= 7.8min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.841992166249592, total= 7.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.857015921372357, total= 7.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8703773589169377, total= 7.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8627427274966499, total= 7.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 159.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8562142532704515, total= 7.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8601406028613202, total= 7.8min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8706733887278175, total= 7.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8612106037426304, total= 6.9min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8606705188204906, total= 7.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8597631741196098, total= 7.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8622384216443622, total= 8.2min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8749217554793762, total= 8.1min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8682769375838684, total= 8.2min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8562142532704515, total= 8.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.86145465885389, total= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 202.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8474734646560744, total= 8.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8517997934570632, total= 8.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8615845956029404, total= 7.9min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8600102455300476, total= 7.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8786300373087952, total= 7.1min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8431799223878431, total= 7.7min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8624854930548, total= 7.7min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8649448729218594, total= 7.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8400049262976161, total= 7.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8298879380280799, total= 7.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8555182442494514, total= 6.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 232.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8552380328254126, total= 8.1min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.856100134908242, total= 7.9min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8422891052841548, total= 7.9min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8514517889465631, total= 8.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8439234033293439, total= 7.7min\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8493181735755994, total= 8.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8437911701764654, total= 7.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8667210517871388, total= 7.9min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8395168160750967, total= 7.2min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8495456733322335, total= 7.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8574763345634012, total= 7.2min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8488135079984542, total= 7.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8432250979045157, total= 7.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 287.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8551844195408552, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.857015921372357, total= 7.2min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.841992166249592, total= 7.3min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8703773589169377, total= 7.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8562142532704515, total= 7.2min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8627427274966499, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8601406028613202, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8606705188204906, total= 7.3min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8706733887278175, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8612106037426304, total= 7.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8597631741196098, total= 7.1min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8749217554793762, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8682769375838684, total= 7.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 330.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8622384216443622, total= 7.8min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8562142532704515, total= 7.4min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8474734646560744, total= 7.8min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8517997934570632, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.86145465885389, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8615845956029404, total= 7.2min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8786300373087952, total= 7.2min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8600102455300476, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8431799223878431, total= 7.8min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8624854930548, total= 7.3min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8649448729218594, total= 7.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8555182442494514, total= 7.1min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8298879380280799, total= 7.4min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8400049262976161, total= 7.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8552380328254126, total= 7.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 386.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.856100134908242, total= 7.3min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8422891052841548, total= 7.4min\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8514517889465631, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8493181735755994, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8439234033293439, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8667210517871388, total= 7.7min\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8437911701764654, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8395168160750967, total= 7.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8495456733322335, total= 7.8min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8574763345634012, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8488135079984542, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8432250979045157, total= 7.5min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8551844195408552, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.857015921372357, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.841992166249592, total= 7.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 430.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8703773589169377, total= 7.6min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8562142532704515, total= 7.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8627427274966499, total= 7.1min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8601406028613202, total= 7.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8606705188204906, total= 7.6min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8706733887278175, total= 7.8min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=2000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8612106037426304, total= 8.0min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8597631741196098, total= 8.1min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8622384216443622, total= 6.9min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=2000, score=0.8682769375838684, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8749217554793762, total= 7.7min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8562142532704515, total= 7.6min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8474734646560744, total= 7.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.86145465885389, total= 7.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8517997934570632, total= 7.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8615845956029404, total= 7.5min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8786300373087952, total= 7.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 498.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8600102455300476, total= 7.3min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8431799223878431, total= 7.1min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=5000, score=0.8624854930548, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8649448729218594, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8400049262976161, total= 7.7min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8298879380280799, total= 7.8min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8555182442494514, total= 8.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8552380328254126, total= 7.1min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8514517889465631, total= 7.3min\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.856100134908242, total= 7.5min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=20000 .........\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8422891052841548, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8493181735755994, total= 7.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=20000, score=0.8439234033293439, total= 7.0min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8667210517871388, total= 7.3min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8437911701764654, total= 7.3min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8395168160750967, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8495456733322335, total= 7.2min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8488135079984542, total= 7.5min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8574763345634012, total= 7.4min\n",
      "[CV] features__tf__topk=1000, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8432250979045157, total= 7.4min\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.8551844195408552, total= 7.1min\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.841992166249592, total= 7.8min\n",
      "[CV]  features__tf__topk=1000, features__tf_idf__chi__k=40000, score=0.857015921372357, total= 7.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed: 570.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_2_chi__k:  2000\n",
      "score_2_topk:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.86323, std: 0.00463, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.85978, std: 0.01049, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.84887, std: 0.00953, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.85033, std: 0.00819, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.86323, std: 0.00463, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.85978, std: 0.01049, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.84887, std: 0.00953, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.85033, std: 0.00819, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.86323, std: 0.00463, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.85978, std: 0.01049, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.84887, std: 0.00953, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.85033, std: 0.00819, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.86323, std: 0.00463, params: {'features__tf__topk': 1000, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.85978, std: 0.01049, params: {'features__tf__topk': 1000, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.84887, std: 0.00953, params: {'features__tf__topk': 1000, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.85033, std: 0.00819, params: {'features__tf__topk': 1000, 'features__tf_idf__chi__k': 40000}],\n",
       " {'features__tf__topk': 50, 'features__tf_idf__chi__k': 2000},\n",
       " 0.8632307122292311)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_2_best_para['topk'] = clf.best_params_['features__tf__topk']\n",
    "print('score_2_chi__k: ', score_2_best_para['chi__k'])\n",
    "print('score_2_topk: ', score_2_best_para['topk'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T09:53:18.714625Z",
     "start_time": "2018-07-25T09:53:18.697156Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chi__k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-87e9b53c5fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                         \u001b[0;34m(\u001b[0m\u001b[0;34m'counts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                         \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                         \u001b[0;34m(\u001b[0m\u001b[0;34m'chi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_1_best_para\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chi__k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                                         ])),\n\u001b[1;32m      7\u001b[0m                                     \u001b[0;34m(\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatskeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_1_best_para\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chi__k'"
     ]
    }
   ],
   "source": [
    "combined_features_1 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_1_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = score_1_best_para['topk'])),\n",
    "                                    ('len_stats', StatsFeatures())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T21:22:07.067614Z",
     "start_time": "2018-07-24T21:22:07.059885Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_features_2 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_2_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = score_2_best_para['topk'])),\n",
    "                                    ('len_stats', StatsFeatures())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### n_estimators\n",
    "- n_estimators\n",
    "    - 弱学习器的最大迭代次数，或者说最大的弱学习器的个数。\n",
    "    - 一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。\n",
    "    - 在实际调参的过程中，我们常常将n_estimators和参数learning_rate一起考虑。\n",
    "- learning_rate    \n",
    "    - 每个弱学习器的权重缩减系数ν，也称作步长，取值范围为0<ν≤11。\n",
    "    - 对于同样的训练集拟合效果，较小的ν 意味着我们需要更多的弱学习器的迭代次数。\n",
    "    - 通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。\n",
    "    - 一般来说，可以从一个小一点的ν开始调参，默认是1。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T21:22:07.084791Z",
     "start_time": "2018-07-24T21:22:07.069776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(10, 71, 10)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', GradientBoostingClassifier(learning_rate = 0.1, random_state=0))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(10,71,10)) # 迭代次数/分类器个数\n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T01:34:22.122002Z",
     "start_time": "2018-07-24T21:22:07.087010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_n_estimators:  70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.94152, std: 0.00668, params: {'classifier__n_estimators': 10},\n",
       "  mean: 0.95314, std: 0.00413, params: {'classifier__n_estimators': 20},\n",
       "  mean: 0.95705, std: 0.00432, params: {'classifier__n_estimators': 30},\n",
       "  mean: 0.95916, std: 0.00407, params: {'classifier__n_estimators': 40},\n",
       "  mean: 0.96038, std: 0.00391, params: {'classifier__n_estimators': 50},\n",
       "  mean: 0.96138, std: 0.00365, params: {'classifier__n_estimators': 60},\n",
       "  mean: 0.96215, std: 0.00356, params: {'classifier__n_estimators': 70}],\n",
       " {'classifier__n_estimators': 70},\n",
       " 0.9621511471661051)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "print('score_1_n_estimators: ', score_1_best_para['n_estimators'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T01:34:22.150462Z",
     "start_time": "2018-07-25T01:34:22.134471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(10, 71, 10)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', GradientBoostingClassifier(learning_rate = 0.1, random_state=0))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(10,71,10)) # 迭代次数/分类器个数\n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T06:06:40.878826Z",
     "start_time": "2018-07-25T01:34:22.152969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_2_n_estimators:  70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.85901, std: 0.00820, params: {'classifier__n_estimators': 10},\n",
       "  mean: 0.87585, std: 0.00496, params: {'classifier__n_estimators': 20},\n",
       "  mean: 0.87948, std: 0.00652, params: {'classifier__n_estimators': 30},\n",
       "  mean: 0.88215, std: 0.00786, params: {'classifier__n_estimators': 40},\n",
       "  mean: 0.88283, std: 0.00622, params: {'classifier__n_estimators': 50},\n",
       "  mean: 0.88432, std: 0.00678, params: {'classifier__n_estimators': 60},\n",
       "  mean: 0.88619, std: 0.00861, params: {'classifier__n_estimators': 70}],\n",
       " {'classifier__n_estimators': 70},\n",
       " 0.8861906589379215)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "print('score_2_n_estimators: ', score_2_best_para['n_estimators'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### max_depth、min_samples_split\n",
    "- 决策树最大深度max_depth: \n",
    "    - 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。\n",
    "    - 一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。\n",
    "    - 常用的可以取值10-100之间。\n",
    "- 内部节点再划分所需最小样本数min_samples_split: \n",
    "    - 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 \n",
    "    - 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T06:06:40.890799Z",
     "start_time": "2018-07-25T06:06:40.882338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': range(3, 14, 2),\n",
       " 'classifier__min_samples_split': range(50, 201, 20)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.072Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "score_1_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('score_1_max_depth: ', score_1_best_para['max_depth'])\n",
    "print('score_1_min_samples_split: ', score_1_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.077Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "score_2_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('score_2_max_depth: ', score_2_best_para['max_depth'])\n",
    "print('score_2_min_samples_split: ', score_2_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_samples_leaf、min_samples_split\n",
    "- 叶子节点最少样本数min_samples_leaf: \n",
    "    - 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 \n",
    "    - 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。\n",
    "    - 如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.079Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_1_best_para['n_estimators'], \n",
    "                                                           max_depth = score_1_best_para['max_depth'], \n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_3 = dict(classifier__min_samples_leaf=range(10,60,10), \n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.081Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_3, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['min_samples_leaf'] = clf.best_params_['classifier__min_samples_leaf']\n",
    "score_1_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('score_1_min_samples_leaf: ', score_1_best_para['min_samples_leaf'])\n",
    "print('score_1_min_samples_split: ', score_1_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.082Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_1_best_para['n_estimators'], \n",
    "                                                           max_depth = score_1_best_para['max_depth'], \n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_3 = dict(classifier__min_samples_leaf=range(10,60,10), \n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.084Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_3, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['min_samples_leaf'] = clf.best_params_['classifier__min_samples_leaf']\n",
    "score_2_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('score_2_min_samples_leaf: ', score_2_best_para['min_samples_leaf'])\n",
    "print('score_2_min_samples_split: ', score_2_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_features\n",
    "- 划分时考虑的最大特征数max_features: \n",
    "    - 可以使用很多种类型的值，默认是\"None\",意味着划分时考虑所有的特征数；\n",
    "    - 如果是\"log2\"意味着划分时最多考虑log 2 N log2N个特征；\n",
    "    - 如果是\"sqrt\"或者\"auto\"意味着划分时最多考虑N − −  √  N个特征。\n",
    "    - 如果是整数，代表考虑的特征绝对数。\n",
    "    - 如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。\n",
    "    - 一般来说，如果样本特征数不多，比如小于50，我们用默认的\"None\"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.087Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_1_best_para['n_estimators'], \n",
    "                                                           max_depth = score_1_best_para['max_depth'],  \n",
    "                                                           min_samples_leaf = score_1_best_para['min_samples_leaf'],\n",
    "                                                           min_samples_split = score_1_best_para['min_samples_split'],\n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_4 = dict(classifier__max_features=[0.25, 0.5, 0.75, 1.0])\n",
    "param_grid_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.089Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_4, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['max_features'] = clf.best_params_['classifier__max_features']\n",
    "print('score_1_max_features: ', score_1_best_para['max_features'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.091Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_1_best_para['n_estimators'], \n",
    "                                                           max_depth = score_1_best_para['max_depth'],  \n",
    "                                                           min_samples_leaf = score_1_best_para['min_samples_leaf'],\n",
    "                                                           min_samples_split = score_1_best_para['min_samples_split'],\n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_4 = dict(classifier__max_features=[0.25, 0.5, 0.75, 1.0])\n",
    "param_grid_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_4, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['max_features'] = clf.best_params_['classifier__max_features']\n",
    "print('score_2_max_features: ', score_2_best_para['max_features'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsample\n",
    "- 子采样的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_1_best_para['n_estimators'], \n",
    "                                                           max_depth = score_1_best_para['max_depth'],  \n",
    "                                                           min_samples_leaf = score_1_best_para['min_samples_leaf'],\n",
    "                                                           min_samples_split = score_1_best_para['min_samples_split'],\n",
    "                                                           max_features = score_1_best_para['max_features'],\n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_5 = dict(classifier__subsample=[0.25, 0.5, 0.75, 1.0])\n",
    "param_grid_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_5, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['subsample'] = clf.best_params_['classifier__subsample']\n",
    "print('score_1_subsample: ', score_1_best_para['subsample'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', GradientBoostingClassifier(n_estimators = score_2_best_para['n_estimators'], \n",
    "                                                           max_depth = score_2_best_para['max_depth'],  \n",
    "                                                           min_samples_leaf = score_2_best_para['min_samples_leaf'],\n",
    "                                                           min_samples_split = score_2_best_para['min_samples_split'],\n",
    "                                                           max_features = score_2_best_para['max_features'],\n",
    "                                                               learning_rate = 0.1, \n",
    "                                                           random_state=0))])\n",
    "\n",
    "param_grid_5 = dict(classifier__subsample=[0.25, 0.5, 0.75, 1.0])\n",
    "param_grid_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_5, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['subsample'] = clf.best_params_['classifier__subsample']\n",
    "print('score_2_subsample: ', score_2_best_para['subsample'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.096Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_1_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_2_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=score_1_best_para['chi__k']))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = score_1_best_para['topk'])),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', GradientBoostingClassifier( n_estimators = score_1_best_para['n_estimators'], \n",
    "                                          max_depth = score_1_best_para['max_depth'],  \n",
    "                                          min_samples_leaf = score_1_best_para['min_samples_leaf'],\n",
    "                                          min_samples_split = score_1_best_para['min_samples_split'],\n",
    "                                          max_features = score_1_best_para['max_features'],\n",
    "                                          subsample = score_1_best_para['subsample'],\n",
    "                                              learning_rate = 0.1, \n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.102Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.105Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=score_2_best_para['chi__k']))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = score_2_best_para['topk'])),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', GradientBoostingClassifier( n_estimators = score_2_best_para['n_estimators'], \n",
    "                                          max_depth = score_2_best_para['max_depth'],  \n",
    "                                          min_samples_leaf = score_2_best_para['min_samples_leaf'],\n",
    "                                          min_samples_split = score_2_best_para['min_samples_split'],\n",
    "                                          max_features = score_2_best_para['max_features'],\n",
    "                                          subsample = score_2_best_para['subsample'],\n",
    "                                              learning_rate = 0.1, \n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-24T08:52:14.107Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

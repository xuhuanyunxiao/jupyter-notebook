{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 保监会 分类模型 2 训练 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:14.929110Z",
     "start_time": "2018-08-27T05:39:11.757910Z"
    }
   },
   "outputs": [],
   "source": [
    "##load packages, needed\n",
    "# encoding=utf-8\n",
    "\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2,mutual_info_classif,f_classif \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:14.959211Z",
     "start_time": "2018-08-27T05:39:14.932092Z"
    }
   },
   "outputs": [],
   "source": [
    "class StatsFeatures_cor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neg = set()\n",
    "        f = open(\"corpus/neg_words_20180704.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.neg.add(content.strip())\n",
    "        f.close()\n",
    "        \n",
    "        self.company = set() # 公司\n",
    "        f = open(\"corpus/bank_company_20180814.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.company.add(content.strip())\n",
    "        f.close()\n",
    "\n",
    "        self.regulators = set() # 监管机构及领导\n",
    "        f = open(\"corpus/bank_regulators_20180815.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.regulators.add(content.strip())\n",
    "        f.close()    \n",
    "        \n",
    "        #初始化字典liwc\n",
    "        self.liwc = {} \n",
    "        f2 = open(\"corpus/scliwc.txt\",'r', encoding = 'gb18030')\n",
    "        for ii in f2:     #ii在scliwc.txt中循环\n",
    "            i = ii.strip().split() \n",
    "            self.liwc[i[0]] = i[1:len(i)]\n",
    "        f2.close      \n",
    "        \n",
    "        self.category = set()\n",
    "        for i in list(self.liwc.values()):\n",
    "            for j in i:\n",
    "                self.category.add(j)         \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def getcnt(self,x):        \n",
    "        return len(list(set(x)))\n",
    "\n",
    "    def getnegcnt(self,x):\n",
    "        negcnt = 0\n",
    "        words = x.split()\n",
    "        for w in words:\n",
    "            if w in self.neg:\n",
    "                negcnt = negcnt+1\n",
    "        return negcnt\n",
    "    \n",
    "    def getorgcnttf(self,x):\n",
    "        companycnt=0\n",
    "        companytf=0\n",
    "        regcnt = 0\n",
    "        regtf = 0\n",
    "        \n",
    "        words = x.split()\n",
    "        words_set=set(words)\n",
    "        for w in words_set:\n",
    "            if w in self.company:\n",
    "                companycnt = companycnt+1\n",
    "                companytf=companytf+words.count(w)\n",
    "                \n",
    "            if w in self.regulators:\n",
    "                regcnt = regcnt+1\n",
    "                regtf=regtf+words.count(w)            \n",
    "                \n",
    "        return companycnt, companytf, regcnt, regtf\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = []\n",
    "        for x in X:\n",
    "            if len(x) == 0:\n",
    "                length  = 1\n",
    "            else :\n",
    "                length = len(x)\n",
    "                \n",
    "            companycnt, companytf, regcnt, regtf=self.getorgcnttf(x)\n",
    "            \n",
    "#             words = x.split()\n",
    "#             psy = []\n",
    "#             for w in words:\n",
    "#                 if w in self.liwc: #是否liwc字典包含分词结果列表words的哪些分词\n",
    "#                     psy += self.liwc[w] \n",
    "            \n",
    "#             cat_tf = []\n",
    "#             for cat in self.category:\n",
    "#                 cat_tf.append(psy.count(cat)) \n",
    "                \n",
    "            data.append([len(x),self.getcnt(x),self.getcnt(x)/length,\n",
    "                         self.getnegcnt(x),self.getnegcnt(x)/length,\n",
    "                         companycnt, companytf, regcnt, regtf]) #  + cat_tf           \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:14.980641Z",
     "start_time": "2018-08-27T05:39:14.961900Z"
    }
   },
   "outputs": [],
   "source": [
    "def getkeywords(X, N = 1000):\n",
    "    '''\n",
    "    训练时生成，合并所有记录，取N个关键词\n",
    "    '''\n",
    "    textrank = analyse.textrank\n",
    "\n",
    "    text_combined = ' '.join(X)\n",
    "    keywords = textrank(text_combined, topK = N)\n",
    "    print('keywords num: ', len(keywords))\n",
    "    if len(keywords) < N : \n",
    "        N  = len(keywords)\n",
    "\n",
    "    if keywords:\n",
    "        f = open(\"corpus/keywords.txt\",\"w+\", encoding='UTF-8')\n",
    "        for content in keywords:\n",
    "            content = content.strip()\n",
    "            f.write(content + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.003483Z",
     "start_time": "2018-08-27T05:39:14.983346Z"
    }
   },
   "outputs": [],
   "source": [
    "class Statskeywords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, topk = 100):\n",
    "        self.topk = topk\n",
    "        \n",
    "        self.keywords = set()\n",
    "        f = open(\"corpus/keywords.txt\",\"r+\", encoding='UTF-8')\n",
    "        num = 0\n",
    "        for content in f:\n",
    "            if num < topk:\n",
    "                self.keywords.add(content.strip().replace('\\n', ''))\n",
    "            num += 1\n",
    "        f.close() \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本中关键词的词频\n",
    "        '''                        \n",
    "        data = []\n",
    "        for x in X:\n",
    "            words = x.split()\n",
    "            word_tf = []\n",
    "            keycnt = 0\n",
    "            for kw in self.keywords:\n",
    "                word_tf.append(words.count(kw)) # 各个关键词的词频\n",
    "                if kw in words:keycnt+=1\n",
    "            word_tf.append(keycnt) # 关键词的个数             \n",
    "                \n",
    "            data.append(word_tf)       \n",
    "            \n",
    "        return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上一版模型读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.019499Z",
     "start_time": "2018-08-27T05:39:15.006182Z"
    }
   },
   "outputs": [],
   "source": [
    "# 上一版模型\n",
    "# from sklearn.externals import joblib\n",
    "# pipeline_old = joblib.load( \"model/circ_8classifier_0808.pkl.z\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.052071Z",
     "start_time": "2018-08-27T05:39:15.022054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6526\n"
     ]
    }
   ],
   "source": [
    "title = []\n",
    "filename = 'data/titles0.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    title.append(f.strip().replace('\\n', ''))\n",
    "fid.close()\n",
    "print(len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.073081Z",
     "start_time": "2018-08-27T05:39:15.054972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小白学 保险 基础 篇 买好 宝宝 保险 第一步 该 做', '过 诉讼时效 债务 如何 处理 最高法院 答复 全']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.243290Z",
     "start_time": "2018-08-27T05:39:15.075897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6526\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "filename = 'data/contents0.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    content.append(f.strip().replace('\\n', ''))\n",
    "fid.close()\n",
    "print(len(content))\n",
    "# content[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.288919Z",
     "start_time": "2018-08-27T05:39:15.245963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6526\n"
     ]
    }
   ],
   "source": [
    "title_content = [t + ' ' + c for t,c in zip(title, content)]\n",
    "print(len(title_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.309132Z",
     "start_time": "2018-08-27T05:39:15.291607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '监管',\n",
       " 2: '行业',\n",
       " 3: '产品销售',\n",
       " 4: '资本市场',\n",
       " 5: '公司内部管理',\n",
       " 6: '消费服务',\n",
       " 7: '其他相关报道',\n",
       " 8: '噪音'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic={'监管':1,'行业':2,'产品销售':3,'资本市场':4,'公司内部管理':5,'消费服务':6,'其他相关报道':7,'噪音':8}\n",
    "class_name_dict = {v: k for k, v in label_dic.items()}\n",
    "class_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.341549Z",
     "start_time": "2018-08-27T05:39:15.311949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['噪音', '噪音', '噪音', '噪音', '噪音']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = []\n",
    "filename = 'data/labels.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    label.append(f.strip().replace('\\n', ''))\n",
    "fid.close()\n",
    "print(len(label))\n",
    "label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.351651Z",
     "start_time": "2018-08-27T05:39:15.344419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8, 8, 8, 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = [label_dic[l] for l in label]\n",
    "label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割训练集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:39:15.376213Z",
     "start_time": "2018-08-27T05:39:15.354072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集： 4568\n",
      "训练集-各类数量： Counter({1: 939, 2: 684, 6: 603, 8: 549, 7: 523, 4: 453, 5: 436, 3: 381})\n",
      "测试集： 1958\n",
      "测试集-各类数量： Counter({1: 395, 2: 300, 6: 274, 8: 251, 7: 197, 5: 196, 4: 189, 3: 156})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(title_content, label, test_size=0.3, random_state=42)\n",
    "print('训练集：',len(y_train))\n",
    "print('训练集-各类数量：',Counter(y_train))\n",
    "print('测试集：',len(y_test))\n",
    "print('测试集-各类数量：',Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T03:07:37.578533Z",
     "start_time": "2018-08-17T03:07:37.570318Z"
    }
   },
   "outputs": [],
   "source": [
    "if 0: \n",
    "    from jieba import analyse\n",
    "    getkeywords(title_content, N = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:43:52.563424Z",
     "start_time": "2018-08-27T05:39:24.041738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\n",
      "1334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.910 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords num:  500\n",
      "984\n",
      "984\n",
      "keywords num:  500\n",
      "537\n",
      "537\n",
      "keywords num:  500\n",
      "642\n",
      "642\n",
      "keywords num:  500\n",
      "632\n",
      "632\n",
      "keywords num:  500\n",
      "877\n",
      "877\n",
      "keywords num:  500\n",
      "720\n",
      "720\n",
      "keywords num:  500\n",
      "800\n",
      "800\n",
      "keywords num:  500\n"
     ]
    }
   ],
   "source": [
    "# j = 1\n",
    "N = 500\n",
    "class_key_dict = {}\n",
    "key_dict = {}\n",
    "for j in range(1,9):\n",
    "    str_list = [m for m,n in zip(title_content, label) if n == j]\n",
    "    print(len(str_list))\n",
    "    print(label.count(j))\n",
    "    textrank = analyse.textrank\n",
    "    text_combined = ' '.join(str_list)\n",
    "    keywords = textrank(text_combined, topK = N)\n",
    "    print('keywords num: ', len(keywords))\n",
    "#     print('%s -- %s: '%(j, class_name_dict[j]) , keywords)\n",
    "    class_key_dict[j] = keywords\n",
    "    for key in keywords:\n",
    "        if key not in key_dict:            \n",
    "            key_dict[key] = 1\n",
    "        else :\n",
    "            key_dict[key] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:43:52.572729Z",
     "start_time": "2018-08-27T05:43:52.566183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "1567\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 6, 8, 5, 8, 5, 7, 7, 8, 4, 8, 8, 6, 6, 5, 7, 8, 4, 7, 8, 8, 7, 5, 5, 3, 1, 2, 6, 8, 7, 5, 4, 8, 7, 4, 5, 8, 1, 3, 5, 2, 8, 5, 8, 7, 8, 3, 5, 1, 2, 7, 6, 7, 3, 4, 4, 7, 7, 8, 6, 7, 8, 4, 7, 7, 7, 5, 6, 3, 5, 7, 8, 4, 5, 8, 3, 8, 6, 8, 8, 7, 5, 7, 8, 8, 3, 6, 8, 5, 5, 1, 8, 6, 2, 8, 5, 6, 6, 6, 6, 1, 5, 7, 5, 8, 5, 7, 7, 8, 4, 6, 5, 4, 3, 6, 7, 5, 3, 5, 8, 6, 2, 7, 6, 6, 7, 6, 3, 6, 7, 7, 4, 1, 8, 6, 6, 4, 2, 7, 1, 8, 5, 1, 5, 4, 7, 3, 1, 4, 6, 4, 2, 5, 4, 6, 3, 7, 4, 5, 3, 1, 7, 7, 7, 7, 2, 4, 8, 1, 6, 6, 4, 4, 2, 1, 3, 2, 4, 6, 3, 7, 1, 3, 8, 8, 4, 3, 7, 2, 6, 7, 8, 2, 7, 7, 5, 6, 5, 6, 5, 5, 1, 3, 6, 1, 4, 1, 6, 8, 5, 4, 2, 1, 7, 8, 7, 6, 3, 8, 8, 8, 7, 1, 1, 3, 8, 5, 3, 6, 4, 4, 6, 4, 2, 4, 7, 6, 6, 1, 6, 3, 4, 6, 8, 2, 8, 4, 1, 2, 8, 6, 4, 7, 6, 5, 5, 2, 3, 3, 3, 6, 3, 4, 4, 1, 5, 7, 4, 3, 5, 5, 3, 5, 3, 5, 4, 7, 6, 8, 1, 4, 7, 1, 2, 4, 5, 4, 2, 1, 6, 2, 1, 3, 1, 3, 4, 3, 3, 1, 3, 4, 4, 5, 1, 6, 6, 6, 2, 3, 5, 8, 3, 3, 1, 4, 4, 2, 2, 3, 6, 2, 7, 5, 2, 7, 1, 3, 6, 5, 2, 6, 7, 3, 4, 5, 1, 2, 3, 4, 7, 1, 4, 5, 1, 3, 1, 6, 6, 2, 7, 2, 3, 2, 4, 4, 3, 5, 4, 2, 5, 7, 2, 1, 7, 3, 8, 5, 1, 4, 6, 4, 8, 7, 3, 4, 3, 1, 4, 2, 3, 6, 5, 8, 5, 2, 2, 5, 1, 7, 1, 8, 1, 7, 5, 1, 6, 6, 1, 2, 2, 1, 7, 5, 2, 1, 5, 7, 1, 1, 1, 2, 4, 7, 1, 3, 2, 2, 3, 4, 4, 7, 1, 6, 5, 2, 2, 2, 1, 7, 4, 2, 3, 1, 4, 3, 4, 3, 2, 1, 6, 1, 6, 2, 6, 1, 1, 3, 2, 3, 2, 1, 2, 1, 1, 6, 1, 2, 3, 2, 3, 2, 5, 1, 2, 1, 7, 3, 4, 8, 1, 1, 6, 4, 1, 1, 6, 6, 5, 2, 4, 3, 3, 3, 3, 6, 3, 4, 3, 6, 3, 6, 5, 4, 3, 2, 6, 5, 1, 3, 6, 2, 5, 3, 1, 3, 3, 3, 2, 5, 3, 3, 3, 5, 2, 2, 2, 2, 4, 4, 2, 7, 1, 2, 1, 3, 2, 4, 3, 6, 3, 6, 3, 5, 1, 2, 6, 1, 3, 2, 2, 2, 2, 5, 4, 3, 3, 4, 5, 1, 2, 3, 1, 1, 5, 2, 2, 1, 2, 1, 5, 2, 4, 4, 1, 3, 2, 4, 1, 2, 1, 3, 4, 3, 2, 2, 2, 1, 2, 1, 1, 3, 4, 5, 3, 5, 3, 2, 1, 1, 3, 3, 4, 6, 2, 2, 2, 2, 1, 3, 1, 2, 2, 6, 3, 5, 2, 3, 3, 2, 1, 1, 4, 1, 1, 3, 4, 3, 3, 1, 5, 1, 5, 5, 2, 1, 5, 1, 3, 1, 3, 1, 1, 3, 2, 1, 3, 4, 1, 1, 1, 1, 2, 4, 2, 3, 1, 1, 2, 2, 3, 1, 1, 1, 4, 1, 2, 2, 2, 2, 3, 2, 1, 2, 2, 4, 2, 3, 2, 2, 3, 2, 1, 1, 1, 2, 4, 1, 2, 1, 3, 1, 1, 1, 2, 2, 1, 2, 2, 3, 2, 3, 1, 1, 3, 1, 3, 2, 6, 2, 1, 5, 3, 2, 2, 1, 3, 1, 1, 4, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 3, 1, 5, 1, 3, 2, 1, 2, 1, 1, 2, 3, 1, 3, 2, 1, 1, 3, 4, 2, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 3, 2, 1, 5, 3, 2, 1, 1, 2, 1, 2, 3, 1, 3, 1, 1, 2, 3, 3, 1, 2, 1, 1, 1, 1, 2, 3, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 3, 2, 1, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 4, 1, 1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 2, 3, 2, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 5, 2, 2, 1, 3, 1, 1, 2, 1, 2, 3, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 4, 4, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 3, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 4, 2, 2, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 3, 1, 2, 4, 2, 2, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 2, 1, 3, 1, 1, 3, 2, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "key_count_list = [k for k,v in key_dict.items() if v > 3]\n",
    "print(len(key_count_list))\n",
    "print(len(key_dict.values()))\n",
    "print(list(key_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:43:53.284089Z",
     "start_time": "2018-08-27T05:43:52.574945Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('data/word_class_freq.xlsx') as writer:\n",
    "    for j in class_key_dict:\n",
    "        class_name = class_name_dict[j]\n",
    "        tmp_data = pd.DataFrame(class_key_dict[j])\n",
    "        tmp_data.columns = ['word']\n",
    "        tmp_data['class_freq'] = tmp_data['word'].apply(lambda x: key_dict[x])\n",
    "        tmp_data.to_excel(writer, class_name)\n",
    "    writer.save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:43:53.334990Z",
     "start_time": "2018-08-27T05:43:53.286627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --\n",
      "去除前： 500\n",
      "去除后： 180\n",
      "2 --\n",
      "去除前： 500\n",
      "去除后： 166\n",
      "3 --\n",
      "去除前： 500\n",
      "去除后： 272\n",
      "4 --\n",
      "去除前： 500\n",
      "去除后： 208\n",
      "5 --\n",
      "去除前： 500\n",
      "去除后： 178\n",
      "6 --\n",
      "去除前： 500\n",
      "去除后： 300\n",
      "7 --\n",
      "去除前： 500\n",
      "去除后： 253\n",
      "8 --\n",
      "去除前： 500\n",
      "去除后： 195\n",
      "['登记', '技巧', '无需', '政治', '钱包', '假币', '讲座', '交叉', '主席', '趋势']\n",
      "len(keywords):  1182\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "for j in range(1,9):\n",
    "    print('%s --'%j)\n",
    "    keyword_list = class_key_dict[j]\n",
    "    print('去除前：', len(keyword_list))\n",
    "    for k in key_count_list:        \n",
    "        if k in keyword_list:\n",
    "            keyword_list.remove(k)\n",
    "    print('去除后：', len(keyword_list)) \n",
    "    \n",
    "    keywords += keyword_list\n",
    "\n",
    "keywords = list(set(keywords))\n",
    "print(keywords[:10])\n",
    "print('len(keywords): ', len(keywords))\n",
    "f = open(\"corpus/keywords.txt\",\"w+\", encoding='UTF-8')\n",
    "for content in keywords:\n",
    "    content = content.strip()\n",
    "    f.write(content + '\\n')\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: 主题模型选特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    #打印每个主题下权重较高的term\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    #打印主题-词语分布矩阵\n",
    "    print(model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:03:35.391330Z",
     "start_time": "2018-08-01T09:03:30.508002Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tf_vectorizer  = CountVectorizer(max_df=0.95, min_df=2)\n",
    "tf = tf_vectorizer .fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T08:54:48.582308Z",
     "start_time": "2018-08-03T08:54:48.574628Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(tf.data), tf.data\n",
    "\n",
    "# tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "# print(tf_feature_names[:10])\n",
    "# len(tf_feature_names)\n",
    "\n",
    "# n_top_words=20\n",
    "# tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "# print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:29.443362Z",
     "start_time": "2018-08-01T09:03:49.887880Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20,\n",
    "                                max_iter=50,\n",
    "                                learning_method='batch',\n",
    "                                random_state=0)\n",
    "docres = lda.fit_transform(tf)\n",
    "lda.perplexity(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:29.452911Z",
     "start_time": "2018-08-01T09:09:29.446388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.90625003e-04, 9.92578125e-01, 3.90625003e-04, ...,\n",
       "        3.90625095e-04, 3.90625002e-04, 3.90625004e-04],\n",
       "       [4.50450455e-04, 1.28539163e-01, 7.50649189e-01, ...,\n",
       "        4.50450465e-04, 4.50450457e-04, 4.50450459e-04],\n",
       "       [6.44618417e-02, 7.77604998e-05, 7.77604987e-05, ...,\n",
       "        2.78697680e-02, 2.06295485e-02, 7.77604990e-05],\n",
       "       ...,\n",
       "       [1.21359227e-04, 1.21359227e-04, 1.21359225e-04, ...,\n",
       "        1.21359226e-04, 1.21359226e-04, 1.21359225e-04],\n",
       "       [4.32523919e-02, 5.10204087e-04, 5.10204084e-04, ...,\n",
       "        3.71540782e-01, 5.10204085e-04, 5.10204082e-04],\n",
       "       [9.32835840e-05, 9.32835834e-05, 9.32835835e-05, ...,\n",
       "        9.32835830e-05, 1.79299844e-01, 2.00158643e-01]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:29.461871Z",
     "start_time": "2018-08-01T09:09:29.455356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00000001e-02, 5.56502649e+00, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000006e-02],\n",
       "       [1.01622379e+01, 5.00000005e-02, 5.00000000e-02, ...,\n",
       "        5.00000006e-02, 5.00000003e-02, 5.00000002e-02],\n",
       "       [5.00000003e-02, 5.00000002e-02, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000040e-02],\n",
       "       ...,\n",
       "       [5.00000001e-02, 5.00000004e-02, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000001e-02],\n",
       "       [5.00000012e-02, 5.00000010e-02, 5.00000000e-02, ...,\n",
       "        5.00000028e-02, 5.00000007e-02, 5.00000001e-02],\n",
       "       [2.06829458e+02, 5.76482636e+01, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000002e-02]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:33.616440Z",
     "start_time": "2018-08-01T09:09:29.637897Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_topic_dist = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T03:52:45.115Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "n_topics = [200,] #range(20, 75, 5)\n",
    "perplexityLst = [1.0]*len(n_topics)\n",
    "\n",
    "#训练LDA并打印训练时间\n",
    "lda_models = []\n",
    "for idx, n_topic in enumerate(n_topics):\n",
    "    lda = LatentDirichletAllocation(n_topics=n_topic,\n",
    "                                    max_iter=1000,\n",
    "                                    learning_method='batch',\n",
    "                                    evaluate_every=200,\n",
    "#                                    perp_tol=0.1, #default                                       \n",
    "#                                    doc_topic_prior=1/n_topic, #default\n",
    "#                                    topic_word_prior=1/n_topic, #default\n",
    "                                    verbose=0)\n",
    "    t0 = time()\n",
    "    lda.fit(tf)\n",
    "    perplexityLst[idx] = lda.perplexity(tf)\n",
    "    lda_models.append(lda)\n",
    "    print(\"# of Topic: %d, \" % n_topics[idx])\n",
    "    print(\"done in %0.3fs, N_iter %d, \" % ((time() - t0), lda.n_iter_))\n",
    "    print(\"Perplexity Score %0.3f\" % perplexityLst[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T10:17:32.308Z"
    }
   },
   "outputs": [],
   "source": [
    "#打印最佳模型\n",
    "best_index = perplexityLst.index(min(perplexityLst))\n",
    "best_n_topic = n_topics[best_index]\n",
    "best_model = lda_models[best_index]\n",
    "print(\"Best # of Topic: \", best_n_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:49:20.084373Z",
     "start_time": "2018-08-27T05:43:53.337570Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997810858143608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9...tate=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer(max_df=0.95, min_df=2)),\n",
    "            ('tf_idf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=2000))\n",
    "        ])),\n",
    "        ('len_stats', StatsFeatures_cor()),\n",
    "        ('tf', Pipeline([\n",
    "            ('tf_k', Statskeywords(topk = 5000)),\n",
    "            ('chi', SelectKBest(chi2, k=500))])),\n",
    "    ])),\n",
    "    ('standard', StandardScaler(with_mean=False)),\n",
    "    ('classifier', XGBClassifier(max_depth=7,objective='multi:softmax', num_class=8))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:50:19.671812Z",
     "start_time": "2018-08-27T05:49:20.087276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.9044943820224719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.96      0.94       395\n",
      "          2       0.85      0.88      0.87       300\n",
      "          3       0.95      0.88      0.91       156\n",
      "          4       0.85      0.81      0.83       189\n",
      "          5       0.87      0.79      0.83       196\n",
      "          6       0.92      0.95      0.93       274\n",
      "          7       0.93      0.93      0.93       197\n",
      "          8       0.94      0.95      0.94       251\n",
      "\n",
      "avg / total       0.90      0.90      0.90      1958\n",
      "\n",
      "confusion_matrix: \n",
      "[[380  10   0   1   1   2   0   1]\n",
      " [ 13 265   2   9   9   1   0   1]\n",
      " [  2   6 137   2   3   2   2   2]\n",
      " [  4  10   0 153   3   9   1   9]\n",
      " [  7  10   0   9 154   8   7   1]\n",
      " [  3   5   2   0   2 259   3   0]\n",
      " [  1   1   3   1   4   1 184   2]\n",
      " [  1   3   0   6   1   1   0 239]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T03:34:32.066098Z",
     "start_time": "2018-08-17T03:33:46.840479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.854006586169045\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.63      0.73      0.68        52\n",
      "          2       0.82      0.91      0.86       299\n",
      "          3       0.82      0.61      0.70        38\n",
      "          4       0.91      0.60      0.73        88\n",
      "          5       0.76      0.62      0.68        63\n",
      "          6       0.90      0.95      0.93        99\n",
      "          7       0.93      0.83      0.87        46\n",
      "          8       0.92      0.98      0.95       226\n",
      "\n",
      "avg / total       0.86      0.85      0.85       911\n",
      "\n",
      "confusion_matrix: \n",
      "[[ 38  13   0   0   0   0   0   1]\n",
      " [ 19 272   2   0   3   1   0   2]\n",
      " [  0   8  23   0   1   0   2   4]\n",
      " [  0  20   0  53   2   3   0  10]\n",
      " [  1  13   1   2  39   5   1   1]\n",
      " [  2   0   1   0   2  94   0   0]\n",
      " [  0   2   1   2   3   0  38   0]\n",
      " [  0   2   0   1   1   1   0 221]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T01:39:29.824415Z",
     "start_time": "2018-08-10T01:38:03.123565Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.9376872378669863\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.96      0.95      0.96       425\n",
      "          2       0.88      0.88      0.88       320\n",
      "          3       0.89      0.93      0.91       215\n",
      "          4       0.92      0.92      0.92       206\n",
      "          5       0.95      0.83      0.89       322\n",
      "          6       0.95      0.95      0.95       407\n",
      "          7       0.94      0.98      0.96       432\n",
      "          8       0.95      0.97      0.96      1011\n",
      "\n",
      "avg / total       0.94      0.94      0.94      3338\n",
      "\n",
      "confusion_matrix: \n",
      "[[404   7   0   0   2   0   1  11]\n",
      " [  3 281   8   5   4   6   4   9]\n",
      " [  0   6 199   0   0   3   3   4]\n",
      " [  2   4   2 189   4   0   0   5]\n",
      " [  5  16   4   3 267   7   8  12]\n",
      " [  0   1   6   0   1 387   6   6]\n",
      " [  1   0   1   0   0   1 425   4]\n",
      " [  6   4   4   8   3   5   3 978]]\n"
     ]
    }
   ],
   "source": [
    "# 上一版模型 \n",
    "y_pred_class = pipeline_old.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T02:57:03.596005Z",
     "start_time": "2018-08-03T02:57:02.750893Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "title_content = np.array(title_content)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T04:28:50.352781Z",
     "start_time": "2018-08-03T02:57:05.133428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 1\n",
      "0.9991673605328892\n",
      "accuracy_score:  0.8408788282290279\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.83      0.88      0.86       286\n",
      "          2       0.52      0.41      0.46        58\n",
      "          3       0.76      0.50      0.60        58\n",
      "          4       0.88      0.78      0.83        98\n",
      "          5       0.82      0.75      0.78       126\n",
      "          6       0.71      0.80      0.75        59\n",
      "          7       0.85      0.81      0.83       151\n",
      "          8       0.88      0.93      0.90       666\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1502\n",
      "\n",
      "confusion_matrix: \n",
      "[[253   5   2   1   1   0   2  22]\n",
      " [ 11  24   0   0   2   3   1  17]\n",
      " [  6   2  29   1   4   2   5   9]\n",
      " [  2   3   0  76   6   0   1  10]\n",
      " [  7   3   1   1  94   3   8   9]\n",
      " [  0   1   1   0   1  47   1   8]\n",
      " [  3   3   1   0   3   8 122  11]\n",
      " [ 21   5   4   7   4   3   4 618]]\n",
      "---- 2\n",
      "0.9993338884263114\n",
      "accuracy_score:  0.8468708388814914\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.90      0.86       309\n",
      "          2       0.67      0.48      0.56        62\n",
      "          3       0.79      0.55      0.65        67\n",
      "          4       0.82      0.73      0.77        81\n",
      "          5       0.79      0.77      0.78       115\n",
      "          6       0.86      0.71      0.78        62\n",
      "          7       0.90      0.83      0.86       144\n",
      "          8       0.88      0.93      0.90       662\n",
      "\n",
      "avg / total       0.84      0.85      0.84      1502\n",
      "\n",
      "confusion_matrix: \n",
      "[[277   4   3   1   6   0   0  18]\n",
      " [  8  30   1   3   3   0   6  11]\n",
      " [  5   3  37   0   3   1   1  17]\n",
      " [  2   1   0  59   3   0   0  16]\n",
      " [ 12   4   1   0  88   4   1   5]\n",
      " [  0   0   1   0   2  44   5  10]\n",
      " [  1   1   2   3   4   2 120  11]\n",
      " [ 31   2   2   6   3   0   1 617]]\n",
      "---- 3\n",
      "0.9988344988344988\n",
      "accuracy_score:  0.8461025982678214\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.87      0.84       302\n",
      "          2       0.60      0.54      0.57        61\n",
      "          3       0.73      0.52      0.61        71\n",
      "          4       0.89      0.72      0.80       101\n",
      "          5       0.79      0.73      0.76       113\n",
      "          6       0.82      0.73      0.78        45\n",
      "          7       0.83      0.83      0.83       132\n",
      "          8       0.89      0.94      0.92       676\n",
      "\n",
      "avg / total       0.84      0.85      0.84      1501\n",
      "\n",
      "confusion_matrix: \n",
      "[[263   7   1   1   7   1   0  22]\n",
      " [ 10  33   1   3   0   0   4  10]\n",
      " [  6   4  37   0   4   2   6  12]\n",
      " [  9   2   1  73   3   0   0  13]\n",
      " [ 12   2   2   1  83   0   7   6]\n",
      " [  0   3   2   0   1  33   2   4]\n",
      " [  2   3   1   0   5   3 110   8]\n",
      " [ 20   1   6   4   2   1   4 638]]\n",
      "---- 4\n",
      "0.9986679986679987\n",
      "accuracy_score:  0.8387741505662891\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.88      0.85       283\n",
      "          2       0.56      0.28      0.38        67\n",
      "          3       0.68      0.56      0.61        61\n",
      "          4       0.90      0.78      0.84        97\n",
      "          5       0.67      0.70      0.68       115\n",
      "          6       0.90      0.67      0.77        66\n",
      "          7       0.86      0.90      0.88       126\n",
      "          8       0.88      0.94      0.91       686\n",
      "\n",
      "avg / total       0.83      0.84      0.83      1501\n",
      "\n",
      "confusion_matrix: \n",
      "[[250   6   0   1  10   0   0  16]\n",
      " [ 15  19   2   1   7   2   3  18]\n",
      " [  3   1  34   0   6   3   0  14]\n",
      " [  4   3   0  76   3   0   0  11]\n",
      " [  8   3   3   1  80   0   6  14]\n",
      " [  2   0   0   1   7  44   3   9]\n",
      " [  0   2   4   0   3   0 113   4]\n",
      " [ 23   0   7   4   3   0   6 643]]\n",
      "---- 5\n",
      "0.9985014985014985\n",
      "accuracy_score:  0.8427714856762158\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.84      0.91      0.87       308\n",
      "          2       0.67      0.46      0.55        69\n",
      "          3       0.81      0.54      0.65        63\n",
      "          4       0.82      0.72      0.77        86\n",
      "          5       0.71      0.71      0.71       115\n",
      "          6       0.79      0.67      0.73        49\n",
      "          7       0.81      0.87      0.84       125\n",
      "          8       0.89      0.92      0.91       686\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1501\n",
      "\n",
      "confusion_matrix: \n",
      "[[279   0   0   0   4   1   2  22]\n",
      " [  9  32   4   1   3   0   6  14]\n",
      " [  4   5  34   0   5   2   3  10]\n",
      " [  4   2   0  62   7   0   0  11]\n",
      " [  6   3   2   2  82   2   9   9]\n",
      " [  0   2   0   1   3  33   2   8]\n",
      " [  4   2   0   1   5   1 109   3]\n",
      " [ 26   2   2   9   7   3   3 634]]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for train_index , test_index in kf.split(title_content):\n",
    "    print('---- %s'%(num+1))\n",
    "    X_train,X_test = title_content[train_index], title_content[test_index]\n",
    "    y_train,y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('tf_idf', Pipeline([\n",
    "                ('counts', CountVectorizer(max_df=0.95, min_df=2)),\n",
    "                ('tf_idf', TfidfTransformer()),\n",
    "                ('chi', SelectKBest(chi2, k=20000))\n",
    "            ])),\n",
    "            ('tf', Statskeywords(topk = 1000)),\n",
    "            ('len_stats', StatsFeatures_cor())\n",
    "        ])),\n",
    "        ('standard', StandardScaler(with_mean=False)),\n",
    "        ('classifier', XGBClassifier(max_depth=7,objective='multi:softmax', num_class=8))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(pipeline.score(X_train, y_train))    \n",
    "    \n",
    "    y_pred_class = pipeline.predict(X_test)\n",
    "    print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "    print('confusion_matrix: ')\n",
    "    print( metrics.confusion_matrix(y_test, y_pred_class))    \n",
    "    \n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T04:00:13.760409Z",
     "start_time": "2018-08-10T04:00:13.756644Z"
    }
   },
   "outputs": [],
   "source": [
    "score_1_best_para = {}\n",
    "score_2_best_para = {}\n",
    "cv = 3\n",
    "score_1 = 'accuracy'\n",
    "score_2 = 'recall_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T04:00:14.299704Z",
     "start_time": "2018-08-10T04:00:14.278560Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_features = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=20000))\n",
    "                                        ])),\n",
    "                                    ('tf', Pipeline([\n",
    "                                        ('tf_k', Statskeywords(topk = 5000)),\n",
    "                                        ('chi', SelectKBest(chi2, k=500))])),\n",
    "                                    ('len_stats', StatsFeatures_cor())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topk、chi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T04:00:15.548708Z",
     "start_time": "2018-08-10T04:00:15.540903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf_idf__chi__k': [2000, 5000, 20000, 40000],\n",
       " 'features__tf__chi__k': [50, 100, 500]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0,\n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [2000, 5000, 20000, 40000 ],\n",
    "                   features__tf__chi__k = [50,100, 500]) \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T07:13:14.807591Z",
     "start_time": "2018-08-10T04:00:16.541989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=2000 ..........\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=2000 ..........\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=2000 ..........\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=2000, score=0.8618699499807618, total= 8.4min\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=5000 ..........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=2000, score=0.8593990755007704, total= 8.4min\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=2000, score=0.8700347088314694, total= 8.4min\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=5000 ..........\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=5000, score=0.8680261639091958, total= 9.7min\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=20000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=5000, score=0.8673351330505207, total= 9.9min\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=20000 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 26.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=5000, score=0.8597842835130971, total= 9.9min\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=20000, score=0.8672566371681416, total=11.9min\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=20000, score=0.8617103235747303, total=12.0min\n",
      "[CV] features__tf__chi__k=50, features__tf_idf__chi__k=40000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=20000, score=0.8708060161974547, total=12.4min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=2000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=40000, score=0.8641785302039245, total=14.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 45.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=2000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=40000, score=0.8601694915254238, total=14.8min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=2000 .........\n",
      "[CV]  features__tf__chi__k=50, features__tf_idf__chi__k=40000, score=0.8731199382954107, total=15.1min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=5000 .........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=2000, score=0.8626394767218161, total=12.1min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=5000 .........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=2000, score=0.8578582434514638, total=11.8min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=5000 .........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=2000, score=0.8650212109525646, total=11.9min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=20000 ........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=5000, score=0.8680261639091958, total=13.9min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=20000 ........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=5000, score=0.8613251155624037, total=13.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 79.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=20000 ........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=5000, score=0.8642499035865793, total=14.1min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=40000 ........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=20000, score=0.8661023470565602, total=16.7min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=40000 ........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=20000, score=0.8605546995377504, total=16.6min\n",
      "[CV] features__tf__chi__k=100, features__tf_idf__chi__k=40000 ........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=20000, score=0.869263401465484, total=16.0min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=2000 .........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=40000, score=0.865332820315506, total=17.5min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=2000 .........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=40000, score=0.863251155624037, total=17.2min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=2000 .........\n",
      "[CV]  features__tf__chi__k=100, features__tf_idf__chi__k=40000, score=0.8684920940994987, total=18.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 114.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=5000 .........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=2000, score=0.8595613697575991, total=10.9min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=5000 .........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=2000, score=0.8613251155624037, total=12.1min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=5000 .........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=2000, score=0.86579251831855, total=11.9min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=20000 ........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=5000, score=0.8691804540207773, total=13.3min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=20000 ........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=5000, score=0.8597842835130971, total=14.6min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=20000 ........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=5000, score=0.8650212109525646, total=15.2min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=40000 ........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=20000, score=0.8618699499807618, total=19.8min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=40000 ........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=20000, score=0.8613251155624037, total=21.9min\n",
      "[CV] features__tf__chi__k=500, features__tf_idf__chi__k=40000 ........\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=20000, score=0.869263401465484, total=21.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed: 165.5min remaining: 15.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=40000, score=0.862254713351289, total=23.1min\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=40000, score=0.8601694915254238, total=19.3min\n",
      "[CV]  features__tf__chi__k=500, features__tf_idf__chi__k=40000, score=0.870420362514462, total=16.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 180.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_tf_idf__chi__k:  20000\n",
      "score_1_tf__chi__k:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.86376, std: 0.00454, params: {'features__tf__chi__k': 50, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.86505, std: 0.00373, params: {'features__tf__chi__k': 50, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.86659, std: 0.00374, params: {'features__tf__chi__k': 50, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.86582, std: 0.00541, params: {'features__tf__chi__k': 50, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.86184, std: 0.00298, params: {'features__tf__chi__k': 100, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.86454, std: 0.00274, params: {'features__tf__chi__k': 100, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.86531, std: 0.00360, params: {'features__tf__chi__k': 100, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.86569, std: 0.00215, params: {'features__tf__chi__k': 100, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.86222, std: 0.00262, params: {'features__tf__chi__k': 500, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.86466, std: 0.00384, params: {'features__tf__chi__k': 500, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.86415, std: 0.00362, params: {'features__tf__chi__k': 500, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.86428, std: 0.00442, params: {'features__tf__chi__k': 500, 'features__tf_idf__chi__k': 40000}],\n",
       " {'features__tf__chi__k': 50, 'features__tf_idf__chi__k': 20000},\n",
       " 0.8665896250642013)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['tf_idf__chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_1_best_para['tf__chi__k'] = clf.best_params_['features__tf__chi__k']\n",
    "print('score_1_tf_idf__chi__k: ', score_1_best_para['tf_idf__chi__k'])\n",
    "print('score_1_tf__chi__k: ', score_1_best_para['tf__chi__k'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T12:43:23.741917Z",
     "start_time": "2018-08-08T12:43:23.732210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf_idf__chi__k': [2000, 5000, 20000, 40000],\n",
       " 'features__tf__topk': [50, 100, 500]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0,\n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [2000, 5000, 20000, 40000 ],\n",
    "                   features__tf__chi__k = [50,100, 500]) \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T14:31:03.158110Z",
     "start_time": "2018-08-08T12:43:23.744646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8231063761425004, total= 5.6min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8118461781032549, total= 5.8min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8300723296263293, total= 5.8min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8212898267295912, total= 8.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8291626269349395, total= 6.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 14.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.810622908581124, total= 6.9min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8181799751454917, total=10.8min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.82764145306582, total=11.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8150179879762034, total=11.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8186643180207618, total=13.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 29.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8324023763764725, total=11.9min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8231063761425004, total= 5.8min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8149401664117953, total=13.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8300723296263293, total= 5.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8118461781032549, total= 5.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8212898267295912, total= 7.9min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8291626269349395, total= 7.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 44.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.810622908581124, total= 7.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8181799751454917, total=11.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.82764145306582, total=10.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8150179879762034, total=10.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8186643180207618, total=12.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8231063761425004, total= 4.9min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8300723296263293, total= 5.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 64.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8324023763764725, total=13.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8149401664117953, total=11.8min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8118461781032549, total= 6.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8291626269349395, total= 8.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8212898267295912, total= 9.2min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.810622908581124, total= 8.9min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8181799751454917, total=10.3min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8150179879762034, total=10.7min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.82764145306582, total=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed: 87.2min remaining:  7.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8186643180207618, total=12.7min\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8324023763764725, total=11.1min\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8149401664117953, total= 8.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 95.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_2_chi__k:  40000\n",
      "score_2_topk:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.82168, std: 0.00751, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.82036, std: 0.00760, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.82028, std: 0.00536, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.82200, std: 0.00751, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.82168, std: 0.00751, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.82036, std: 0.00760, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.82028, std: 0.00536, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.82200, std: 0.00751, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.82168, std: 0.00751, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.82036, std: 0.00760, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.82028, std: 0.00536, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.82200, std: 0.00751, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 40000}],\n",
       " {'features__tf__topk': 50, 'features__tf_idf__chi__k': 40000},\n",
       " 0.8220019441703408)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['tf_idf__chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_2_best_para['tf__chi__k'] = clf.best_params_['features__tf__chi__k']\n",
    "print('score_2_tf_idf__chi__k: ', score_2_best_para['tf_idf__chi__k'])\n",
    "print('score_2_tf__chi__k: ', score_2_best_para['tf__chi__k'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T14:31:03.196189Z",
     "start_time": "2018-08-08T14:31:03.160682Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_features_1 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_1_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Pipeline([\n",
    "                                        ('tf_k', Statskeywords(topk = 5000)),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_1_best_para['topk']))])), \n",
    "                                    ('len_stats', StatsFeatures_cor())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T14:31:03.232543Z",
     "start_time": "2018-08-08T14:31:03.199137Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_features_2 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_2_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Pipeline([\n",
    "                                        ('tf_k', Statskeywords(topk = 5000)),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_2_best_para['topk']))])), \n",
    "                                    ('len_stats', StatsFeatures_cor())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T14:36:21.356993Z",
     "start_time": "2018-08-08T14:31:03.235042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998931195724783\n",
      "accuracy_score:  0.878428927680798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.83      0.90      0.87       377\n",
      "          2       0.79      0.75      0.77       283\n",
      "          3       0.82      0.74      0.78       145\n",
      "          4       0.86      0.83      0.84       201\n",
      "          5       0.84      0.82      0.83       318\n",
      "          6       0.92      0.93      0.92       434\n",
      "          7       0.90      0.92      0.91       434\n",
      "          8       0.92      0.91      0.92      1016\n",
      "\n",
      "avg / total       0.88      0.88      0.88      3208\n",
      "\n",
      "confusion_matrix: \n",
      "[[340  17   0   1   5   0   2  12]\n",
      " [ 14 213   5   6  10  11   5  19]\n",
      " [  1   6 108   1   5   7   5  12]\n",
      " [  5   5   0 167   9   0   3  12]\n",
      " [  5  16   4   3 262   5  12  11]\n",
      " [  1   4   7   1   3 402  10   6]\n",
      " [  4   1   2   1  11   6 399  10]\n",
      " [ 39   7   5  15   8   7   8 927]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T14:47:01.617134Z",
     "start_time": "2018-08-08T14:36:21.359503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995991983967936\n",
      "accuracy_score:  0.8806109725685786\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.84      0.90      0.87       377\n",
      "          2       0.81      0.76      0.79       283\n",
      "          3       0.81      0.72      0.77       145\n",
      "          4       0.85      0.84      0.84       201\n",
      "          5       0.83      0.82      0.82       318\n",
      "          6       0.92      0.93      0.93       434\n",
      "          7       0.90      0.93      0.91       434\n",
      "          8       0.92      0.91      0.92      1016\n",
      "\n",
      "avg / total       0.88      0.88      0.88      3208\n",
      "\n",
      "confusion_matrix: \n",
      "[[340  15   1   1   4   0   2  14]\n",
      " [ 14 215   5   5  12   9   6  17]\n",
      " [  2   6 105   0   5   6   9  12]\n",
      " [  5   4   0 168  10   0   3  11]\n",
      " [  6  18   4   4 261   5   9  11]\n",
      " [  1   1   9   1   2 405   8   7]\n",
      " [  4   0   1   1  10   5 402  11]\n",
      " [ 34   5   4  18  11   8   7 929]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators、learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T14:47:01.643271Z",
     "start_time": "2018-08-08T14:47:01.619884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(10, 171, 30),\n",
       " 'classifier__learning_rate': [0.01, 0.1, 0.3]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(100,271,30), # 迭代次数/分类器个数\n",
    "                   classifier__learning_rate=[0.01, 0.1, 0.3]) \n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T16:06:36.189847Z",
     "start_time": "2018-08-08T14:47:01.645842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_n_estimators:  160\n",
      "score_1_learning_rate:  0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.79399, std: 0.00823, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 10},\n",
       "  mean: 0.81296, std: 0.00664, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 40},\n",
       "  mean: 0.82311, std: 0.00497, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 70},\n",
       "  mean: 0.82685, std: 0.00471, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 100},\n",
       "  mean: 0.83260, std: 0.00296, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 130},\n",
       "  mean: 0.83447, std: 0.00176, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 160},\n",
       "  mean: 0.82619, std: 0.00693, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 10},\n",
       "  mean: 0.84756, std: 0.00574, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 40},\n",
       "  mean: 0.85464, std: 0.00313, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 70},\n",
       "  mean: 0.85919, std: 0.00375, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 100},\n",
       "  mean: 0.86132, std: 0.00477, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 130},\n",
       "  mean: 0.86159, std: 0.00431, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 160},\n",
       "  mean: 0.83901, std: 0.00228, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 10},\n",
       "  mean: 0.85638, std: 0.00295, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 40},\n",
       "  mean: 0.85905, std: 0.00278, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 70},\n",
       "  mean: 0.85985, std: 0.00258, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 100},\n",
       "  mean: 0.85985, std: 0.00367, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 130},\n",
       "  mean: 0.85945, std: 0.00301, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 160}],\n",
       " {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 160},\n",
       " 0.8615898463593854)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "score_1_best_para['learning_rate'] = clf.best_params_['classifier__learning_rate']\n",
    "print('score_1_n_estimators: ', score_1_best_para['n_estimators'])\n",
    "print('score_1_learning_rate: ', score_1_best_para['learning_rate'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T16:16:26.750555Z",
     "start_time": "2018-08-08T16:06:36.192639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "accuracy_score:  0.8812344139650873\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.84      0.90      0.87       377\n",
      "          2       0.80      0.77      0.79       283\n",
      "          3       0.81      0.74      0.78       145\n",
      "          4       0.86      0.84      0.85       201\n",
      "          5       0.85      0.82      0.84       318\n",
      "          6       0.92      0.93      0.92       434\n",
      "          7       0.90      0.93      0.91       434\n",
      "          8       0.92      0.91      0.92      1016\n",
      "\n",
      "avg / total       0.88      0.88      0.88      3208\n",
      "\n",
      "confusion_matrix: \n",
      "[[338  17   1   1   4   0   2  14]\n",
      " [ 13 219   5   5  10   9   5  17]\n",
      " [  2   7 108   0   4   7   5  12]\n",
      " [  3   5   0 168   9   0   3  13]\n",
      " [  6  15   5   3 261   5  11  12]\n",
      " [  1   4   7   1   3 402   9   7]\n",
      " [  4   0   2   1   9   6 402  10]\n",
      " [ 35   6   5  17   7   8   9 929]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T16:16:26.784035Z",
     "start_time": "2018-08-08T16:16:26.752952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(30, 161, 20),\n",
       " 'classifier__learning_rate': [0.01, 0.1, 0.3]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(30,161,20), # 迭代次数/分类器个数\n",
    "                   classifier__learning_rate=[0.01, 0.1, 0.3]) \n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T19:33:33.797828Z",
     "start_time": "2018-08-08T16:16:26.786478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_2_n_estimators:  150\n",
      "score_2_learning_rate:  0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.76202, std: 0.01255, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 30},\n",
       "  mean: 0.76853, std: 0.00973, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 50},\n",
       "  mean: 0.77944, std: 0.00715, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 70},\n",
       "  mean: 0.77983, std: 0.00743, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 90},\n",
       "  mean: 0.78303, std: 0.00422, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 110},\n",
       "  mean: 0.78614, std: 0.00210, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 130},\n",
       "  mean: 0.78836, std: 0.00123, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 150},\n",
       "  mean: 0.79746, std: 0.00808, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 30},\n",
       "  mean: 0.80711, std: 0.00718, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 50},\n",
       "  mean: 0.81476, std: 0.00762, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 70},\n",
       "  mean: 0.81797, std: 0.00799, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 90},\n",
       "  mean: 0.82044, std: 0.00796, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 110},\n",
       "  mean: 0.82398, std: 0.00650, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 130},\n",
       "  mean: 0.82406, std: 0.00856, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 150},\n",
       "  mean: 0.81757, std: 0.00901, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 30},\n",
       "  mean: 0.82064, std: 0.00668, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 50},\n",
       "  mean: 0.82158, std: 0.00759, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 70},\n",
       "  mean: 0.82152, std: 0.00697, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 90},\n",
       "  mean: 0.82140, std: 0.00830, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 110},\n",
       "  mean: 0.82039, std: 0.00821, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 130},\n",
       "  mean: 0.82186, std: 0.00799, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 150}],\n",
       " {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 150},\n",
       " 0.824055238322389)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "score_2_best_para['learning_rate'] = clf.best_params_['classifier__learning_rate']\n",
    "print('score_2_n_estimators: ', score_2_best_para['n_estimators'])\n",
    "print('score_2_learning_rate: ', score_2_best_para['learning_rate'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T19:38:26.775707Z",
     "start_time": "2018-08-08T19:33:33.801013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "accuracy_score:  0.8793640897755611\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.83      0.90      0.87       377\n",
      "          2       0.80      0.76      0.78       283\n",
      "          3       0.81      0.74      0.78       145\n",
      "          4       0.85      0.83      0.84       201\n",
      "          5       0.85      0.82      0.83       318\n",
      "          6       0.92      0.93      0.92       434\n",
      "          7       0.90      0.93      0.91       434\n",
      "          8       0.92      0.91      0.91      1016\n",
      "\n",
      "avg / total       0.88      0.88      0.88      3208\n",
      "\n",
      "confusion_matrix: \n",
      "[[339  16   1   1   4   0   2  14]\n",
      " [ 14 216   5   6  10   9   5  18]\n",
      " [  2   7 108   1   4   7   5  11]\n",
      " [  4   5   0 167   9   0   3  13]\n",
      " [  6  15   5   3 260   5  12  12]\n",
      " [  1   4   7   1   3 402   9   7]\n",
      " [  4   0   2   1   9   6 402  10]\n",
      " [ 36   7   5  16   8   8   9 927]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  n_estimators = score_2_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_2_best_para['learning_rate'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_depth、min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T19:38:26.788775Z",
     "start_time": "2018-08-08T19:38:26.780018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': range(3, 14, 2),\n",
       " 'classifier__min_child_weight': [4, 5, 6]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_child_weight=[4, 5, 6]) \n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T21:02:22.225079Z",
     "start_time": "2018-08-08T19:38:26.791539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_max_depth:  5\n",
      "score_1_min_child_weight:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.85691, std: 0.00251, params: {'classifier__max_depth': 3, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.85731, std: 0.00522, params: {'classifier__max_depth': 3, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.85424, std: 0.00512, params: {'classifier__max_depth': 3, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.86399, std: 0.00251, params: {'classifier__max_depth': 5, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.86012, std: 0.00566, params: {'classifier__max_depth': 5, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.86025, std: 0.00248, params: {'classifier__max_depth': 5, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.86039, std: 0.00154, params: {'classifier__max_depth': 7, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.86039, std: 0.00265, params: {'classifier__max_depth': 7, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.85892, std: 0.00384, params: {'classifier__max_depth': 7, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.85905, std: 0.00463, params: {'classifier__max_depth': 9, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.85825, std: 0.00278, params: {'classifier__max_depth': 9, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.85852, std: 0.00534, params: {'classifier__max_depth': 9, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.85959, std: 0.00458, params: {'classifier__max_depth': 11, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.85985, std: 0.00282, params: {'classifier__max_depth': 11, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.85611, std: 0.00552, params: {'classifier__max_depth': 11, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.85999, std: 0.00428, params: {'classifier__max_depth': 13, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.85905, std: 0.00602, params: {'classifier__max_depth': 13, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.85665, std: 0.00537, params: {'classifier__max_depth': 13, 'classifier__min_child_weight': 6}],\n",
       " {'classifier__max_depth': 5, 'classifier__min_child_weight': 4},\n",
       " 0.8639946559786239)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "score_1_best_para['min_child_weight'] = clf.best_params_['classifier__min_child_weight']\n",
    "print('score_1_max_depth: ', score_1_best_para['max_depth'])\n",
    "print('score_1_min_child_weight: ', score_1_best_para['min_child_weight'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T02:07:57.161781Z",
     "start_time": "2018-08-09T02:04:40.009269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9931863727454909\n",
      "accuracy_score:  0.8884039900249376\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.85      0.89      0.87       377\n",
      "          2       0.83      0.77      0.80       283\n",
      "          3       0.83      0.76      0.79       145\n",
      "          4       0.84      0.84      0.84       201\n",
      "          5       0.86      0.84      0.85       318\n",
      "          6       0.93      0.94      0.93       434\n",
      "          7       0.90      0.93      0.92       434\n",
      "          8       0.92      0.92      0.92      1016\n",
      "\n",
      "avg / total       0.89      0.89      0.89      3208\n",
      "\n",
      "confusion_matrix: \n",
      "[[337  15   1   5   4   0   2  13]\n",
      " [ 12 219   4   5  10  10   5  18]\n",
      " [  2   5 110   0   3   6   8  11]\n",
      " [  4   3   0 168   7   0   5  14]\n",
      " [  6  13   3   3 266   5  10  12]\n",
      " [  1   2   6   1   3 408   7   6]\n",
      " [  4   0   2   1   8   5 405   9]\n",
      " [ 30   6   6  18   7   5   7 937]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(gamma = 0, \n",
    "                                                  n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],        \n",
    "                                                  max_depth = score_1_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_1_best_para['min_child_weight'],                                                   \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T01:54:29.987011Z",
     "start_time": "2018-08-09T01:54:29.980299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chi__k': 2000,\n",
       " 'topk': 50,\n",
       " 'n_estimators': 160,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 4}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_1_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T07:14:39.829787Z",
     "start_time": "2018-08-10T07:14:39.794524Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_features_1 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=20000))\n",
    "                                        ])),\n",
    "                                    ('tf', Pipeline([\n",
    "                                        ('tf_k', Statskeywords(topk = 5000)),\n",
    "                                        ('chi', SelectKBest(chi2, k=50))])), \n",
    "                                    ('len_stats', StatsFeatures_cor())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T07:32:57.566003Z",
     "start_time": "2018-08-10T07:14:42.601503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952491011813046\n",
      "accuracy_score:  0.8855602156980228\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.87      0.87      0.87       425\n",
      "          2       0.81      0.81      0.81       320\n",
      "          3       0.88      0.79      0.83       215\n",
      "          4       0.89      0.79      0.83       206\n",
      "          5       0.83      0.81      0.82       322\n",
      "          6       0.92      0.96      0.94       407\n",
      "          7       0.93      0.92      0.92       432\n",
      "          8       0.90      0.94      0.92      1011\n",
      "\n",
      "avg / total       0.89      0.89      0.88      3338\n",
      "\n",
      "confusion_matrix: \n",
      "[[370  13   0   4   3   0   4  31]\n",
      " [ 10 258   8   6  10   9   3  16]\n",
      " [  1  10 170   0   8   8   4  14]\n",
      " [  6   7   1 162  14   0   0  16]\n",
      " [  8  21   3   2 262   6  10  10]\n",
      " [  0   1   3   0   1 391   4   7]\n",
      " [  4   2   3   0  10   4 396  13]\n",
      " [ 26   6   6   9   7   6   4 947]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(gamma = 0, \n",
    "                                                  n_estimators = 160,\n",
    "                                                  learning_rate = 0.1,        \n",
    "                                                  max_depth = 5,\n",
    "                                                  min_child_weight = 4,                                                   \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T09:51:12.095774Z",
     "start_time": "2018-06-13T09:51:11.751754Z"
    },
    "collapsed": true
   },
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:50:23.216060Z",
     "start_time": "2018-08-27T05:50:19.674164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/cbrc_8classifier_0827.pkl.z']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"model/cbrc_8classifier_0827.pkl.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T06:53:13.560161Z",
     "start_time": "2018-07-03T06:53:13.537160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    import datetime as dt\n",
    "    \n",
    "    def output_HTML(read_file, output_file):\n",
    "        from nbconvert import HTMLExporter\n",
    "        import codecs\n",
    "        import nbformat\n",
    "        exporter = HTMLExporter()\n",
    "        # read_file is '.ipynb', output_file is '.html'\n",
    "        output_notebook = nbformat.read(read_file, as_version=4)\n",
    "        output, resources = exporter.from_notebook_node(output_notebook)\n",
    "        codecs.open(output_file, 'w', encoding='utf-8').write(output)\n",
    "\n",
    "    html_file_folder = 'html_files'\n",
    "    if not os.path.exists(html_file_folder):\n",
    "        os.makedirs(html_file_folder)\n",
    "\n",
    "    today = dt.datetime.now().strftime('%Y%m%d')\n",
    "    current_file = 'circ_cor_model_2_train.ipynb'\n",
    "    output_file = 'html_files\\%s_%s.html'%(os.path.splitext(current_file)[0], today)\n",
    "    output_HTML(current_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

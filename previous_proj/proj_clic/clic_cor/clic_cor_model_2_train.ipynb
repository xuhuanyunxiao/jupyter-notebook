{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 中国人寿 相关性模型 2 训练 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:35:45.013170Z",
     "start_time": "2018-07-17T06:35:41.892617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.022 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "##load packages, needed\n",
    "# encoding=utf-8\n",
    "\n",
    "import jieba\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2,mutual_info_classif,f_classif \n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "\n",
    "import joblib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_cor\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io import sql\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:35:45.087773Z",
     "start_time": "2018-07-17T06:35:45.013170Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StatsFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neg = set()\n",
    "        f = open(\"corpus/neg_words.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.neg.add(content)\n",
    "        f.close()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def getcnt(self,x):        \n",
    "        return len(list(set(x)))\n",
    "\n",
    "    def getnegcnt(self,x):\n",
    "        negcnt = 0\n",
    "        words = x.split()\n",
    "        for w in words:\n",
    "            if w in self.neg:\n",
    "                negcnt = negcnt+1\n",
    "        return negcnt\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = []\n",
    "        for x in X:\n",
    "            if len(x) == 0:\n",
    "                length  = 1\n",
    "            else :\n",
    "                length = len(x)\n",
    "            data.append([len(x),self.getcnt(x),self.getcnt(x)/length,\n",
    "                         self.getnegcnt(x),self.getnegcnt(x)/length])            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上一版模型读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T06:49:41.731325Z",
     "start_time": "2018-07-03T06:49:38.024113Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 上一版模型\n",
    "# from sklearn.externals import joblib\n",
    "# pipeline_old = joblib.load( \"model/circ_cor_0625.pkl.z\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:35:49.519199Z",
     "start_time": "2018-07-17T06:35:49.462196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2988\n",
      "2988\n"
     ]
    }
   ],
   "source": [
    "folder = '20180717'\n",
    "\n",
    "# 相关数据\n",
    "corpus_cor = []\n",
    "label_cor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_cor_0717.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_cor.append(f)\n",
    "    label_cor.append(1)\n",
    "fid.close()\n",
    "print(len(corpus_cor))\n",
    "print(len(label_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:45:14.435647Z",
     "start_time": "2018-07-17T06:45:14.219635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3358\n",
      "3358\n"
     ]
    }
   ],
   "source": [
    "# 不相关数据\n",
    "corpus_uncor = []\n",
    "label_uncor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_uncor_0717.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_uncor.append(f)\n",
    "    label_uncor.append(0)\n",
    "fid.close()\n",
    "print(len(corpus_uncor))\n",
    "print(len(label_uncor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割训练集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:45:16.459754Z",
     "start_time": "2018-07-17T06:45:16.400751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集： 5711\n",
      "训练集-各类数量： Counter({0: 3055, 1: 2656})\n",
      "测试集： 635\n",
      "测试集-各类数量： Counter({1: 332, 0: 303})\n"
     ]
    }
   ],
   "source": [
    "# 未加入系统中噪音\n",
    "corpus = corpus_cor + corpus_uncor\n",
    "label = label_cor + label_uncor\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, label, test_size=0.1, random_state=42)\n",
    "print('训练集：',len(y_train))\n",
    "print('训练集-各类数量：',Counter(y_train))\n",
    "print('测试集：',len(y_test))\n",
    "print('测试集-各类数量：',Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:47:29.226829Z",
     "start_time": "2018-07-17T06:45:20.875386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0...tate=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tf_idf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=20000))\n",
    "        ])),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', XGBClassifier(max_depth=7,objective='multi:softmax', num_class=2))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:47:41.645509Z",
     "start_time": "2018-07-17T06:47:31.387737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9891437576606549\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:47:45.012473Z",
     "start_time": "2018-07-17T06:47:43.813405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.9401574803149606\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.93      0.94       303\n",
      "          1       0.93      0.95      0.94       332\n",
      "\n",
      "avg / total       0.94      0.94      0.94       635\n",
      "\n",
      "confusion_matrix: \n",
      "[[281  22]\n",
      " [ 16 316]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:47:47.220160Z",
     "start_time": "2018-07-17T06:47:47.215159Z"
    }
   },
   "outputs": [],
   "source": [
    "# 上一版模型 \n",
    "# y_pred_class = pipeline_old.predict(X_test)\n",
    "# print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "# print(metrics.classification_report(y_test, y_pred_class))\n",
    "# print('confusion_matrix: ')\n",
    "# print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T09:51:12.095774Z",
     "start_time": "2018-06-13T09:51:11.751754Z"
    },
    "collapsed": true
   },
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:47:54.338334Z",
     "start_time": "2018-07-17T06:47:49.376283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/clic_cor_0717.pkl.z']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"model/clic_cor_0717.pkl.z\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T06:53:13.560161Z",
     "start_time": "2018-07-03T06:53:13.537160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    import datetime as dt\n",
    "    \n",
    "    def output_HTML(read_file, output_file):\n",
    "        from nbconvert import HTMLExporter\n",
    "        import codecs\n",
    "        import nbformat\n",
    "        exporter = HTMLExporter()\n",
    "        # read_file is '.ipynb', output_file is '.html'\n",
    "        output_notebook = nbformat.read(read_file, as_version=4)\n",
    "        output, resources = exporter.from_notebook_node(output_notebook)\n",
    "        codecs.open(output_file, 'w', encoding='utf-8').write(output)\n",
    "\n",
    "    html_file_folder = 'html_files'\n",
    "    if not os.path.exists(html_file_folder):\n",
    "        os.makedirs(html_file_folder)\n",
    "\n",
    "    today = dt.datetime.now().strftime('%Y%m%d')\n",
    "    current_file = 'circ_cor_model_2_train.ipynb'\n",
    "    output_file = 'html_files\\%s_%s.html'%(os.path.splitext(current_file)[0], today)\n",
    "    output_HTML(current_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

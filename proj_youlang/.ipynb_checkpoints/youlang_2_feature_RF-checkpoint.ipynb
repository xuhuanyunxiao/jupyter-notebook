{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本文件说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优浪公司项目\n",
    "- 预处理及特征值计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:47.633887Z",
     "start_time": "2018-07-24T03:12:44.761523Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##load packages, needed\n",
    "# encoding=utf-8\n",
    "\n",
    "import jieba\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import joblib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_cor\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io import sql\n",
    "from collections import Counter\n",
    "\n",
    "from jieba import analyse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:47.659891Z",
     "start_time": "2018-07-24T03:12:47.636888Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getkeywords(X, N = 1000):\n",
    "    '''\n",
    "    训练时生成，合并所有记录，取N个关键词\n",
    "    '''\n",
    "    textrank = analyse.textrank\n",
    "\n",
    "    text_combined = ' '.join(X)\n",
    "    keywords = textrank(text_combined, topK = N)\n",
    "    print('keywords num: ', len(keywords))\n",
    "    if len(keywords) < N : \n",
    "        N  = len(keywords)\n",
    "\n",
    "    if keywords:\n",
    "        f = open(\"corpus/keywords.txt\",\"w+\", encoding='UTF-8')\n",
    "        for content in keywords:\n",
    "            content = content.strip()\n",
    "            if content != ':AB:':\n",
    "                f.write(content + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:47.791407Z",
     "start_time": "2018-07-24T03:12:47.672392Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Statskeywords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, topk = 100):\n",
    "        self.topk = topk\n",
    "#         print(self.topk)\n",
    "        self.keywords = set()\n",
    "        f = open(\"corpus/keywords.txt\",\"r+\", encoding='UTF-8')\n",
    "        num = 0\n",
    "        for content in f:\n",
    "            if num < topk:\n",
    "                self.keywords.add(content.strip().replace('\\n', ''))\n",
    "            num += 1\n",
    "        f.close() \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本中关键词的词频\n",
    "        '''                        \n",
    "        data = []\n",
    "        for x in X:\n",
    "            words = x.split()\n",
    "            word_tf = []\n",
    "            keycnt = 0\n",
    "            for kw in self.keywords:\n",
    "                word_tf.append(words.count(kw)) # 各个关键词的词频\n",
    "                if kw in words:keycnt+=1\n",
    "            word_tf.append(keycnt) # 关键词的个数\n",
    "            data.append(word_tf)            \n",
    "        return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:47.906422Z",
     "start_time": "2018-07-24T03:12:47.795408Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StatsFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neg = set()\n",
    "        f = open(\"corpus/neg_words.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.neg.add(content)\n",
    "        f.close()       \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def getcnt(self,x): \n",
    "        '''词个数'''\n",
    "        return len(list(set(x.split())))\n",
    "\n",
    "    def getnegcnt(self,x):\n",
    "        '''负面词个数'''\n",
    "        negcnt = 0\n",
    "        words = x.split()\n",
    "        for w in words:\n",
    "            if w in self.neg:\n",
    "                negcnt = negcnt+1\n",
    "        return negcnt\n",
    "\n",
    "    def getrepcnt(self,x):\n",
    "        '''重复词个数'''\n",
    "        repcnt =0\n",
    "        words = x.split()        \n",
    "        for w in list(set(words)):\n",
    "            if words.count(w)>1: # 记录重复词汇（词频大于1）\n",
    "                repcnt += 1\n",
    "        return repcnt\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本长度、词个数、词比例、\n",
    "        负面词个数、负面词比例、\n",
    "        重复词个数、重复词比例\n",
    "        '''\n",
    "        data = []\n",
    "        for x in X:\n",
    "            if len(x) == 0:\n",
    "                length  = 1\n",
    "            else :\n",
    "                length = len(x)\n",
    "            data.append([len(x),self.getcnt(x),self.getcnt(x)/length,\n",
    "                         self.getnegcnt(x),self.getnegcnt(x)/length,\n",
    "                         self.getrepcnt(x),self.getrepcnt(x)/length])            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理后数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:47.986432Z",
     "start_time": "2018-07-24T03:12:47.909922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 诈骗电话\n",
    "corpus_pos = []\n",
    "label_pos = []\n",
    "\n",
    "filename = 'data/pos_pre_20180723.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_pos.append(f)\n",
    "    label_pos.append(1)\n",
    "fid.close()\n",
    "print(len(corpus_pos))\n",
    "print(len(label_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:48.068943Z",
     "start_time": "2018-07-24T03:12:47.991933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 非诈骗电话\n",
    "corpus_neg = []\n",
    "label_neg = []\n",
    "\n",
    "filename = 'data/neg_pre_20180723.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_neg.append(f)\n",
    "    label_neg.append(0)\n",
    "fid.close()\n",
    "print(len(corpus_neg))\n",
    "print(len(label_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:48.270968Z",
     "start_time": "2018-07-24T03:12:48.073943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111\n",
      "6111\n"
     ]
    }
   ],
   "source": [
    "folder = '20180703'\n",
    "\n",
    "# 相关数据\n",
    "corpus_cor = []\n",
    "label_cor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_cor_0703.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:    \n",
    "    corpus_cor.append(f)\n",
    "    label_cor.append(1)\n",
    "fid.close()\n",
    "print(len(corpus_cor))\n",
    "print(len(label_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:48.471994Z",
     "start_time": "2018-07-24T03:12:48.274969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8949\n",
      "8949\n"
     ]
    }
   ],
   "source": [
    "# 不相关数据\n",
    "corpus_uncor = []\n",
    "label_uncor = []\n",
    "\n",
    "filename = 'data/{0}/corpus_pre_uncor_0703.txt'.format(folder)\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    corpus_uncor.append(f)\n",
    "    label_uncor.append(0)\n",
    "fid.close()\n",
    "print(len(corpus_uncor))\n",
    "print(len(label_uncor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:48.590009Z",
     "start_time": "2018-07-24T03:12:48.476494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集： 180\n",
      "训练集-各类数量： Counter({0: 90, 1: 90})\n",
      "测试集： 20\n",
      "测试集-各类数量： Counter({1: 10, 0: 10})\n"
     ]
    }
   ],
   "source": [
    "# corpus = corpus_pos + corpus_neg\n",
    "# label = label_pos + label_neg\n",
    "corpus = corpus_cor[:100] + corpus_uncor[:100]\n",
    "label = label_cor[:100] + label_uncor[:100]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, label, test_size=0.1, random_state=42)\n",
    "print('训练集：',len(y_train))\n",
    "print('训练集-各类数量：',Counter(y_train))\n",
    "print('测试集：',len(y_test))\n",
    "print('测试集-各类数量：',Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:48.659518Z",
     "start_time": "2018-07-24T03:12:48.594009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0: getkeywords(corpus, N = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型:RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:12:48.726526Z",
     "start_time": "2018-07-24T03:12:48.663518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_print(pipeline, param_grid):\n",
    "    train_res = []\n",
    "    scores = ['roc_auc', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    for score in scores:\n",
    "        print(\"### Tuning hyper-parameters for %s\" % score)\n",
    "        \n",
    "        t0 = datetime.datetime.now()\n",
    "        clf = GridSearchCV(pipeline, param_grid, cv=3, # n_jobs = 2, \n",
    "                           scoring = score, iid=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        t1 = datetime.datetime.now()\n",
    "        print ('  耗时： %s s'%(t1 - t0).seconds)\n",
    "\n",
    "        print(\"---- Best parameters set found on development set:\")\n",
    "        print(clf.best_params_)\n",
    "        print(\"---- Grid scores on development set:\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        \n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"    %0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "\n",
    "        print(\"---- Detailed classification report:\")\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "\n",
    "        train_res.append([score,clf.cv_results_,  clf.grid_scores_, \n",
    "                          clf.best_params_, clf.best_score_])\n",
    "        \n",
    "    return train_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:11:55.129146Z",
     "start_time": "2018-07-24T06:11:52.383297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9944444444444445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0...estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( random_state=0))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline\n",
    "# clf_xgb = GridSearchCV(pipeline, param_grid=param_grid, verbose=10, cv = 10)\n",
    "# clf_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:11:56.119272Z",
     "start_time": "2018-07-24T06:11:55.998756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.95\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.95      0.95      0.95        20\n",
      "\n",
      "confusion_matrix: \n",
      "[[ 9  1]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:56:06.974962Z",
     "start_time": "2018-07-24T03:56:06.969461Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_best_para = {}\n",
    "recall_best_para = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:56:08.548662Z",
     "start_time": "2018-07-24T03:56:08.527659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(10, 71, 10)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(10,71,10)) # 迭代次数/分类器个数)\n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:57:19.210134Z",
     "start_time": "2018-07-24T03:56:09.735312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_n_estimators:  70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.88222, std: 0.05346, params: {'classifier__n_estimators': 10},\n",
       "  mean: 0.87630, std: 0.06468, params: {'classifier__n_estimators': 20},\n",
       "  mean: 0.87741, std: 0.07103, params: {'classifier__n_estimators': 30},\n",
       "  mean: 0.88259, std: 0.05725, params: {'classifier__n_estimators': 40},\n",
       "  mean: 0.88315, std: 0.06275, params: {'classifier__n_estimators': 50},\n",
       "  mean: 0.88222, std: 0.06802, params: {'classifier__n_estimators': 60},\n",
       "  mean: 0.88611, std: 0.06859, params: {'classifier__n_estimators': 70}],\n",
       " {'classifier__n_estimators': 70},\n",
       " 0.8861111111111111)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, cv = 3, scoring='roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "auc_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "print('auc_n_estimators: ', auc_best_para['n_estimators'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:57:20.836341Z",
     "start_time": "2018-07-24T03:57:20.803837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(10, 71, 10)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(10,71,10))  # 迭代次数/分类器个数)\n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:58:34.510196Z",
     "start_time": "2018-07-24T03:57:22.505053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_n_estimators:  40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.81111, std: 0.04374, params: {'classifier__n_estimators': 10},\n",
       "  mean: 0.81111, std: 0.08315, params: {'classifier__n_estimators': 20},\n",
       "  mean: 0.80556, std: 0.07495, params: {'classifier__n_estimators': 30},\n",
       "  mean: 0.82778, std: 0.06136, params: {'classifier__n_estimators': 40},\n",
       "  mean: 0.80556, std: 0.06983, params: {'classifier__n_estimators': 50},\n",
       "  mean: 0.80000, std: 0.06804, params: {'classifier__n_estimators': 60},\n",
       "  mean: 0.82222, std: 0.06431, params: {'classifier__n_estimators': 70}],\n",
       " {'classifier__n_estimators': 40},\n",
       " 0.8277777777777777)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, cv = 3, scoring='recall_macro')\n",
    "clf.fit(X_train, y_train)\n",
    "recall_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "print('recall_n_estimators: ', recall_best_para['n_estimators'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### max_depth、min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T03:58:35.652341Z",
     "start_time": "2018-07-24T03:58:35.621337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': range(3, 14, 2),\n",
       " 'classifier__min_samples_split': range(50, 201, 20)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = auc_best_para['n_estimators'], random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T04:06:27.004695Z",
     "start_time": "2018-07-24T03:58:36.815489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_max_depth:  11\n",
      "auc_min_samples_split:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.87222, std: 0.07238, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.85111, std: 0.06919, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.87574, std: 0.07249, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.85111, std: 0.06919, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.87648, std: 0.07225, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.85111, std: 0.06919, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.87574, std: 0.07169, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.85111, std: 0.06919, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.87722, std: 0.07283, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.85111, std: 0.06919, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.87722, std: 0.07283, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.85111, std: 0.06919, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 190}],\n",
       " {'classifier__max_depth': 11, 'classifier__min_samples_split': 50},\n",
       " 0.8772222222222222)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, cv = 3, scoring='roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "auc_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "auc_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('auc_max_depth: ', auc_best_para['max_depth'])\n",
    "print('auc_min_samples_split: ', auc_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T04:06:28.104335Z",
     "start_time": "2018-07-24T04:06:28.055329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': range(3, 14, 2),\n",
       " 'classifier__min_samples_split': range(50, 201, 20)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = recall_best_para['n_estimators'], random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T04:13:55.286620Z",
     "start_time": "2018-07-24T04:06:29.253481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_max_depth:  5\n",
      "recall_min_samples_split:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.78333, std: 0.05443, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.77778, std: 0.05500, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 3, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.79444, std: 0.04374, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.77778, std: 0.05500, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 5, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.79444, std: 0.04374, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.77778, std: 0.05500, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 7, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.79444, std: 0.04374, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.77778, std: 0.05500, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 9, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.79444, std: 0.04374, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.77778, std: 0.05500, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 11, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.79444, std: 0.04374, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.77778, std: 0.05500, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__max_depth': 13, 'classifier__min_samples_split': 190}],\n",
       " {'classifier__max_depth': 5, 'classifier__min_samples_split': 50},\n",
       " 0.7944444444444444)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, cv = 3, scoring='recall_macro')\n",
    "clf.fit(X_train, y_train)\n",
    "recall_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "recall_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('recall_max_depth: ', recall_best_para['max_depth'])\n",
    "print('recall_min_samples_split: ', recall_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_samples_leaf、min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T05:43:15.212744Z",
     "start_time": "2018-07-24T05:43:15.189241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__min_samples_leaf': range(10, 60, 10),\n",
       " 'classifier__min_samples_split': range(50, 201, 20)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = auc_best_para['n_estimators'], \n",
    "                                          max_depth = auc_best_para['max_depth'],                                          \n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_3 = dict(classifier__min_samples_leaf=range(10,60,10), \n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T05:50:32.349754Z",
     "start_time": "2018-07-24T05:43:40.709982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_min_samples_leaf:  10\n",
      "auc_min_samples_split:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.85093, std: 0.06932, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.83852, std: 0.07765, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.76185, std: 0.07366, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.76130, std: 0.09979, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.67222, std: 0.06651, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.66944, std: 0.06776, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.56370, std: 0.06974, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.56370, std: 0.06974, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 190}],\n",
       " {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 50},\n",
       " 0.8509259259259259)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_3, cv = 3, scoring='roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "auc_best_para['min_samples_leaf'] = clf.best_params_['classifier__min_samples_leaf']\n",
    "auc_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('auc_min_samples_leaf: ', auc_best_para['min_samples_leaf'])\n",
    "print('auc_min_samples_split: ', auc_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T05:50:33.618915Z",
     "start_time": "2018-07-24T05:50:33.591411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__min_samples_leaf': range(10, 60, 10),\n",
       " 'classifier__min_samples_split': range(50, 201, 20)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = recall_best_para['n_estimators'], \n",
    "                                          max_depth = recall_best_para['max_depth'], \n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_3 = dict(classifier__min_samples_leaf=range(10,60,10), \n",
    "                    classifier__min_samples_split=range(50,201,20)) # 内部节点再划分所需最小样本数)\n",
    "param_grid_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T05:57:21.510710Z",
     "start_time": "2018-07-24T05:50:34.740057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_min_samples_leaf:  10\n",
      "recall_min_samples_split:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.77222, std: 0.04157, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.75000, std: 0.05443, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.66667, std: 0.04907, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.66667, std: 0.04082, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 20, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.60000, std: 0.04082, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.60000, std: 0.04082, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 30, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 40, 'classifier__min_samples_split': 190},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 50},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 70},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 90},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 110},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 130},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 150},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 170},\n",
       "  mean: 0.50000, std: 0.00000, params: {'classifier__min_samples_leaf': 50, 'classifier__min_samples_split': 190}],\n",
       " {'classifier__min_samples_leaf': 10, 'classifier__min_samples_split': 50},\n",
       " 0.7722222222222223)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_3, cv = 3, scoring='recall_macro')\n",
    "clf.fit(X_train, y_train)\n",
    "recall_best_para['min_samples_leaf'] = clf.best_params_['classifier__min_samples_leaf']\n",
    "recall_best_para['min_samples_split'] = clf.best_params_['classifier__min_samples_split']\n",
    "print('recall_min_samples_leaf: ', recall_best_para['min_samples_leaf'])\n",
    "print('recall_min_samples_split: ', recall_best_para['min_samples_split'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:00:06.157618Z",
     "start_time": "2018-07-24T06:00:06.136115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_features': [0.25, 0.5, 0.75, 1.0]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = auc_best_para['n_estimators'], \n",
    "                                          max_depth = auc_best_para['max_depth'],  \n",
    "                                          min_samples_leaf = auc_best_para['min_samples_leaf'],\n",
    "                                          min_samples_split = auc_best_para['min_samples_split'],\n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_4 = dict(classifier__max_features=[0.25, 0.5, 0.75, 1.0])\n",
    "param_grid_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:00:38.421215Z",
     "start_time": "2018-07-24T06:00:07.210252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_max_features:  0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.84000, std: 0.05644, params: {'classifier__max_features': 0.25},\n",
       "  mean: 0.84111, std: 0.04914, params: {'classifier__max_features': 0.5},\n",
       "  mean: 0.82648, std: 0.03532, params: {'classifier__max_features': 0.75},\n",
       "  mean: 0.82241, std: 0.02688, params: {'classifier__max_features': 1.0}],\n",
       " {'classifier__max_features': 0.5},\n",
       " 0.8411111111111111)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_4, cv = 3, scoring='roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "auc_best_para['max_features'] = clf.best_params_['classifier__max_features']\n",
    "print('auc_max_features: ', auc_best_para['max_features'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:00:39.345332Z",
     "start_time": "2018-07-24T06:00:39.322329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_features': [0.25, 0.5, 0.75, 1.0]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = recall_best_para['n_estimators'], \n",
    "                                          max_depth = recall_best_para['max_depth'],  \n",
    "                                          min_samples_leaf = recall_best_para['min_samples_leaf'],\n",
    "                                          min_samples_split = recall_best_para['min_samples_split'],\n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_4 = dict(classifier__max_features=[0.25, 0.5, 0.75, 1.0])\n",
    "param_grid_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:01:10.438281Z",
     "start_time": "2018-07-24T06:00:40.161436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_max_features:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.73333, std: 0.03600, params: {'classifier__max_features': 0.25},\n",
       "  mean: 0.73333, std: 0.06236, params: {'classifier__max_features': 0.5},\n",
       "  mean: 0.73333, std: 0.04714, params: {'classifier__max_features': 0.75},\n",
       "  mean: 0.73889, std: 0.03425, params: {'classifier__max_features': 1.0}],\n",
       " {'classifier__max_features': 1.0},\n",
       " 0.7388888888888889)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_4, cv = 3, scoring='recall_macro')\n",
    "clf.fit(X_train, y_train)\n",
    "recall_best_para['max_features'] = clf.best_params_['classifier__max_features']\n",
    "print('recall_max_features: ', recall_best_para['max_features'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:12:28.257853Z",
     "start_time": "2018-07-24T06:12:28.250352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 11,\n",
       " 'max_features': 0.5,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 50,\n",
       " 'n_estimators': 70}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:12:39.195242Z",
     "start_time": "2018-07-24T06:12:39.185740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5,\n",
       " 'max_features': 1.0,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 50,\n",
       " 'n_estimators': 40}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:01:14.177755Z",
     "start_time": "2018-07-24T06:01:11.266386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8555555555555555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0...estimators=70, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = auc_best_para['n_estimators'], \n",
    "                                          max_depth = auc_best_para['max_depth'],  \n",
    "                                          min_samples_leaf = auc_best_para['min_samples_leaf'],\n",
    "                                          min_samples_split = auc_best_para['min_samples_split'],\n",
    "                                          max_features = auc_best_para['max_features'],\n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:01:15.091371Z",
     "start_time": "2018-07-24T06:01:14.972856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.95\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.95      0.95      0.95        20\n",
      "\n",
      "confusion_matrix: \n",
      "[[10  0]\n",
      " [ 1  9]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:01:18.777840Z",
     "start_time": "2018-07-24T06:01:15.879471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0...estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=200))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 50)),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier( n_estimators = recall_best_para['n_estimators'], \n",
    "                                          max_depth = recall_best_para['max_depth'],  \n",
    "                                          min_samples_leaf = recall_best_para['min_samples_leaf'],\n",
    "                                          min_samples_split = recall_best_para['min_samples_split'],\n",
    "                                          max_features = recall_best_para['max_features'],\n",
    "                                          random_state=0))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-24T06:01:19.729460Z",
     "start_time": "2018-07-24T06:01:19.585442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.95\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.95      0.95      0.95        20\n",
      "\n",
      "confusion_matrix: \n",
      "[[10  0]\n",
      " [ 1  9]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

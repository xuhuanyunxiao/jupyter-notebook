{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Stoplists for Historical Languages\n",
    "Code repository associated with the Constructing Stoplists for Historical Languages for Digital Classics Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer = CorpusImporter('latin')\n",
    "# corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.stop.latin import CorpusStoplist\n",
    "from cltk.stop.latin import PERSEUS_STOPS\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for preprocessing texts\n",
    "\n",
    "def truncate_text(text):\n",
    "    temp = text[500:-500]\n",
    "    start = temp.find(' ')\n",
    "    end = temp.rfind(' ')\n",
    "    return temp[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "import html\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "replacer = JVReplacer()\n",
    "\n",
    "def preprocess(text):    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2152 files in the CLTK Latin Library corpus.\n"
     ]
    }
   ],
   "source": [
    "# Load CLTK Latin Library corpus; get size\n",
    "\n",
    "ll_files = latinlibrary.fileids()\n",
    "ll_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ll_files]\n",
    "ll_docs = [doc for doc in ll_docs if len(doc) > 100]\n",
    "ll_size = len(ll_files)\n",
    "\n",
    "# CITED IN ARTICLE\n",
    "print(f'There are {ll_size} files in the CLTK Latin Library corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16287634 tokens in the CLTK Latin Library corpus.\n",
      "There are 482172 unique tokens in the CLTK Latin Library corpus.\n",
      "Of the tokens appearing in the CLTK Latin Library corpus, 225612 tokens appear once.\n"
     ]
    }
   ],
   "source": [
    "# Get tokens for Latin Library; get stats\n",
    "\n",
    "ll_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ll_docs]\n",
    "ll_tokens = [item for sublist in ll_tokens for item in sublist]\n",
    "\n",
    "# CITED IN ARTICLE\n",
    "\n",
    "print(f'There are {len(ll_tokens)} tokens in the CLTK Latin Library corpus.')\n",
    "print(f'There are {len(set(ll_tokens))} unique tokens in the CLTK Latin Library corpus.')\n",
    "print(f'Of the tokens appearing in the CLTK Latin Library corpus, {len([k for k, v in Counter(ll_tokens).items() if v == 1 ])} tokens appear once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create stoplist instance\n",
    "\n",
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get frequency stops for corpora\n",
    "\n",
    "ll_freq_stops = c.build_stoplist(ll_docs, size=25, basis='frequency', inc_values=True, sort_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Results for Different Stoplist Construction \"Bases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get stoplists for different bases\n",
    "\n",
    "ll_mean_stops = c.build_stoplist(ll_docs, size=500, basis='mean', inc_values=True, sort_words=False)\n",
    "ll_variance_stops = c.build_stoplist(ll_docs, size=500, basis='variance', inc_values=True, sort_words=False)\n",
    "ll_entropy_stops = c.build_stoplist(ll_docs, size=500, basis='entropy', inc_values=True, sort_words=False)\n",
    "ll_zou_stops = c.build_stoplist(ll_docs, size=100, basis='zou', inc_values=True, sort_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get relevant figures for 'zou' derived list\n",
    "\n",
    "ll_zou_words = list(zip(*ll_zou_stops))[0]\n",
    "ll_zou_mean = [round(dict(ll_mean_stops)[word], 4) for word in ll_zou_words]\n",
    "ll_zou_variance = [round(dict(ll_variance_stops)[word], 6) for word in ll_zou_words]\n",
    "ll_zou_entropy = [round(dict(ll_entropy_stops)[word], 4) for word in ll_zou_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LL 'Zou' Stopwords</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>et</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>93.1182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>66.4403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>46.3060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>43.3043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>34.3664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ut</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>34.3602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cum</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>32.1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quod</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>30.3615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>qui</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>27.5285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>si</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>23.4608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sed</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>24.0613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>de</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>22.2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quae</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>21.1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>per</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>17.7789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>quam</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>18.2709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ex</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>17.3856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nec</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>16.2572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>esse</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>15.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sunt</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>15.5193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>se</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>14.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>enim</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>13.5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hoc</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>14.7302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>autem</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>12.4945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>aut</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>12.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ab</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>13.5396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>te</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>10.5771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>quid</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>11.6469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>me</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>10.0371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>eius</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>10.9327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>etiam</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>12.1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>quoque</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>6.4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>nisi</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>6.7036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>nos</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5.6678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>ea</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>6.1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>illa</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>6.0678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>sicut</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5.0464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>ante</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5.6854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>secundum</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.1477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>dum</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5.5865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>deus</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4.4971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>esset</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5.6174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>dei</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4.4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>modo</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.8444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ei</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.8544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>sub</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5.0522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>omnes</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>6.0635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>nobis</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.8837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>sine</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.8031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>potest</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.9189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>quis</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.5514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>res</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.7637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>omnibus</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.9669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>ubi</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.6713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>causa</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>uos</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.4524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>cui</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.7954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>igitur</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>apud</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.6811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>tunc</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.3748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>cuius</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.7018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LL 'Zou' Stopwords  Mean Prob.  Var. Prob.  Entropy\n",
       "1                   et      0.0324    0.001366  93.1182\n",
       "2                   in      0.0201    0.000503  66.4403\n",
       "3                  est      0.0130    0.000261  46.3060\n",
       "4                  non      0.0118    0.000196  43.3043\n",
       "5                   ad      0.0087    0.000112  34.3664\n",
       "6                   ut      0.0086    0.000104  34.3602\n",
       "7                  cum      0.0081    0.000107  32.1202\n",
       "8                 quod      0.0077    0.000111  30.3615\n",
       "9                  qui      0.0066    0.000068  27.5285\n",
       "10                  si      0.0056    0.000060  23.4608\n",
       "11                 sed      0.0056    0.000046  24.0613\n",
       "12                  de      0.0052    0.000061  22.2423\n",
       "13                quae      0.0048    0.000037  21.1070\n",
       "14                 per      0.0040    0.000038  17.7789\n",
       "15                quam      0.0040    0.000024  18.2709\n",
       "16                  ex      0.0038    0.000027  17.3856\n",
       "17                 nec      0.0036    0.000024  16.2572\n",
       "18                esse      0.0033    0.000024  15.1250\n",
       "19                sunt      0.0034    0.000024  15.5193\n",
       "20                  se      0.0031    0.000019  14.7277\n",
       "21                enim      0.0030    0.000019  13.5983\n",
       "22                 hoc      0.0031    0.000016  14.7302\n",
       "23               autem      0.0027    0.000020  12.4945\n",
       "24                 aut      0.0027    0.000016  12.9518\n",
       "25                  ab      0.0028    0.000016  13.5396\n",
       "26                  te      0.0024    0.000022  10.5771\n",
       "27                quid      0.0025    0.000014  11.6469\n",
       "28                  me      0.0023    0.000020  10.0371\n",
       "29                eius      0.0023    0.000015  10.9327\n",
       "30               etiam      0.0025    0.000011  12.1186\n",
       "..                 ...         ...         ...      ...\n",
       "71              quoque      0.0012    0.000004   6.4997\n",
       "72                nisi      0.0012    0.000003   6.7036\n",
       "73                 nos      0.0011    0.000005   5.6678\n",
       "74                  ea      0.0011    0.000004   6.1394\n",
       "75                illa      0.0011    0.000004   6.0678\n",
       "76               sicut      0.0010    0.000005   5.0464\n",
       "77                ante      0.0010    0.000004   5.6854\n",
       "78            secundum      0.0009    0.000009   4.1477\n",
       "79                 dum      0.0011    0.000004   5.5865\n",
       "80                deus      0.0009    0.000006   4.4971\n",
       "81               esset      0.0011    0.000004   5.6174\n",
       "82                 dei      0.0009    0.000006   4.4010\n",
       "83                modo      0.0011    0.000003   5.8444\n",
       "84                  ei      0.0009    0.000004   4.8544\n",
       "85                 sub      0.0009    0.000004   5.0522\n",
       "86               omnes      0.0011    0.000003   6.0635\n",
       "87               nobis      0.0009    0.000003   4.8837\n",
       "88                sine      0.0011    0.000003   5.8031\n",
       "89              potest      0.0009    0.000003   4.9189\n",
       "90                quis      0.0010    0.000003   5.5514\n",
       "91                 res      0.0009    0.000003   4.7637\n",
       "92             omnibus      0.0009    0.000003   4.9669\n",
       "93                 ubi      0.0010    0.000003   5.6713\n",
       "94               causa      0.0008    0.000004   4.4450\n",
       "95                 uos      0.0007    0.000006   3.4524\n",
       "96                 cui      0.0009    0.000003   4.7954\n",
       "97              igitur      0.0009    0.000003   4.7994\n",
       "98                apud      0.0009    0.000003   4.6811\n",
       "99                tunc      0.0008    0.000003   4.3748\n",
       "100              cuius      0.0008    0.000003   4.7018\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print table\n",
    "\n",
    "data = {\n",
    "    'LL \\'Zou\\' Stopwords': ll_zou_words,\n",
    "    'Mean Prob.': ll_zou_mean,\n",
    "    'Var. Prob.': ll_zou_variance,\n",
    "    'Entropy': ll_zou_entropy,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.index += 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Comparison of Different Latin Stoplists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'adhic', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque', 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo', 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui', 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic', 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero', 'unus', 'ut']\n",
      "The Perseus stoplist has 92 words.\n"
     ]
    }
   ],
   "source": [
    "# Show Perseus stoplist\n",
    "\n",
    "print(PERSEUS_STOPS)\n",
    "print(f'The Perseus stoplist has {len(PERSEUS_STOPS)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cui', 'cuius', 'cum', 'de', 'dei', 'deus', 'dum', 'ea', 'ego', 'ei', 'eius', 'enim', 'eo', 'erat', 'ergo', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'his', 'hoc', 'iam', 'id', 'igitur', 'illa', 'ille', 'in', 'inter', 'ipse', 'ita', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'per', 'post', 'potest', 'pro', 'qua', 'quae', 'quam', 'quem', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'res', 'se', 'secundum', 'sed', 'si', 'sibi', 'sic', 'sicut', 'sine', 'sit', 'sub', 'sunt', 'tamen', 'te', 'tibi', 'tu', 'tunc', 'ubi', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# Show 100-word LL 'Zou' stoplist\n",
    "\n",
    "ll_stops = c.build_stoplist(ll_docs, size=100, basis='zou')\n",
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 words shared by the two lists. This amounts to 58.69565217391305% of the Perseus list.\n",
      "['ab', 'ac', 'ad', 'ante', 'apud', 'atque', 'aut', 'autem', 'cum', 'de', 'dum', 'ego', 'enim', 'ergo', 'est', 'et', 'etiam', 'ex', 'hic', 'iam', 'igitur', 'ille', 'in', 'inter', 'ipse', 'ita', 'modo', 'nam', 'ne', 'nec', 'neque', 'nisi', 'non', 'nos', 'per', 'post', 'pro', 'quae', 'quam', 'qui', 'quia', 'quidem', 'quis', 'quo', 'sed', 'si', 'sic', 'sub', 'tamen', 'tu', 'ubi', 'uel', 'uero', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# Show intersection of Perseus & LL\n",
    "\n",
    "perseus_intersection = set(PERSEUS_STOPS).intersection(set(ll_stops))\n",
    "print(f'There are {len(perseus_intersection)} words shared by the two lists. This amounts to {(len(perseus_intersection)/len(PERSEUS_STOPS))*100}% of the Perseus list.')\n",
    "print(sorted(perseus_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 words from the LL list that are not found on the Perseus list.\n",
      "['causa', 'cui', 'cuius', 'dei', 'deus', 'ea', 'ei', 'eius', 'eo', 'erat', 'esse', 'esset', 'eum', 'fuit', 'haec', 'his', 'hoc', 'id', 'illa', 'me', 'mihi', 'nihil', 'nobis', 'nunc', 'omnes', 'omnia', 'omnibus', 'potest', 'qua', 'quem', 'quibus', 'quid', 'quod', 'quoque', 'res', 'se', 'secundum', 'sibi', 'sicut', 'sine', 'sit', 'sunt', 'te', 'tibi', 'tunc', 'uos']\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Perseus & LL\n",
    "\n",
    "perseus_difference = set(ll_stops).difference(set(PERSEUS_STOPS))\n",
    "print(f'There are {len(perseus_intersection)} words from the LL list that are not found on the Perseus list.')\n",
    "print(sorted(perseus_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 words from the Perseus list that are not found on the LL list.\n",
      "['adhic', 'aliqui', 'aliquis', 'an', 'at', 'cur', 'deinde', 'es', 'etsi', 'fio', 'haud', 'idem', 'infra', 'interim', 'is', 'magis', 'mox', 'necque', 'o', 'ob', 'possum', 'quare', 'quicumque', 'quilibet', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quoniam', 'sive', 'sui', 'sum', 'super', 'suus', 'tam', 'trans', 'tum', 'unus']\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Perseus & LL\n",
    "\n",
    "perseus_difference = set(PERSEUS_STOPS).difference(set(ll_stops))\n",
    "print(f'There are {len(perseus_intersection)} words from the Perseus list that are not found on the LL list.')\n",
    "print(sorted(perseus_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ab', 'ac', 'ad', 'at', 'atque', 'aut', 'autem', 'cum', 'de', 'dum', 'e', 'erant', 'erat', 'est', 'et', 'etiam', 'ex', 'haec', 'hic', 'hoc', 'in', 'ita', 'me', 'nec', 'neque', 'non', 'per', 'qua', 'quae', 'quam', 'qui', 'quibus', 'quidem', 'quo', 'quod', 're', 'rebus', 'rem', 'res', 'sed', 'si', 'sic', 'sunt', 'tamen', 'tandem', 'te', 'ut', 'vel']\n",
      "The stopwords-json stoplist has 49 words.\n"
     ]
    }
   ],
   "source": [
    "# Show stopwords-json list\n",
    "\n",
    "json_stops = [\"a\",\"ab\",\"ac\",\"ad\",\"at\",\"atque\",\"aut\",\"autem\",\"cum\",\"de\",\"dum\",\"e\",\"erant\",\"erat\",\"est\",\"et\",\"etiam\",\"ex\",\"haec\",\"hic\",\"hoc\",\"in\",\"ita\",\"me\",\"nec\",\"neque\",\"non\",\"per\",\"qua\",\"quae\",\"quam\",\"qui\",\"quibus\",\"quidem\",\"quo\",\"quod\",\"re\",\"rebus\",\"rem\",\"res\",\"sed\",\"si\",\"sic\",\"sunt\",\"tamen\",\"tandem\",\"te\",\"ut\",\"vel\"]\n",
    "\n",
    "print(json_stops)\n",
    "print(f'The stopwords-json stoplist has {len(json_stops)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 words shared by the two lists. This amounts to 81.63265306122449% of the stopwords-json list.\n",
      "{'dum', 'quo', 'nec', 'haec', 'quibus', 'hic', 'me', 'ut', 'si', 'ab', 'de', 'quod', 'neque', 'te', 'hoc', 'sed', 'in', 'qui', 'sunt', 'qua', 'autem', 'et', 'sic', 'ac', 'ad', 'erat', 'quam', 'ita', 'tamen', 'ex', 'quae', 'res', 'quidem', 'cum', 'etiam', 'est', 'per', 'aut', 'atque', 'non'}\n"
     ]
    }
   ],
   "source": [
    "# Show intersection of stopwords-json & LL\n",
    "\n",
    "json_intersection = set(json_stops).intersection(set(ll_stops))\n",
    "print(f'There are {len(json_intersection)} words shared by the two lists. This amounts to {(len(json_intersection)/len(json_stops))*100}% of the stopwords-json list.')\n",
    "print(json_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 words from the LL list that are not found on the stopwords-json list.\n",
      "{'ipse', 'mihi', 'sub', 'enim', 'omnibus', 'nisi', 'fuit', 'dei', 'causa', 'post', 'omnia', 'tunc', 'iam', 'ei', 'uos', 'eum', 'igitur', 'ergo', 'ante', 'sibi', 'tu', 'ille', 'eius', 'apud', 'uero', 'eo', 'secundum', 'ne', 'nunc', 'cuius', 'sine', 'cui', 'quia', 'quoque', 'tibi', 'pro', 'uel', 'nos', 'modo', 'se', 'ea', 'illa', 'esset', 'ego', 'his', 'deus', 'esse', 'quis', 'omnes', 'id', 'inter', 'nam', 'ubi', 'quem', 'nihil', 'nobis', 'sicut', 'potest', 'sit', 'quid'}\n"
     ]
    }
   ],
   "source": [
    "# Show difference of stopwords-json & LL\n",
    "\n",
    "json_difference = set(ll_stops).difference(set(json_stops))\n",
    "print(f'There are {len(json_intersection)} words from the LL list that are not found on the stopwords-json list.')\n",
    "print(json_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 words from the stopwords-json list that are not found on the LL list.\n",
      "{'a', 'at', 'rebus', 'vel', 're', 'rem', 'e', 'erant', 'tandem'}\n"
     ]
    }
   ],
   "source": [
    "# Show difference of stopwords-json & LL\n",
    "\n",
    "json_difference = set(json_stops).difference(set(ll_stops))\n",
    "print(f'There are {len(json_intersection)} words from the stopwords-json list that are not found on the LL list.')\n",
    "print(json_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/serial/voyant.p', 'rb') as f:\n",
    "    voyant_stops = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Voyant Tools stoplist has 4015 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'The Voyant Tools stoplist has {len(voyant_stops)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "{'dum', 'ipse', 'mihi', 'fuit', 'post', 'omnia', 'si', 'igitur', 'quod', 'sibi', 'ille', 'apud', 'uero', 'qua', 'et', 'sic', 'ac', 'nunc', 'ita', 'nos', 'ea', 'se', 'esse', 'quis', 'quid', 'sub', 'omnibus', 'quibus', 'me', 'iam', 'tunc', 'de', 'hoc', 'in', 'erat', 'ne', 'cuius', 'sine', 'cui', 'illa', 'inter', 'potest', 'haec', 'ab', 'ergo', 'neque', 'te', 'eius', 'eo', 'quam', 'tamen', 'quia', 'pro', 'ex', 'res', 'esset', 'cum', 'etiam', 'nam', 'quem', 'nobis', 'sicut', 'aut', 'atque', 'enim', 'quo', 'nec', 'nisi', 'causa', 'ei', 'hic', 'uos', 'ut', 'eum', 'ante', 'sed', 'tu', 'qui', 'sunt', 'autem', 'secundum', 'ad', 'quoque', 'tibi', 'uel', 'modo', 'quae', 'quidem', 'ego', 'his', 'omnes', 'id', 'ubi', 'nihil', 'est', 'per', 'sit', 'non'}\n"
     ]
    }
   ],
   "source": [
    "# Show intersection of Voyant Tools & LL\n",
    "\n",
    "voyant_intersection = set(voyant_stops).intersection(set(ll_stops))\n",
    "print(len(voyant_intersection))\n",
    "print(voyant_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'deus', 'dei'}\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Voyant Tools & LL\n",
    "\n",
    "voyant_difference = set(ll_stops).difference(set(voyant_stops))\n",
    "print(len(voyant_difference))\n",
    "print(voyant_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3848\n",
      "['', '!', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/', '0', '1', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86']\n",
      "['visust', 'visuve', 'vit', 'vix', 'vobis', 'vobiscum', 'vobismet', 'vobisne', 'vobisque', 'vol', 'von', 'vop', 'vos', 'vosmet', 'vosne', 'vosque', 'voster', 'vostra', 'vostrae', 'vostraeque', 'vostram', 'vostraque', 'vostrarum', 'vostras', 'vostrast', 'vostri', 'vostris', 'vostrist', 'vostro', 'vostrorum', 'vostros', 'vostrosque', 'vostrost', 'vostrum', 'vostrumque', 'vostrumst', 'vulg', 'vv', 'w', 'x', 'xc', 'xci', 'xcii', 'xciii', 'xciv', 'xcix', 'xcv', 'xcvi', 'xcvii', 'xcviii', 'xi', 'xii', 'xiii', 'xiv', 'xix', 'xl', 'xli', 'xlii', 'xliii', 'xliv', 'xlix', 'xlv', 'xlvi', 'xlvii', 'xlviii', 'xv', 'xvi', 'xvii', 'xviii', 'xx', 'xxi', 'xxii', 'xxiii', 'xxiv', 'xxix', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxx', 'xxxi', 'xxxii', 'xxxiii', 'xxxiv', 'xxxix', 'xxxv', 'xxxvi', 'xxxvii', 'xxxviii', 'y', 'z', '{', '|', '}', '§', '°', '¿', 'ɔ', 'γρ', '†']\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Voyant Tools & LL (selection)\n",
    "\n",
    "voyant_difference = set(voyant_stops).difference(set(ll_stops))\n",
    "print(len(voyant_difference))\n",
    "print(sorted(voyant_difference)[:100])\n",
    "print(sorted(voyant_difference)[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix C: 100-Word Stoplists for various Latin corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create subcorpora of LL corpus\n",
    "\n",
    "# Cicero files/tokens\n",
    "cic_files = [file for file in latinlibrary.fileids() if 'cicero/' in file]\n",
    "cic_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in cic_files]\n",
    "cic_tokens = [WordPunctTokenizer().tokenize(doc) for doc in cic_docs]\n",
    "cic_tokens = [item for sublist in cic_tokens for item in sublist] #flatten\n",
    "\n",
    "# Biblia Sacra files/tokens\n",
    "bib_files = [file for file in latinlibrary.fileids() if 'bible/' in file]\n",
    "bib_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in bib_files]\n",
    "bib_tokens = [WordPunctTokenizer().tokenize(doc) for doc in bib_docs]\n",
    "bib_tokens = [item for sublist in bib_tokens for item in sublist] #flatten\n",
    "\n",
    "# Roman Legal Texts files/tokens\n",
    "ius_files = [file for file in latinlibrary.fileids() if 'justinian' in file \n",
    "                     or 'gaius' in file \n",
    "                     or 'theod' in file]\n",
    "ius_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ius_files]\n",
    "ius_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ius_docs]\n",
    "ius_tokens = [item for sublist in ius_tokens for item in sublist] #flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cui', 'cuius', 'cum', 'de', 'dei', 'deus', 'dum', 'ea', 'ego', 'ei', 'eius', 'enim', 'eo', 'erat', 'ergo', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'his', 'hoc', 'iam', 'id', 'igitur', 'illa', 'ille', 'in', 'inter', 'ipse', 'ita', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'per', 'post', 'potest', 'pro', 'qua', 'quae', 'quam', 'quem', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'res', 'se', 'secundum', 'sed', 'si', 'sibi', 'sic', 'sicut', 'sine', 'sit', 'sub', 'sunt', 'tamen', 'te', 'tibi', 'tu', 'tunc', 'ubi', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL 'zou' stoplist\n",
    "\n",
    "ll_stops = c.build_stoplist(ll_docs, size=100, basis='zou')\n",
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'an', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cum', 'de', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'hoc', 'iam', 'id', 'igitur', 'iis', 'illa', 'ille', 'illud', 'in', 'ipse', 'is', 'ita', 'itaque', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'omnium', 'per', 'potest', 'pro', 'publica', 'publicae', 'qua', 'quae', 'quam', 'quasi', 'quem', 'qui', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quos', 're', 'rebus', 'rei', 'rem', 'rerum', 'res', 'se', 'sed', 'senatus', 'si', 'sic', 'sine', 'sit', 'sunt', 'tam', 'tamen', 'te', 'tibi', 'tu', 'tum', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL-Cic 'zou' stoplist\n",
    "\n",
    "ll_cic_stops = c.build_stoplist(cic_docs, size=100, basis='zou')\n",
    "print(ll_cic_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ad', 'ait', 'aut', 'autem', 'caput', 'christi', 'cum', 'de', 'dei', 'deo', 'deus', 'dicit', 'die', 'dixit', 'domini', 'domino', 'dominum', 'dominus', 'domus', 'ecce', 'ego', 'ei', 'eis', 'eius', 'enim', 'eo', 'eorum', 'eos', 'erat', 'ergo', 'erit', 'est', 'estis', 'et', 'eum', 'ex', 'filii', 'filius', 'fratres', 'haec', 'hierusalem', 'his', 'hoc', 'iesu', 'in', 'ipse', 'israhel', 'me', 'mea', 'meum', 'mihi', 'ne', 'nec', 'neque', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'omnis', 'per', 'pro', 'propter', 'quae', 'quam', 'quasi', 'quem', 'qui', 'quia', 'quid', 'quis', 'quod', 'quoniam', 'rex', 'se', 'secundum', 'sed', 'si', 'sicut', 'suis', 'sum', 'sunt', 'super', 'suum', 'te', 'terra', 'terram', 'tibi', 'tu', 'tua', 'tuam', 'tui', 'tuum', 'uobis', 'uos', 'usque', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL-Bib 'zou' stoplist\n",
    "\n",
    "ll_bib_stops = c.build_stoplist(bib_docs, size=100, basis='zou')\n",
    "print(ll_bib_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'actio', 'actionem', 'ad', 'ait', 'an', 'aut', 'autem', 'causa', 'ci', 'conss', 'constantinopoli', 'contra', 'cth', 'cum', 'dat', 'de', 'debet', 'dig', 'ea', 'eam', 'ed', 'ei', 'eius', 'enim', 'eo', 'eorum', 'eos', 'erit', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuerit', 'hereditatem', 'heres', 'his', 'hoc', 'id', 'idem', 'imperator', 'imperatores', 'in', 'is', 'ita', 'iulianus', 'iure', 'ius', 'kal', 'mihi', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nomine', 'non', 'paulus', 'per', 'posse', 'possit', 'post', 'potest', 'pp', 'pr', 'pro', 'quae', 'quam', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'rei', 'rem', 'res', 'sab', 'se', 'sed', 'si', 'sibi', 'sine', 'sit', 'siue', 'sunt', 'tamen', 'uel', 'uero', 'ulpianus', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL-Ius 'zou' stoplist\n",
    "\n",
    "ll_ius_stops = c.build_stoplist(ius_docs, size=100, basis='zou')\n",
    "print(ll_ius_stops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

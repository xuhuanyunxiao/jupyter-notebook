{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T05:59:22.325573Z",
     "start_time": "2018-04-04T05:59:21.189508Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "today = dt.datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io import sql\n",
    "\n",
    "from impala.dbapi import connect\n",
    "from impala.util import as_pandas\n",
    "\n",
    "names = locals()\n",
    "\n",
    "#\n",
    "pyfile_folder = r'D:\\XH\\Python_Project\\Proj_2\\files'\n",
    "data_folder = r'D:\\XH\\Python_Project\\Proj_2\\data\\ETL_data'\n",
    "result_folder = r'D:\\XH\\Python_Project\\Proj_2\\result\\ETL_result'\n",
    "\n",
    "os.chdir(pyfile_folder)\n",
    "sys.path.append(pyfile_folder)\n",
    "\n",
    "from Tookits import specific_func  \n",
    "from Tookits import cal_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T03:44:05.773333Z",
     "start_time": "2018-04-04T03:44:05.761332Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#% 结果文件夹结构\n",
    "database_list = ['odm_1','sdm_2','fdm_3','gdm_4',\n",
    "                 'standard_lib_5','supplemental_lib_6']\n",
    "for folder_n in database_list:\n",
    "    if not os.path.exists(result_folder + '\\\\' + folder_n):\n",
    "        os.mkdir(result_folder + '\\\\' + folder_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T05:59:32.025128Z",
     "start_time": "2018-04-04T05:59:31.854118Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% 建立连接\n",
    "# MySQL\n",
    "DB_CON_STR = 'mysql+pymysql://root:123456@localhost/supplemental_lib_6_mysql?charset=utf8'  \n",
    "engine = create_engine(DB_CON_STR, echo=False) \n",
    "\n",
    "# Hive\n",
    "conn = connect(host=\"192.168.20.102\", port=10000,  # database=\"system\", \n",
    "               auth_mechanism=\"PLAIN\",\n",
    "               user = 'admin', password = 'admin')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_hive_query(sql):   \n",
    "    cursor.execute(sql)  \n",
    "    return cursor.fetchall() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T05:59:37.231426Z",
     "start_time": "2018-04-04T05:59:36.881406Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_name = database_name = 'supplemental_lib_6'\n",
    "\n",
    "save_filename = os.path.join(result_folder, \n",
    "                             'supplemental_lib_6\\\\hadoop语句_supplemental_lib_' + today + '.txt')\n",
    "file = open(save_filename,\"w\")\n",
    "\n",
    "cursor.execute(\"create database if not exists {0} \".format(database_name))\n",
    "cursor.execute(\"use \"+ database_name)\n",
    "\n",
    "file_name = 'outbound_investment_0329.csv'\n",
    "file_path = data_folder + '\\\\' + file_name\n",
    "table_name = os.path.splitext(file_name)[0]\n",
    "names['%s' %table_name] = pd.read_csv(file_path, nrows = 5)\n",
    "#file_path = result_folder +  '\\\\supplemental_lib_6\\\\' + table_name + '.csv'\n",
    "#names['%s' %table_name].to_csv(file_path, index = False, \n",
    " #                              header = False,  encoding = 'utf-8')\n",
    "\n",
    "field = [x.strip() + ' string' for x in names['%s' %table_name].columns.tolist()]\n",
    "\n",
    "# 在hive上建立标准表 \n",
    "cursor.execute('drop table if exists %s;' %table_name)     \n",
    "sql_code  =  \"create external table if not exists {0}{1}\".\\\n",
    "        format(table_name,tuple(field)).replace(\"'\",\"\") \\\n",
    "        + '\\n' + \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\" \\\n",
    "        + '\\n' + \"LOCATION '/tmp/20180314/{0}'\".format(table_name)      \n",
    "cursor.execute(sql_code)\n",
    "\n",
    "file.write(\"hdfs dfs -put -f '/home/hadoop/Public/ETL_data/{0}/{1}.csv' '/tmp/20180314/{1}'\".\\\n",
    "           format(database_name,table_name) + \"\\n\")    \n",
    "file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T06:18:44.695057Z",
     "start_time": "2018-04-04T06:18:43.578993Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'mbcs' codec can't decode bytes in position 0--1: No mapping for the Unicode character exists in the target code page.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14858)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17387)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_decode (pandas\\_libs\\parsers.c:23510)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\encodings\\mbcs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmbcs_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'mbcs' codec can't decode bytes in position 0--1: No mapping for the Unicode character exists in the target code page.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1a831f85ded5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         names['%s' %table_name] = pd.read_csv(os.path.join(data_folder + '\\\\' + folder_name, file_name), \n\u001b[1;32m---> 20\u001b[1;33m                                encoding = 'ANSI')\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult_folder\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;34m'\\\\supplemental_lib_6\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         names['%s' %table_name].to_csv(file_path, index = False, \n",
      "\u001b[1;32mD:\\software\\conda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1748\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1749\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas\\_libs\\parsers.c:10862)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas\\_libs\\parsers.c:11138)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas\\_libs\\parsers.c:12175)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas\\_libs\\parsers.c:14136)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14972)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17387)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_decode (pandas\\_libs\\parsers.c:23510)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\lib\\encodings\\mbcs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmbcs_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'mbcs' codec can't decode bytes in position 0--1: No mapping for the Unicode character exists in the target code page."
     ]
    }
   ],
   "source": [
    "folder_name = database_name = 'supplemental_lib_6'\n",
    "file_list = os.listdir(os.path.join(data_folder, folder_name))\n",
    "\n",
    "if not os.path.exists(result_folder + '\\\\' + folder_name + '\\\\' + today):\n",
    "    os.makedirs(result_folder + '\\\\' + folder_name + '\\\\' + today) \n",
    "    \n",
    "save_filename = os.path.join(result_folder, \n",
    "                             'supplemental_lib_6\\\\hadoop语句_supplemental_lib_' + today + '.txt')\n",
    "file = open(save_filename,\"w\")\n",
    "\n",
    "cursor.execute(\"create database if not exists {0} \".format(database_name))\n",
    "cursor.execute(\"use \"+ database_name)\n",
    "\n",
    "table_list = [name[0] for name in run_hive_query(\"show tables\")] \n",
    "\n",
    "for file_name in file_list:\n",
    "    table_name = os.path.splitext(file_name)[0]\n",
    "    if table_name not in table_list:\n",
    "        names['%s' %table_name] = pd.read_csv(os.path.join(data_folder + '\\\\' + folder_name, file_name), \n",
    "                                              nrows = 5)\n",
    "                             #  encoding = 'ANSI')\n",
    "        #file_path = result_folder +  '\\\\supplemental_lib_6\\\\' + table_name + '.csv'\n",
    "        #names['%s' %table_name].to_csv(file_path, index = False, \n",
    "                                       header = False,  encoding = 'utf-8')\n",
    "        # 统计\n",
    "      #  fea_filename = os.path.join(result_folder + '\\\\' + folder_name + '\\\\' + today, table_name + '.xlsx')        \n",
    "      #  single_fea_desc = cal_func.describe(names['%s' %table_name],fea_filename, data_rate = 0.1)\n",
    "\n",
    "        #  field = [x.strip() + ' string' \n",
    "        #           if ind > 0 else 'index string'\n",
    "        #           for ind, x in enumerate(names['%s' %table_name].columns.tolist())]\n",
    "        field = [x.strip() + ' string' for x in names['%s' %table_name].columns.tolist()]\n",
    "        # 在hive上建立标准表 \n",
    "        cursor.execute('drop table if exists %s;' %table_name)     \n",
    "        sql_code  =  \"create external table if not exists {0}{1}\".\\\n",
    "                format(table_name,tuple(field)).replace(\"'\",\"\") \\\n",
    "                + '\\n' + \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\" \\\n",
    "                + '\\n' + \"LOCATION '/tmp/20180314/{0}'\".format(table_name)      \n",
    "        cursor.execute(sql_code)\n",
    "\n",
    "        # 写入mysql\n",
    "    #    sql.to_sql(names['%s' %table_name], table_name, \n",
    "    #       engine, schema='supplemental_lib_6_mysql', if_exists='replace') \n",
    "\n",
    "        # 打印在hadoop上操作的语句\n",
    "    #    file.write(\"load data inpath '/home/hadoop/Public/ETL_data/{0}/{1}.csv' \\ # hive 上用\n",
    "    #               into table {1};\".format(database_name,table_n) + \"\\n\")\n",
    "        file.write(\"hdfs dfs -put -f '/home/hadoop/Public/ETL_data/{0}/{1}.csv' '/tmp/20180314/{1}'\".\\\n",
    "                   format(database_name,table_name) + \"\\n\")    \n",
    "    # 如果已经put过，需加参数 -f\n",
    "    #hdfs dfs -put -f '/home/hadoop/Public/ETL_data/standard_lib/city_symbol.csv' '/tmp/20180302/city_symbol'\n",
    "    \n",
    "file.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全量  ODM to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_hive_query(sql_command):   \n",
    "    cursor.execute(sql_command)  \n",
    "    return cursor.fetchall() \n",
    "\n",
    "database_name = 'supplemental_lib_6'\n",
    "cursor.execute(\"use \"+ database_name) \n",
    "\n",
    "table_list = [name[0] for name in run_hive_query(\"show tables\")] \n",
    "\n",
    "for table_name in table_list:\n",
    "    cursor.execute(\"select * from %s\"%table_name)\n",
    "    tmp_data = as_pandas(cursor)\n",
    "    sql.to_sql(tmp_data, table_name, \n",
    "           engine, schema='supplemental_lib_6_mysql', if_exists='replace') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

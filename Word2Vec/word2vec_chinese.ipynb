{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T06:58:17.409581Z",
     "start_time": "2019-01-04T06:58:17.400589Z"
    }
   },
   "source": [
    "# 本文件说明\n",
    "- 训练中文词向量：Word2vec\n",
    "  - 银监会、保监会"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:10:22.924272Z",
     "start_time": "2019-01-07T09:10:22.889496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:11:12.169608Z",
     "start_time": "2019-01-07T09:11:12.082596Z"
    }
   },
   "outputs": [],
   "source": [
    "from toolkits.setup import specific_func\n",
    "specific_func.set_ch_pd()\n",
    "from toolkits.setup.date_time import get_day_list\n",
    "from toolkits.nlp import pre_cor_circ\n",
    "from toolkits.nlp import pre_cor_cbrc\n",
    "from toolkits.nlp import myclass_circ\n",
    "from toolkits.nlp import myclass_cbrc\n",
    "from toolkits.nlp import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:12:01.114174Z",
     "start_time": "2019-01-07T09:12:01.086511Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:12:36.580757Z",
     "start_time": "2019-01-07T09:12:36.556581Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保监会数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T06:39:20.124262Z",
     "start_time": "2019-01-07T03:19:29.780126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  circ_class_predict_mysql_2018-10-11.xlsx (26182, 7)\n",
      "filename:  circ_class_predict_mysql_2018-09-22.xlsx (17000, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-14.xlsx (14931, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-05.xlsx (11195, 7)\n",
      "filename:  circ_class_predict_mysql_2018-11-08.xlsx (25330, 7)\n",
      "filename:  circ_class_predict_mysql_2018-09-21.xlsx (26577, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-03.xlsx (10945, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-13.xlsx (20354, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-02.xlsx (11517, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-18.xlsx (5306, 7)\n",
      "filename:  circ_class_predict_mysql_2018-11-10.xlsx (15144, 7)\n",
      "filename:  circ_class_predict_mysql_2018-11-09.xlsx (27563, 7)\n",
      "filename:  circ_class_predict_mysql_2018-11-23.xlsx (23964, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-12.xlsx (20948, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-07.xlsx (11443, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-06.xlsx (10998, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-01.xlsx (13000, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-04.xlsx (10471, 7)\n",
      "filename:  circ_class_predict_mysql_2018-11-22.xlsx (22798, 7)\n",
      "filename:  circ_class_predict_mysql_2018-10-19.xlsx (2738, 7)\n",
      "file_num:  328404\n",
      "sentence_num:  14667692\n"
     ]
    }
   ],
   "source": [
    "filepath_raw = 'datasets_raw/circ/'\n",
    "filename_list = os.listdir(filepath_raw)\n",
    "# filename_list = filename_list[:1]\n",
    "print('文件数：', len(filename_list))\n",
    "\n",
    "file_num = 0\n",
    "sentence_num = 0\n",
    "with open('datasets_pre/circ_pre.txt', 'w') as f:\n",
    "    for index, filename in enumerate(filename_list):\n",
    "        tmp_data = pd.read_excel(filepath_raw + filename)\n",
    "        print('filename: ', index +1, filename, tmp_data.shape)\n",
    "        file_num += tmp_data.shape[0]\n",
    "        for index in tmp_data.index:\n",
    "            content = str(tmp_data.loc[index, 'title']) + \"。\" + str(tmp_data.loc[index, 'content'])\n",
    "            sentences = [i.strip() for i in utils.cut_sentences(content)]\n",
    "            for sentence in sentences :\n",
    "                string_pre = pre_cor_circ.handle_contents([sentence])\n",
    "                f.write(string_pre[0] + '\\n')\n",
    "                sentence_num += 1\n",
    "\n",
    "del tmp_data\n",
    "print('file_num: ', file_num)\n",
    "print('sentence_num: ', sentence_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 银监会数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath_raw = 'datasets_raw/cbrc/'\n",
    "# filename_list = os.listdir(filepath_raw)\n",
    "# filename_list = filename_list[:1]\n",
    "\n",
    "# file_num = 0\n",
    "# with open('datasets_pre/cbrc_pre.txt', 'w') as f:\n",
    "#     for filename in filename_list:\n",
    "#         tmp_data = pd.read_excel(filepath_raw + filename)\n",
    "#         print('filename: ', filename, tmp_data.shape)\n",
    "#         file_num += tmp_data.shape[0]\n",
    "#         for index in tmp_data.index:\n",
    "#             string = str(tmp_data.loc[index, 'title']) + \"。\" + str(tmp_data.loc[index, 'content'])\n",
    "#             string_pre = pre_cor_circ.handle_contents([string])\n",
    "#             f.write(string_pre[0] + '\\n')\n",
    "\n",
    "# del tmp_data\n",
    "# print('file_num: ', file_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T07:40:09.678394Z",
     "start_time": "2019-01-07T07:39:39.298156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circ_pre 文件数： 14667692\n",
      "datasets_pre sentence_num： 14667692\n"
     ]
    }
   ],
   "source": [
    "circ_pre = []\n",
    "with open('datasets_pre/circ_pre.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        circ_pre.append(line.strip()) # 把末尾的'\\n'删掉        \n",
    "print('circ_pre 文件数：', len(circ_pre))\n",
    "\n",
    "# cbrc_pre = []\n",
    "# with open('datasets_pre/cbrc_pre.txt', 'r') as f:\n",
    "#     for line in f.readlines():\n",
    "#         circ_pre.append(line.strip()) # 把末尾的'\\n'删掉        \n",
    "# print('cbrc_pre 文件数：', len(cbrc_pre))\n",
    "\n",
    "# datasets_pre = circ_pre + cbrc_pre\n",
    "datasets_pre = circ_pre\n",
    "\n",
    "with open('datasets_pre/datasets_pre.txt', 'w') as f:\n",
    "    for line in datasets_pre:\n",
    "        f.write(line + '\\n')\n",
    "print('datasets_pre sentence_num：', len(datasets_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train\n",
    "- 语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型\n",
    "- 语料大的时候用 CBOW 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-gram模型\n",
    "- 在一般的NLP处理中，会需要去停用词。\n",
    "- 由于word2vec的算法依赖于上下文，而上下文有可能就是停词。因此对于word2vec，我们可以不用去停词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T07:45:09.610081Z",
     "start_time": "2019-01-07T07:45:09.586750Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets_pre_path = 'datasets_pre/datasets_pre.txt'\n",
    "model_path = 'model/cbirc_skip_gram.model' # 输出模型\n",
    "vector_path = 'model/cbirc_skip_gram.vector' # 原始c版本word2vec的vector格式的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T07:45:11.959Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(LineSentence(datasets_pre_path), \n",
    "                 size=400, window=5, min_count=40,\n",
    "                 workers=multiprocessing.cpu_count())\n",
    "\n",
    "# 保存模型\n",
    "model.save(model_path)\n",
    "model.wv.save_word2vec_format(vector_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T07:45:26.950Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.wv.syn0.shape)\n",
    "print(model['保监会'][:10]) # 查看单个词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T08:45:38.554396Z",
     "start_time": "2019-01-04T08:45:38.528221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32547, 400)\n",
      "[ 0.15556747 -0.896676    0.19152008 -1.2532637   0.20065337 -1.1594774\n",
      "  0.5081621   0.2302387   0.7268413  -1.6614144 ]\n"
     ]
    }
   ],
   "source": [
    "# syn0的每一行，即代表词汇表中的一个单词，即有16490个单词。\n",
    "# 列代表特征向量的大小，即400，这个训练时设定的数字。\n",
    "# 设置的最小单词频度是40（即出现40次以下的单词会被忽略），\n",
    "# 最后得到一个有16492个单词的词汇表，每个词有300个特征。\n",
    "print(model.wv.syn0.shape)\n",
    "print(model['保监会'][:10]) # 查看单个词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_pre_path = 'datasets_pre/datasets_pre.txt'\n",
    "# model_path = 'model/cbirc_CBOW.model' # 输出模型\n",
    "# vector_path = 'model/cbirc_CBOW.vector' # 原始c版本word2vec的vector格式的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/glory1234work2115/article/details/52454141\n",
    "# model_dm = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, workers=3)\n",
    "# model_dbow = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, dm=0, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:24:35.386263Z",
     "start_time": "2019-01-07T09:24:33.764762Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:25:08.742181Z",
     "start_time": "2019-01-07T09:25:08.175391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国保监会 0.7136878967285156\n",
      "银保监会 0.633984386920929\n",
      "银监会 0.6290504932403564\n",
      "中国保险监督管理委员会 0.5769743919372559\n",
      "证监会 0.5272318124771118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6290504389051719"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = model.most_similar(u\"保监会\", topn=5)\n",
    "for t in word:\n",
    "    print(t[0],t[1])\n",
    "    \n",
    "model.wv.similarity('保监会', '银监会')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T08:56:16.483139Z",
     "start_time": "2019-01-04T08:56:16.301645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'卫生部'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('保监会 银监会 银保监会 卫生部'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T09:02:42.194642Z",
     "start_time": "2019-01-04T09:02:42.159209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "银监会 0.7442243099212646\n",
      "中国保监会 0.6307400465011597\n",
      "卫生部 0.5117448568344116\n",
      "中国保险监督管理委员会 0.5058220028877258\n",
      "银保监会 0.49333423376083374\n"
     ]
    }
   ],
   "source": [
    "word = model.most_similar(u\"保监会\", topn=5)\n",
    "for t in word:\n",
    "    print(t[0],t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T09:05:45.664997Z",
     "start_time": "2019-01-04T09:05:45.636547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7442243271420652"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('保监会', '银监会')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:38:42.611903Z",
     "start_time": "2019-01-07T09:38:42.580045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14170"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = model.wv.vectors\n",
    "num_clusters = word_vectors.shape[0] // 10\n",
    "num_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

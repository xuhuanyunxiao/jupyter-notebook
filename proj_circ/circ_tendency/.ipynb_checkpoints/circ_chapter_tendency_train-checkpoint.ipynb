{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- 保监会 篇章级倾向性模型 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:47:09.252019Z",
     "start_time": "2018-06-19T07:47:07.973946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\conda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "\n",
    "import jieba\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2,mutual_info_classif,f_classif\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn import svm  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:47:25.995976Z",
     "start_time": "2018-06-19T07:47:25.933973Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StatsFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neg = set()\n",
    "        f = open(\"corpus/neg_words.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.neg.add(content)\n",
    "        f.close()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def getcnt(self,x):        \n",
    "        return len(list(set(x)))\n",
    "\n",
    "    def getnegcnt(self,x):\n",
    "        negcnt = 0\n",
    "        words = x.split()\n",
    "        for w in words:\n",
    "            if w in self.neg:\n",
    "                negcnt = negcnt+1\n",
    "        return negcnt\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = []\n",
    "        for x in X:\n",
    "            if len(x) == 0:\n",
    "                length  = 1\n",
    "            else :\n",
    "                length = len(x)\n",
    "            data.append([len(x),self.getcnt(x),self.getcnt(x)/length,\n",
    "                         self.getnegcnt(x),self.getnegcnt(x)/length])            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:47:21.943745Z",
     "start_time": "2018-06-19T07:47:21.762734Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "lab = []\n",
    "\n",
    "f = open(\"data/i_p_1.txt\",\"r+\", encoding='UTF-8')\n",
    "for content in f:\n",
    "    corpus.append(content.split(\"\\t\")[0])\n",
    "    lab.append(1)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/0609_p_1.txt\",\"r+\", encoding='UTF-8')\n",
    "for content in f:\n",
    "    corpus.append(content.split(\"\\t\")[0])\n",
    "    lab.append(1)\n",
    "f.close()\n",
    "f = open(\"data/0609_p_2.txt\",\"r+\", encoding='UTF-8')\n",
    "for content in f:\n",
    "    corpus.append(content.split(\"\\t\")[0])\n",
    "    lab.append(1)\n",
    "f.close()\n",
    "\n",
    "#neg sample\n",
    "f = open(\"data/i_n_1.txt\",\"r+\", encoding='UTF-8')\n",
    "for content in f:\n",
    "    corpus.append(content.split(\"\\t\")[0])\n",
    "    lab.append(0)\n",
    "f.close()\n",
    "f = open(\"data/0609_n_1.txt\",\"r+\", encoding='UTF-8')\n",
    "for content in f:\n",
    "    corpus.append(content.split(\"\\t\")[0])\n",
    "    lab.append(0)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:49:12.140048Z",
     "start_time": "2018-06-19T07:47:41.049838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0...tate=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer()),\n",
    "            ('tf_idf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=20000))\n",
    "        ])),\n",
    "        ('len_stats', StatsFeatures())\n",
    "    ])),\n",
    "    ('classifier', XGBClassifier(max_depth=7,objective='multi:softmax', num_class=2))\n",
    "])\n",
    "\n",
    "#params = {'nthread':8,'max_depth':6, 'eta':0.2, 'eval_metric':'merror', 'silent':1, 'objective':'multi:softmax', 'num_class':2}  # 参数\n",
    "#\n",
    "pipeline.fit(corpus, lab)\n",
    " \n",
    "#kf = KFold(len(corpus), n_folds=10, shuffle=True)    \n",
    "#result_set = [(pipeline.fit(corpus[train], lab[train]).predict(corpus[test]), test) for train, test in kf]    \n",
    "#score = [accuracy(lab[result[1]], result[0]) for result in result_set]    \n",
    "#print(score,np.average(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T03:39:22.531358Z",
     "start_time": "2018-06-13T03:39:17.686930Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\conda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9921100917431193"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(corpus, lab)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:53:20.570257Z",
     "start_time": "2018-06-19T07:53:20.313242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9899497487437185 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\conda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "corpus_test = []\n",
    "f = open(\"data/0609_n_1.txt\",\"r+\", encoding='UTF-8')\n",
    "for content in f:\n",
    "    if(len(content)>1):\n",
    "        corpus_test.append(content.strip())\n",
    "f.close()\n",
    "\n",
    "res = pipeline.predict(corpus_test)\n",
    "print(1- float(sum(res))/len(res),len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0609_NP_model_i.pkl.z']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import joblib\n",
    "#joblib.dump(pipeline, \"0609_NP_model_i.pkl.z\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:53:30.291813Z",
     "start_time": "2018-06-19T07:53:27.348645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0619_circ_chapter_tendency_2.pkl.z']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"0619_circ_chapter_tendency_2.pkl.z\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 保监会 分类模型 2 训练 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T02:53:03.265244Z",
     "start_time": "2018-08-03T02:52:58.005793Z"
    }
   },
   "outputs": [],
   "source": [
    "##load packages, needed\n",
    "# encoding=utf-8\n",
    "\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2,mutual_info_classif,f_classif \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_cor\n",
    "import dict_dbutils\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io import sql\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T02:53:03.279263Z",
     "start_time": "2018-08-03T02:53:03.267759Z"
    }
   },
   "outputs": [],
   "source": [
    "class StatsFeatures_cor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neg = set()\n",
    "        f = open(\"corpus/neg_words.txt\",\"r+\", encoding='UTF-8')\n",
    "        for content in f:\n",
    "            self.neg.add(content)\n",
    "        f.close()\n",
    "        dict_org=dict_dbutils.get_dicts()\n",
    "        self.org=set(dict_org.keys())\n",
    "        \n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def getcnt(self,x):        \n",
    "        return len(list(set(x)))\n",
    "\n",
    "    def getnegcnt(self,x):\n",
    "        negcnt = 0\n",
    "        words = x.split()\n",
    "        for w in words:\n",
    "            if w in self.neg:\n",
    "                negcnt = negcnt+1\n",
    "        return negcnt\n",
    "    def getorgcnttf(self,x):\n",
    "        orgcnt=0\n",
    "        orgtf=0\n",
    "        words = x.split()\n",
    "        words_set=set(words)\n",
    "        for w in words_set:\n",
    "            if w in self.org:\n",
    "                orgcnt = orgcnt+1\n",
    "                orgtf=orgtf+words.count(w)\n",
    "        if(orgcnt>0):\n",
    "            return orgcnt,orgtf\n",
    "        else:\n",
    "            return orgcnt,orgtf\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = []\n",
    "        for x in X:\n",
    "            if len(x) == 0:\n",
    "                length  = 1\n",
    "            else :\n",
    "                length = len(x)\n",
    "            orgcnt,orgtf=self.getorgcnttf(x)\n",
    "            data.append([len(x),self.getcnt(x),self.getcnt(x)/length,\n",
    "                         self.getnegcnt(x),self.getnegcnt(x)/length,\n",
    "                         orgcnt,orgtf])            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T02:53:03.305760Z",
     "start_time": "2018-08-03T02:53:03.281977Z"
    }
   },
   "outputs": [],
   "source": [
    "def getkeywords(X, N = 1000):\n",
    "    '''\n",
    "    训练时生成，合并所有记录，取N个关键词\n",
    "    '''\n",
    "    textrank = analyse.textrank\n",
    "\n",
    "    text_combined = ' '.join(X)\n",
    "    keywords = textrank(text_combined, topK = N)\n",
    "    print('keywords num: ', len(keywords))\n",
    "    if len(keywords) < N : \n",
    "        N  = len(keywords)\n",
    "\n",
    "    if keywords:\n",
    "        f = open(\"corpus/keywords.txt\",\"w+\", encoding='UTF-8')\n",
    "        for content in keywords:\n",
    "            content = content.strip()\n",
    "            f.write(content + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T02:53:03.330362Z",
     "start_time": "2018-08-03T02:53:03.308404Z"
    }
   },
   "outputs": [],
   "source": [
    "class Statskeywords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, topk = 100):\n",
    "        self.topk = topk\n",
    "#         print(self.topk)\n",
    "        self.keywords = set()\n",
    "        f = open(\"corpus/keywords.txt\",\"r+\", encoding='UTF-8')\n",
    "        num = 0\n",
    "        for content in f:\n",
    "            if num < topk:\n",
    "                self.keywords.add(content.strip().replace('\\n', ''))\n",
    "            num += 1\n",
    "        f.close() \n",
    "        \n",
    "        #初始化字典liwc\n",
    "        self.liwc = {} \n",
    "        f2 = open(\"corpus/scliwc.txt\",'r', encoding = 'gb18030')\n",
    "        for ii in f2:     #ii在scliwc.txt中循环\n",
    "            i = ii.strip().split() \n",
    "            self.liwc[i[0]] = i[1:len(i)]\n",
    "        f2.close      \n",
    "        \n",
    "        self.category = set()\n",
    "        for i in list(self.liwc.values()):\n",
    "            for j in i:\n",
    "                self.category.add(j)        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        文本中关键词的词频\n",
    "        '''                        \n",
    "        data = []\n",
    "        for x in X:\n",
    "            words = x.split()\n",
    "            word_tf = []\n",
    "            keycnt = 0\n",
    "            for kw in self.keywords:\n",
    "                word_tf.append(words.count(kw)) # 各个关键词的词频\n",
    "                if kw in words:keycnt+=1\n",
    "            word_tf.append(keycnt) # 关键词的个数\n",
    "            \n",
    "            psy = []\n",
    "            for w in words:\n",
    "                if w in self.liwc: #是否liwc字典包含分词结果列表words的哪些分词\n",
    "                    psy += self.liwc[w]  \n",
    "            cat_tf = []\n",
    "            for cat in self.category:\n",
    "                cat_tf.append(psy.count(cat))                \n",
    "                \n",
    "            data.append(word_tf + cat_tf)            \n",
    "        return data        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上一版模型读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T02:53:07.219097Z",
     "start_time": "2018-08-03T02:53:03.332830Z"
    }
   },
   "outputs": [],
   "source": [
    "# 上一版模型\n",
    "from sklearn.externals import joblib\n",
    "pipeline_old = joblib.load( \"model/circ_8classifier_0730_1.pkl.z\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:13.912396Z",
     "start_time": "2018-08-03T07:35:13.894777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10530\n"
     ]
    }
   ],
   "source": [
    "title = []\n",
    "filename = 'data/titles.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    title.append(f.strip().replace('\\n', ''))\n",
    "fid.close()\n",
    "print(len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:14.607292Z",
     "start_time": "2018-08-03T07:35:14.600189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['做好 规划 不当 月光族 守财奴', '车险 代理 前景 如何 信合 财富 事业 迈上 高度']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:15.368331Z",
     "start_time": "2018-08-03T07:35:15.207407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10530\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "filename = 'data/contents.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    content.append(f.strip().replace('\\n', ''))\n",
    "fid.close()\n",
    "print(len(content))\n",
    "# content[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:16.114169Z",
     "start_time": "2018-08-03T07:35:15.950134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10530\n"
     ]
    }
   ],
   "source": [
    "title_content = [t + ' ' + c for t,c in zip(title, content)]\n",
    "print(len(title_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:16.696879Z",
     "start_time": "2018-08-03T07:35:16.688880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '监管',\n",
       " 2: '行业',\n",
       " 3: '产品销售',\n",
       " 4: '资本市场',\n",
       " 5: '公司内部管理',\n",
       " 6: '消费服务',\n",
       " 7: '其他相关报道',\n",
       " 8: '噪音'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic={'监管':1,'行业':2,'产品销售':3,'资本市场':4,'公司内部管理':5,'消费服务':6,'其他相关报道':7,'噪音':8}\n",
    "class_name_dict = {v: k for k, v in label_dic.items()}\n",
    "class_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:17.421377Z",
     "start_time": "2018-08-03T07:35:17.407672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['噪音', '噪音', '产品销售', '噪音', '消费服务']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = []\n",
    "filename = 'data/labels.txt'\n",
    "fid = open(filename, \"r+\", encoding='UTF-8')\n",
    "for f in fid:\n",
    "    label.append(f.strip().replace('\\n', ''))\n",
    "fid.close()\n",
    "print(len(label))\n",
    "label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:18.519363Z",
     "start_time": "2018-08-03T07:35:18.512580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8, 3, 8, 6]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = [label_dic[l] for l in label]\n",
    "label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割训练集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:35:20.402648Z",
     "start_time": "2018-08-03T07:35:20.265218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集： 7371\n",
      "训练集-各类数量： Counter({8: 2358, 6: 1117, 1: 1056, 7: 1005, 5: 638, 2: 520, 4: 410, 3: 267})\n",
      "测试集： 3159\n",
      "测试集-各类数量： Counter({8: 1018, 6: 456, 7: 451, 1: 432, 5: 263, 2: 243, 4: 173, 3: 123})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(title_content, label, test_size=0.3, random_state=42)\n",
    "print('训练集：',len(y_train))\n",
    "print('训练集-各类数量：',Counter(y_train))\n",
    "print('测试集：',len(y_test))\n",
    "print('测试集-各类数量：',Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T03:50:15.361659Z",
     "start_time": "2018-08-01T03:50:15.354321Z"
    }
   },
   "outputs": [],
   "source": [
    "if 0: \n",
    "    from jieba import analyse\n",
    "    getkeywords(title_content, N = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:45:59.211991Z",
     "start_time": "2018-08-03T07:35:40.022669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488\n",
      "1488\n",
      "keywords num:  200\n",
      "763\n",
      "763\n",
      "keywords num:  200\n",
      "390\n",
      "390\n",
      "keywords num:  200\n",
      "583\n",
      "583\n",
      "keywords num:  200\n",
      "901\n",
      "901\n",
      "keywords num:  200\n",
      "1573\n",
      "1573\n",
      "keywords num:  200\n",
      "1456\n",
      "1456\n",
      "keywords num:  200\n",
      "3376\n",
      "3376\n",
      "keywords num:  200\n"
     ]
    }
   ],
   "source": [
    "# j = 1\n",
    "N = 200\n",
    "class_key_dict = {}\n",
    "key_dict = {}\n",
    "for j in range(1,9):\n",
    "    str_list = [m for m,n in zip(title_content, label) if n == j]\n",
    "    print(len(str_list))\n",
    "    print(label.count(j))\n",
    "    textrank = analyse.textrank\n",
    "    text_combined = ' '.join(str_list)\n",
    "    keywords = textrank(text_combined, topK = N)\n",
    "    print('keywords num: ', len(keywords))\n",
    "#     print('%s -- %s: '%(j, class_name_dict[j]) , keywords)\n",
    "    class_key_dict[j] = keywords\n",
    "    for key in keywords:\n",
    "        if key not in key_dict:            \n",
    "            key_dict[key] = 1\n",
    "        else :\n",
    "            key_dict[key] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:45:59.224011Z",
     "start_time": "2018-08-03T07:45:59.216073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "697\n",
      "[7, 7, 8, 8, 7, 8, 8, 7, 7, 7, 8, 8, 3, 7, 5, 8, 1, 8, 2, 8, 7, 8, 4, 8, 3, 8, 8, 8, 8, 2, 5, 8, 2, 8, 2, 3, 8, 3, 7, 1, 4, 7, 8, 8, 1, 6, 1, 2, 8, 3, 6, 1, 8, 5, 7, 2, 1, 4, 7, 1, 7, 4, 1, 4, 4, 3, 1, 2, 8, 3, 8, 6, 6, 4, 1, 7, 1, 2, 2, 7, 3, 1, 5, 2, 1, 4, 6, 1, 2, 1, 4, 3, 2, 3, 8, 7, 4, 7, 6, 1, 5, 4, 6, 1, 5, 1, 1, 6, 1, 2, 4, 1, 7, 6, 7, 1, 2, 6, 3, 3, 1, 4, 5, 1, 6, 7, 5, 1, 1, 2, 1, 2, 1, 5, 7, 2, 2, 7, 6, 1, 2, 5, 1, 2, 2, 7, 7, 1, 2, 1, 1, 2, 1, 1, 7, 1, 3, 3, 6, 2, 5, 6, 1, 1, 4, 2, 7, 3, 1, 5, 1, 1, 5, 4, 1, 1, 5, 4, 1, 4, 1, 2, 7, 8, 4, 1, 5, 1, 4, 4, 3, 1, 1, 1, 5, 3, 2, 1, 2, 2, 5, 4, 4, 5, 2, 4, 6, 2, 2, 3, 3, 5, 3, 2, 6, 2, 2, 4, 3, 5, 3, 2, 3, 5, 3, 5, 4, 5, 5, 2, 1, 2, 6, 7, 6, 3, 5, 1, 3, 1, 3, 2, 4, 2, 5, 1, 5, 3, 1, 1, 2, 1, 3, 1, 3, 3, 1, 2, 1, 6, 2, 1, 4, 2, 3, 1, 3, 2, 4, 2, 1, 1, 3, 3, 1, 1, 2, 4, 2, 4, 3, 2, 2, 3, 1, 3, 3, 5, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 3, 1, 1, 1, 1, 1, 2, 4, 1, 3, 4, 4, 2, 1, 2, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 2, 3, 1, 1, 2, 3, 2, 1, 3, 2, 2, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 4, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 4, 3, 2, 1, 2, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "key_count_list = [k for k,v in key_dict.items() if v > 4]\n",
    "print(len(key_count_list))\n",
    "print(len(key_dict.values()))\n",
    "print(list(key_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:45:59.251574Z",
     "start_time": "2018-08-03T07:45:59.226459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --\n",
      "去除前： 200\n",
      "去除后： 117\n",
      "2 --\n",
      "去除前： 200\n",
      "去除后： 99\n",
      "3 --\n",
      "去除前： 200\n",
      "去除后： 111\n",
      "4 --\n",
      "去除前： 200\n",
      "去除后： 123\n",
      "5 --\n",
      "去除前： 200\n",
      "去除后： 111\n",
      "6 --\n",
      "去除前： 200\n",
      "去除后： 139\n",
      "7 --\n",
      "去除前： 200\n",
      "去除后： 123\n",
      "8 --\n",
      "去除前： 200\n",
      "去除后： 119\n",
      "len(keywords):  942\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "for j in range(1,9):\n",
    "    print('%s --'%j)\n",
    "    keyword_list = class_key_dict[j]\n",
    "    print('去除前：', len(keyword_list))\n",
    "    for k in key_count_list:        \n",
    "        if k in keyword_list:\n",
    "            keyword_list.remove(k)\n",
    "    print('去除后：', len(keyword_list)) \n",
    "    \n",
    "    keywords += keyword_list\n",
    "\n",
    "print('len(keywords): ', len(keywords))\n",
    "f = open(\"corpus/keywords.txt\",\"w+\", encoding='UTF-8')\n",
    "for content in keywords:\n",
    "    content = content.strip()\n",
    "    f.write(content + '\\n')\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: 主题模型选特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:03:35.391330Z",
     "start_time": "2018-08-01T09:03:30.508002Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tf_vectorizer  = CountVectorizer(max_df=0.95, min_df=2)\n",
    "tf = tf_vectorizer .fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:03:35.401422Z",
     "start_time": "2018-08-01T09:03:35.394294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1613520, array([1, 1, 1, ..., 1, 6, 1], dtype=int64))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.data), tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:03:35.518120Z",
     "start_time": "2018-08-01T09:03:35.404440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aaaa', 'aaaaa', 'aaf', 'aa制', 'ab', 'abb', 'abbott', 'abbvie']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60262"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print(tf_feature_names[:10])\n",
    "len(tf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:29.443362Z",
     "start_time": "2018-08-01T09:03:49.887880Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20,\n",
    "                                max_iter=50,\n",
    "                                learning_method='batch',\n",
    "                                random_state=0)\n",
    "docres = lda.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:29.452911Z",
     "start_time": "2018-08-01T09:09:29.446388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.90625003e-04, 9.92578125e-01, 3.90625003e-04, ...,\n",
       "        3.90625095e-04, 3.90625002e-04, 3.90625004e-04],\n",
       "       [4.50450455e-04, 1.28539163e-01, 7.50649189e-01, ...,\n",
       "        4.50450465e-04, 4.50450457e-04, 4.50450459e-04],\n",
       "       [6.44618417e-02, 7.77604998e-05, 7.77604987e-05, ...,\n",
       "        2.78697680e-02, 2.06295485e-02, 7.77604990e-05],\n",
       "       ...,\n",
       "       [1.21359227e-04, 1.21359227e-04, 1.21359225e-04, ...,\n",
       "        1.21359226e-04, 1.21359226e-04, 1.21359225e-04],\n",
       "       [4.32523919e-02, 5.10204087e-04, 5.10204084e-04, ...,\n",
       "        3.71540782e-01, 5.10204085e-04, 5.10204082e-04],\n",
       "       [9.32835840e-05, 9.32835834e-05, 9.32835835e-05, ...,\n",
       "        9.32835830e-05, 1.79299844e-01, 2.00158643e-01]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:29.461871Z",
     "start_time": "2018-08-01T09:09:29.455356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00000001e-02, 5.56502649e+00, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000006e-02],\n",
       "       [1.01622379e+01, 5.00000005e-02, 5.00000000e-02, ...,\n",
       "        5.00000006e-02, 5.00000003e-02, 5.00000002e-02],\n",
       "       [5.00000003e-02, 5.00000002e-02, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000040e-02],\n",
       "       ...,\n",
       "       [5.00000001e-02, 5.00000004e-02, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000001e-02],\n",
       "       [5.00000012e-02, 5.00000010e-02, 5.00000000e-02, ...,\n",
       "        5.00000028e-02, 5.00000007e-02, 5.00000001e-02],\n",
       "       [2.06829458e+02, 5.76482636e+01, 5.00000000e-02, ...,\n",
       "        5.00000000e-02, 5.00000000e-02, 5.00000002e-02]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:29.635338Z",
     "start_time": "2018-08-01T09:09:29.464468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "招标 项目 文件 投标 采购 图片 工程 提供 公告 投标人 要求 时间 单位 资格 企业 具有 注册 条件 原件 报名\n",
      "Topic #1:\n",
      "公司 亿元 保险 增长 业务 保险公司 同比 股东 产品 寿险 保费 保费收入 集团 车险 发展 市场 中国 显示 其中 行业\n",
      "Topic #2:\n",
      "社保 部门 养老保险 缴费 征收 税务 企业 就业 参保 统一 职工 基本 养老金 医疗保险 社保费 缴纳 养老 规定 个人 医疗\n",
      "Topic #3:\n",
      "招聘 人员 报名 岗位 面试 资格 工作 笔试 时间 体检 聘用 条件 进行 考试 公开 教师 成绩 应聘 专业 具有\n",
      "Topic #4:\n",
      "疫苗 公告 中国 企业 国家 公司 生物 长生 汽车 美国 发布 项目 事件 一路 一带 新闻 亿美元 国际 上半年 美元\n",
      "Topic #5:\n",
      "教师 讲学 工作 答案 点击 解析 查看 招募 学校 计划 服务 电梯 教育 农村 根据 暂无 救援 活动 退休 单位\n",
      "Topic #6:\n",
      "中国 公司 美国 集团 杭州 日本 世界 泰康 有限公司 香港 德国 英国 光伏 服饰 group 国家 融资担保 医养 国际 法国\n",
      "Topic #7:\n",
      "工作 企业 建设 服务 开展 标准 发展 推进 安全 加强 金融 问题 生产 要求 部门 管理 制度 改革 实施 机制\n",
      "Topic #8:\n",
      "投资 股权 保险 计划 ff 保险资金 企业 关联交易 设立 业务 通知 实体 资产 融资 经济 保监会 重大 方式 管理 项目\n",
      "Topic #9:\n",
      "活动 孩子 一个 学生 生活 教育 扶贫 保险 社会 图片 没有 家庭 学校 宣传 富德生命人寿 老师 家长 开始 作品 员工\n",
      "Topic #10:\n",
      "分公司 云南 社会 云南省 寿险 中国人寿保险股份有限公司 管理 招聘 任职 资格 销售 关于 支公司 公司 批复 昆明市 核准 之日起 许可 中心\n",
      "Topic #11:\n",
      "工作 招聘 要求 经验 以上 电话 联系电话 公司 工资 优先 待遇 地址 相关 面议 岗位 销售 能力 职位 联系 薪资\n",
      "Topic #12:\n",
      "保险 保险公司 理赔 如果 保障 客户 没有 一个 产品 投保 购买 需要 图片 很多 因为 保费 什么 保单 医院 可能\n",
      "Topic #13:\n",
      "活动 车辆 责任 承担 旅游 费用 赔偿 声明 平台 事故 报名 汽车 任何 交强险 酒店 交通事故 发生 景区 内容 时间\n",
      "Topic #14:\n",
      "产品 投资 资管 资产 资产管理 业务 监管 理财产品 新规 机构 管理 要求 银行 金融机构 规定 风险 理财 办法 投资者 私募\n",
      "Topic #15:\n",
      "保险 公司 销售 保监会 规定 保险公司 产品 保单 行为 监管 万元 消费者 罚款 处罚 机构 进行 相关 问题 开门红 决定\n",
      "Topic #16:\n",
      "股份 中国 报价 港股 亿元 个股 资金 有效 板块 涨幅 上涨 中国平安 a股 买入 图片 行情 市值 股价 净流入 茅台\n",
      "Topic #17:\n",
      "基金 有限公司 投资 管理 证券 公司 有效 电话 地址 股份 法定代表 资产 上海 办公 债券 联系人 总经理 经理 网址 有限责任\n",
      "Topic #18:\n",
      "平台 发展 金融 服务 中国 行业 客户 监管 数据 企业 科技 公司 风险 保险 合作 创新 网贷 市场 技术 通过\n",
      "Topic #19:\n",
      "市场 银行 投资 政策 经济 央行 资金 融资 风险 企业 贷款 影响 认为 利率 出现 货币 可能 下降 预期 增长\n",
      "\n",
      "[[5.00000001e-02 5.56502649e+00 5.00000000e-02 ... 5.00000000e-02\n",
      "  5.00000000e-02 5.00000006e-02]\n",
      " [1.01622379e+01 5.00000005e-02 5.00000000e-02 ... 5.00000006e-02\n",
      "  5.00000003e-02 5.00000002e-02]\n",
      " [5.00000003e-02 5.00000002e-02 5.00000000e-02 ... 5.00000000e-02\n",
      "  5.00000000e-02 5.00000040e-02]\n",
      " ...\n",
      " [5.00000001e-02 5.00000004e-02 5.00000000e-02 ... 5.00000000e-02\n",
      "  5.00000000e-02 5.00000001e-02]\n",
      " [5.00000012e-02 5.00000010e-02 5.00000000e-02 ... 5.00000028e-02\n",
      "  5.00000007e-02 5.00000001e-02]\n",
      " [2.06829458e+02 5.76482636e+01 5.00000000e-02 ... 5.00000000e-02\n",
      "  5.00000000e-02 5.00000002e-02]]\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    #打印每个主题下权重较高的term\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    #打印主题-词语分布矩阵\n",
    "    print(model.components_)\n",
    "\n",
    "n_top_words=20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:33.616440Z",
     "start_time": "2018-08-01T09:09:29.637897Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_topic_dist = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T09:09:40.007054Z",
     "start_time": "2018-08-01T09:09:33.619148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2737.4835277309144"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T03:52:45.115Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "n_topics = [200,] #range(20, 75, 5)\n",
    "perplexityLst = [1.0]*len(n_topics)\n",
    "\n",
    "#训练LDA并打印训练时间\n",
    "lda_models = []\n",
    "for idx, n_topic in enumerate(n_topics):\n",
    "    lda = LatentDirichletAllocation(n_topics=n_topic,\n",
    "                                    max_iter=1000,\n",
    "                                    learning_method='batch',\n",
    "                                    evaluate_every=200,\n",
    "#                                    perp_tol=0.1, #default                                       \n",
    "#                                    doc_topic_prior=1/n_topic, #default\n",
    "#                                    topic_word_prior=1/n_topic, #default\n",
    "                                    verbose=0)\n",
    "    t0 = time()\n",
    "    lda.fit(tf)\n",
    "    perplexityLst[idx] = lda.perplexity(tf)\n",
    "    lda_models.append(lda)\n",
    "    print(\"# of Topic: %d, \" % n_topics[idx])\n",
    "    print(\"done in %0.3fs, N_iter %d, \" % ((time() - t0), lda.n_iter_))\n",
    "    print(\"Perplexity Score %0.3f\" % perplexityLst[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T10:17:32.308Z"
    }
   },
   "outputs": [],
   "source": [
    "#打印最佳模型\n",
    "best_index = perplexityLst.index(min(perplexityLst))\n",
    "best_n_topic = n_topics[best_index]\n",
    "best_model = lda_models[best_index]\n",
    "print(\"Best # of Topic: \", best_n_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:58:59.461322Z",
     "start_time": "2018-08-03T07:45:59.254207Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9968796635463302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tf_idf', Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9...tate=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tf_idf', Pipeline([\n",
    "            ('counts', CountVectorizer(max_df=0.95, min_df=2)),\n",
    "            ('tf_idf', TfidfTransformer()),\n",
    "            ('chi', SelectKBest(chi2, k=20000))\n",
    "        ])),\n",
    "        ('tf', Statskeywords(topk = 1000)),\n",
    "        ('len_stats', StatsFeatures_cor())\n",
    "    ])),\n",
    "    ('standard', StandardScaler(with_mean=False)),\n",
    "    ('classifier', XGBClassifier(max_depth=7,objective='multi:softmax', num_class=8))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T07:59:44.357797Z",
     "start_time": "2018-08-03T07:58:59.464059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.8448876226654004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.79      0.87      0.83       432\n",
      "          2       0.69      0.61      0.65       243\n",
      "          3       0.77      0.49      0.60       123\n",
      "          4       0.82      0.75      0.78       173\n",
      "          5       0.81      0.79      0.80       263\n",
      "          6       0.92      0.92      0.92       456\n",
      "          7       0.87      0.88      0.88       451\n",
      "          8       0.87      0.92      0.89      1018\n",
      "\n",
      "avg / total       0.84      0.84      0.84      3159\n",
      "\n",
      "confusion_matrix: \n",
      "[[375  14   3   3  10   1   1  25]\n",
      " [ 26 148   4   5   7  12  17  24]\n",
      " [  5   5  60   0   7   9  13  24]\n",
      " [  9  15   0 130   3   0   0  16]\n",
      " [ 15  10   3   1 207   5   9  13]\n",
      " [  0   5   1   0   3 419  11  17]\n",
      " [  7  10   4   2   8   4 398  18]\n",
      " [ 38   6   3  18  10   4   7 932]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T07:06:58.974728Z",
     "start_time": "2018-08-01T07:06:51.693194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.9001331557922769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.90      0.95      0.93       279\n",
      "          2       0.76      0.69      0.73        65\n",
      "          3       0.94      0.51      0.66        63\n",
      "          4       0.95      0.82      0.88        89\n",
      "          5       0.88      0.78      0.83       128\n",
      "          6       0.96      0.81      0.88        59\n",
      "          7       0.89      0.91      0.90       143\n",
      "          8       0.91      0.97      0.94       676\n",
      "\n",
      "avg / total       0.90      0.90      0.90      1502\n",
      "\n",
      "confusion_matrix: \n",
      "[[266   4   0   0   3   1   0   5]\n",
      " [  4  45   0   1   1   0   4  10]\n",
      " [  1   2  32   0   1   1   5  21]\n",
      " [  2   1   0  73   0   0   0  13]\n",
      " [ 10   0   2   0 100   0   5  11]\n",
      " [  0   2   0   0   2  48   1   6]\n",
      " [  6   2   0   0   2   0 130   3]\n",
      " [  6   3   0   3   5   0   1 658]]\n"
     ]
    }
   ],
   "source": [
    "# 上一版模型 \n",
    "y_pred_class = pipeline_old.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T02:57:03.596005Z",
     "start_time": "2018-08-03T02:57:02.750893Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "title_content = np.array(title_content)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T04:28:50.352781Z",
     "start_time": "2018-08-03T02:57:05.133428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 1\n",
      "0.9991673605328892\n",
      "accuracy_score:  0.8408788282290279\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.83      0.88      0.86       286\n",
      "          2       0.52      0.41      0.46        58\n",
      "          3       0.76      0.50      0.60        58\n",
      "          4       0.88      0.78      0.83        98\n",
      "          5       0.82      0.75      0.78       126\n",
      "          6       0.71      0.80      0.75        59\n",
      "          7       0.85      0.81      0.83       151\n",
      "          8       0.88      0.93      0.90       666\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1502\n",
      "\n",
      "confusion_matrix: \n",
      "[[253   5   2   1   1   0   2  22]\n",
      " [ 11  24   0   0   2   3   1  17]\n",
      " [  6   2  29   1   4   2   5   9]\n",
      " [  2   3   0  76   6   0   1  10]\n",
      " [  7   3   1   1  94   3   8   9]\n",
      " [  0   1   1   0   1  47   1   8]\n",
      " [  3   3   1   0   3   8 122  11]\n",
      " [ 21   5   4   7   4   3   4 618]]\n",
      "---- 2\n",
      "0.9993338884263114\n",
      "accuracy_score:  0.8468708388814914\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.90      0.86       309\n",
      "          2       0.67      0.48      0.56        62\n",
      "          3       0.79      0.55      0.65        67\n",
      "          4       0.82      0.73      0.77        81\n",
      "          5       0.79      0.77      0.78       115\n",
      "          6       0.86      0.71      0.78        62\n",
      "          7       0.90      0.83      0.86       144\n",
      "          8       0.88      0.93      0.90       662\n",
      "\n",
      "avg / total       0.84      0.85      0.84      1502\n",
      "\n",
      "confusion_matrix: \n",
      "[[277   4   3   1   6   0   0  18]\n",
      " [  8  30   1   3   3   0   6  11]\n",
      " [  5   3  37   0   3   1   1  17]\n",
      " [  2   1   0  59   3   0   0  16]\n",
      " [ 12   4   1   0  88   4   1   5]\n",
      " [  0   0   1   0   2  44   5  10]\n",
      " [  1   1   2   3   4   2 120  11]\n",
      " [ 31   2   2   6   3   0   1 617]]\n",
      "---- 3\n",
      "0.9988344988344988\n",
      "accuracy_score:  0.8461025982678214\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.87      0.84       302\n",
      "          2       0.60      0.54      0.57        61\n",
      "          3       0.73      0.52      0.61        71\n",
      "          4       0.89      0.72      0.80       101\n",
      "          5       0.79      0.73      0.76       113\n",
      "          6       0.82      0.73      0.78        45\n",
      "          7       0.83      0.83      0.83       132\n",
      "          8       0.89      0.94      0.92       676\n",
      "\n",
      "avg / total       0.84      0.85      0.84      1501\n",
      "\n",
      "confusion_matrix: \n",
      "[[263   7   1   1   7   1   0  22]\n",
      " [ 10  33   1   3   0   0   4  10]\n",
      " [  6   4  37   0   4   2   6  12]\n",
      " [  9   2   1  73   3   0   0  13]\n",
      " [ 12   2   2   1  83   0   7   6]\n",
      " [  0   3   2   0   1  33   2   4]\n",
      " [  2   3   1   0   5   3 110   8]\n",
      " [ 20   1   6   4   2   1   4 638]]\n",
      "---- 4\n",
      "0.9986679986679987\n",
      "accuracy_score:  0.8387741505662891\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.88      0.85       283\n",
      "          2       0.56      0.28      0.38        67\n",
      "          3       0.68      0.56      0.61        61\n",
      "          4       0.90      0.78      0.84        97\n",
      "          5       0.67      0.70      0.68       115\n",
      "          6       0.90      0.67      0.77        66\n",
      "          7       0.86      0.90      0.88       126\n",
      "          8       0.88      0.94      0.91       686\n",
      "\n",
      "avg / total       0.83      0.84      0.83      1501\n",
      "\n",
      "confusion_matrix: \n",
      "[[250   6   0   1  10   0   0  16]\n",
      " [ 15  19   2   1   7   2   3  18]\n",
      " [  3   1  34   0   6   3   0  14]\n",
      " [  4   3   0  76   3   0   0  11]\n",
      " [  8   3   3   1  80   0   6  14]\n",
      " [  2   0   0   1   7  44   3   9]\n",
      " [  0   2   4   0   3   0 113   4]\n",
      " [ 23   0   7   4   3   0   6 643]]\n",
      "---- 5\n",
      "0.9985014985014985\n",
      "accuracy_score:  0.8427714856762158\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.84      0.91      0.87       308\n",
      "          2       0.67      0.46      0.55        69\n",
      "          3       0.81      0.54      0.65        63\n",
      "          4       0.82      0.72      0.77        86\n",
      "          5       0.71      0.71      0.71       115\n",
      "          6       0.79      0.67      0.73        49\n",
      "          7       0.81      0.87      0.84       125\n",
      "          8       0.89      0.92      0.91       686\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1501\n",
      "\n",
      "confusion_matrix: \n",
      "[[279   0   0   0   4   1   2  22]\n",
      " [  9  32   4   1   3   0   6  14]\n",
      " [  4   5  34   0   5   2   3  10]\n",
      " [  4   2   0  62   7   0   0  11]\n",
      " [  6   3   2   2  82   2   9   9]\n",
      " [  0   2   0   1   3  33   2   8]\n",
      " [  4   2   0   1   5   1 109   3]\n",
      " [ 26   2   2   9   7   3   3 634]]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for train_index , test_index in kf.split(title_content):\n",
    "    print('---- %s'%(num+1))\n",
    "    X_train,X_test = title_content[train_index], title_content[test_index]\n",
    "    y_train,y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('tf_idf', Pipeline([\n",
    "                ('counts', CountVectorizer(max_df=0.95, min_df=2)),\n",
    "                ('tf_idf', TfidfTransformer()),\n",
    "                ('chi', SelectKBest(chi2, k=20000))\n",
    "            ])),\n",
    "            ('tf', Statskeywords(topk = 1000)),\n",
    "            ('len_stats', StatsFeatures_cor())\n",
    "        ])),\n",
    "        ('standard', StandardScaler(with_mean=False)),\n",
    "        ('classifier', XGBClassifier(max_depth=7,objective='multi:softmax', num_class=8))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(pipeline.score(X_train, y_train))    \n",
    "    \n",
    "    y_pred_class = pipeline.predict(X_test)\n",
    "    print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "    print('confusion_matrix: ')\n",
    "    print( metrics.confusion_matrix(y_test, y_pred_class))    \n",
    "    \n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T01:27:04.118258Z",
     "start_time": "2018-07-31T01:27:04.114269Z"
    }
   },
   "outputs": [],
   "source": [
    "score_1_best_para = {}\n",
    "score_2_best_para = {}\n",
    "cv = 3\n",
    "score_1 = 'accuracy'\n",
    "score_2 = 'recall_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T01:27:04.926294Z",
     "start_time": "2018-07-31T01:27:04.891722Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_features = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=20000))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = 100)),\n",
    "                                    ('len_stats', StatsFeatures_cor())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topk、chi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T01:27:06.253472Z",
     "start_time": "2018-07-31T01:27:06.246295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf_idf__chi__k': [2000, 5000, 20000, 40000],\n",
       " 'features__tf__topk': [50, 100, 500]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0,\n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [2000, 5000, 20000, 40000 ],\n",
    "                   features__tf__topk=[50,100, 500]) \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T02:35:21.387632Z",
     "start_time": "2018-07-31T01:27:07.254445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8128921848260126, total= 2.9min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8187001140250855, total= 3.0min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.8148571428571428, total= 3.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.822690992018244, total= 4.0min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8203080433542499, total= 4.7min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  9.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.8148571428571428, total= 5.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.822690992018244, total= 8.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.814033086138049, total= 8.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.8154285714285714, total= 8.7min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.822690992018244, total=10.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 20.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8187001140250855, total= 3.2min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8134626354820308, total= 9.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.8114285714285714, total= 9.1min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8128921848260126, total= 3.2min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.8148571428571428, total= 3.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.822690992018244, total= 4.8min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8203080433542499, total= 5.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 30.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.8148571428571428, total= 6.0min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.822690992018244, total= 8.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.814033086138049, total= 7.8min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.8154285714285714, total= 7.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.822690992018244, total= 7.9min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8187001140250855, total= 3.2min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8128921848260126, total= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 44.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8134626354820308, total= 8.4min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.8114285714285714, total= 7.9min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.8148571428571428, total= 3.4min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.822690992018244, total= 4.2min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8203080433542499, total= 4.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=5000, score=0.8148571428571428, total= 5.0min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.822690992018244, total= 7.6min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.814033086138049, total= 7.3min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=20000, score=0.8154285714285714, total= 7.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed: 58.6min remaining:  5.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.822690992018244, total= 7.8min\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8134626354820308, total= 7.3min\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=40000, score=0.8114285714285714, total= 6.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 64.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_chi__k:  5000\n",
      "score_1_topk:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.81548, std: 0.00241, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.81929, std: 0.00328, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.81739, std: 0.00380, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.81586, std: 0.00490, params: {'features__tf__topk': 50, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.81548, std: 0.00241, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.81929, std: 0.00328, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.81739, std: 0.00380, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.81586, std: 0.00490, params: {'features__tf__topk': 100, 'features__tf_idf__chi__k': 40000},\n",
       "  mean: 0.81548, std: 0.00241, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 2000},\n",
       "  mean: 0.81929, std: 0.00328, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 5000},\n",
       "  mean: 0.81739, std: 0.00380, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 20000},\n",
       "  mean: 0.81586, std: 0.00490, params: {'features__tf__topk': 500, 'features__tf_idf__chi__k': 40000}],\n",
       " {'features__tf__topk': 50, 'features__tf_idf__chi__k': 5000},\n",
       " 0.8192885676241202)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_1_best_para['topk'] = clf.best_params_['features__tf__topk']\n",
    "print('score_1_chi__k: ', score_1_best_para['chi__k'])\n",
    "print('score_1_topk: ', score_1_best_para['topk'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T09:18:52.970377Z",
     "start_time": "2018-07-30T09:18:52.962284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__tf_idf__chi__k': [2000, 5000, 20000, 40000],\n",
       " 'features__tf__topk': [50, 100, 500]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "param_grid_0 = dict(features__tf_idf__chi__k = [2000, 5000, 20000, 40000 ],# \n",
    "                   features__tf__topk=[50,100, 500]) # \n",
    "param_grid_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-30T09:18:53.841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=2000 ............\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.6637165839517644, total= 3.1min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.6897705282761166, total= 3.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=5000 ............\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=2000, score=0.6721476122550027, total= 3.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.6966093961388902, total= 4.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.6829495831182235, total= 4.4min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=20000 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  8.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=5000, score=0.6604975784755553, total= 4.2min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.6854724848440027, total= 6.3min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.6773942476791797, total= 6.6min\n",
      "[CV] features__tf__topk=50, features__tf_idf__chi__k=40000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=20000, score=0.6702156852743684, total= 6.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.6867400401203034, total= 8.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 18.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.6732463467676054, total= 7.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.6897705282761166, total= 3.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=50, features__tf_idf__chi__k=40000, score=0.6667103034908572, total= 8.4min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.6721476122550027, total= 3.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=2000, score=0.6637165839517644, total= 3.2min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.6966093961388902, total= 4.3min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.6829495831182235, total= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 25.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=20000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=5000, score=0.6604975784755553, total= 4.6min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.6854724848440027, total= 6.7min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.6773942476791797, total= 6.5min\n",
      "[CV] features__tf__topk=100, features__tf_idf__chi__k=40000 ..........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=20000, score=0.6702156852743684, total= 6.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.6867400401203034, total= 7.8min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.6897705282761166, total= 3.1min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=2000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.6732463467676054, total= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 39.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.6721476122550027, total= 3.4min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=100, features__tf_idf__chi__k=40000, score=0.6667103034908572, total= 7.8min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=5000 ...........\n",
      "[CV]  features__tf__topk=500, features__tf_idf__chi__k=2000, score=0.6637165839517644, total= 3.5min\n",
      "[CV] features__tf__topk=500, features__tf_idf__chi__k=20000 ..........\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_0, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1, verbose=10)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['chi__k'] = clf.best_params_['features__tf_idf__chi__k']\n",
    "score_2_best_para['topk'] = clf.best_params_['features__tf__topk']\n",
    "print('score_2_chi__k: ', score_2_best_para['chi__k'])\n",
    "print('score_2_topk: ', score_2_best_para['topk'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T02:38:07.006070Z",
     "start_time": "2018-07-31T02:38:06.960158Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_features_1 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_1_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = score_1_best_para['topk'])),\n",
    "                                    ('len_stats', StatsFeatures_cor())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features_2 = FeatureUnion([\n",
    "                                    ('tf_idf', Pipeline([\n",
    "                                        ('counts', CountVectorizer()),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                        ('chi', SelectKBest(chi2, k=score_2_best_para['chi__k']))\n",
    "                                        ])),\n",
    "                                    ('tf', Statskeywords(topk = score_2_best_para['topk'])),\n",
    "                                    ('len_stats', StatsFeatures_cor())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T09:26:24.541827Z",
     "start_time": "2018-07-31T09:23:11.148378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998096688237533\n",
      "accuracy_score:  0.8313359964491789\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.81      0.88      0.84       434\n",
      "          2       0.69      0.46      0.56        97\n",
      "          3       0.70      0.48      0.57        98\n",
      "          4       0.85      0.73      0.79       137\n",
      "          5       0.77      0.71      0.74       177\n",
      "          6       0.87      0.69      0.77        88\n",
      "          7       0.84      0.81      0.83       210\n",
      "          8       0.86      0.93      0.89      1012\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2253\n",
      "\n",
      "confusion_matrix: \n",
      "[[384   1   0   1   9   2   3  34]\n",
      " [ 15  45   0   5   3   2   5  22]\n",
      " [  6   3  47   0   5   3   6  28]\n",
      " [  8   3   1 100   3   0   0  22]\n",
      " [ 22   3   4   0 126   0   9  13]\n",
      " [  1   1   3   0   3  61   2  17]\n",
      " [  6   4   4   2   4   2 171  17]\n",
      " [ 34   5   8   9  10   0   7 939]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators、learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T02:42:57.251170Z",
     "start_time": "2018-07-31T02:42:57.234406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__n_estimators': range(10, 171, 30),\n",
       " 'classifier__learning_rate': [0.01, 0.1, 0.3]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(10,171,30), # 迭代次数/分类器个数\n",
    "                   classifier__learning_rate=[0.01, 0.1, 0.3]) \n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T03:45:52.070019Z",
     "start_time": "2018-07-31T02:43:08.512885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_n_estimators:  70\n",
      "score_1_learning_rate:  0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.76812, std: 0.00620, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 10},\n",
       "  mean: 0.78372, std: 0.01000, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 40},\n",
       "  mean: 0.79056, std: 0.01112, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 70},\n",
       "  mean: 0.79627, std: 0.01202, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 100},\n",
       "  mean: 0.79893, std: 0.00989, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 130},\n",
       "  mean: 0.80179, std: 0.00953, params: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 160},\n",
       "  mean: 0.79494, std: 0.00763, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 10},\n",
       "  mean: 0.81054, std: 0.00502, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 40},\n",
       "  mean: 0.81701, std: 0.00596, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 70},\n",
       "  mean: 0.81872, std: 0.00600, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 100},\n",
       "  mean: 0.81853, std: 0.00502, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 130},\n",
       "  mean: 0.81834, std: 0.00494, params: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 160},\n",
       "  mean: 0.80160, std: 0.01047, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 10},\n",
       "  mean: 0.81548, std: 0.00876, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 40},\n",
       "  mean: 0.81986, std: 0.00483, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 70},\n",
       "  mean: 0.81853, std: 0.00381, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 100},\n",
       "  mean: 0.81853, std: 0.00397, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 130},\n",
       "  mean: 0.81891, std: 0.00501, params: {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 160}],\n",
       " {'classifier__learning_rate': 0.3, 'classifier__n_estimators': 70},\n",
       " 0.8198592353053072)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "score_1_best_para['learning_rate'] = clf.best_params_['classifier__learning_rate']\n",
    "print('score_1_n_estimators: ', score_1_best_para['n_estimators'])\n",
    "print('score_1_learning_rate: ', score_1_best_para['learning_rate'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T03:48:15.404460Z",
     "start_time": "2018-07-31T03:45:52.072560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9984782195168347\n",
      "accuracy_score:  0.8354037267080745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.84      0.88      0.86       456\n",
      "          2       0.62      0.40      0.48       100\n",
      "          3       0.70      0.54      0.61        91\n",
      "          4       0.86      0.71      0.78       132\n",
      "          5       0.70      0.75      0.72       181\n",
      "          6       0.85      0.76      0.80        82\n",
      "          7       0.83      0.81      0.82       207\n",
      "          8       0.88      0.93      0.90      1005\n",
      "\n",
      "avg / total       0.83      0.84      0.83      2254\n",
      "\n",
      "confusion_matrix: \n",
      "[[401   8   2   2  14   1   1  27]\n",
      " [ 11  40   6   2   9   3   8  21]\n",
      " [  4   1  49   1   7   2   5  22]\n",
      " [  8   1   0  94   6   0   0  23]\n",
      " [ 15   2   3   3 135   1   8  14]\n",
      " [  0   1   1   0   3  62   5  10]\n",
      " [  6   4   2   0  12   2 167  14]\n",
      " [ 32   8   7   7   7   2   7 935]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_2), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_1 = dict(classifier__n_estimators = range(30,161,20), # 迭代次数/分类器个数\n",
    "                   classifier__learning_rate=[0.01, 0.1, 0.3]) \n",
    "param_grid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_1, \n",
    "                   cv = cv, scoring=score_2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_2_best_para['n_estimators'] = clf.best_params_['classifier__n_estimators']\n",
    "score_2_best_para['learning_rate'] = clf.best_params_['classifier__learning_rate']\n",
    "print('score_2_n_estimators: ', score_2_best_para['n_estimators'])\n",
    "print('score_2_learning_rate: ', score_2_best_para['learning_rate'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(max_depth = 7, gamma = 0, \n",
    "                                                  n_estimators = score_2_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_2_best_para['learning_rate'],                                                  \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_depth、min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T03:48:15.416309Z",
     "start_time": "2018-07-31T03:48:15.407349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': range(3, 14, 2),\n",
       " 'classifier__min_child_weight': [4, 5, 6]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],\n",
    "                                                  objective='multi:softmax', num_class=2))])\n",
    "\n",
    "param_grid_2 = dict(classifier__max_depth=range(3,14,2), # 决策树最大深度\n",
    "                    classifier__min_child_weight=[4, 5, 6]) \n",
    "param_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T04:29:38.276047Z",
     "start_time": "2018-07-31T03:48:15.419252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1_max_depth:  5\n",
      "score_1_min_child_weight:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.81720, std: 0.00696, params: {'classifier__max_depth': 3, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.81777, std: 0.00553, params: {'classifier__max_depth': 3, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.81663, std: 0.00853, params: {'classifier__max_depth': 3, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.82366, std: 0.00396, params: {'classifier__max_depth': 5, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.81929, std: 0.00435, params: {'classifier__max_depth': 5, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.81929, std: 0.00475, params: {'classifier__max_depth': 5, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.81758, std: 0.00233, params: {'classifier__max_depth': 7, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.82214, std: 0.00560, params: {'classifier__max_depth': 7, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.82043, std: 0.00555, params: {'classifier__max_depth': 7, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.81948, std: 0.00286, params: {'classifier__max_depth': 9, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.82043, std: 0.00527, params: {'classifier__max_depth': 9, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.82005, std: 0.00696, params: {'classifier__max_depth': 9, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.81910, std: 0.00300, params: {'classifier__max_depth': 11, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.81948, std: 0.00358, params: {'classifier__max_depth': 11, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.82233, std: 0.00537, params: {'classifier__max_depth': 11, 'classifier__min_child_weight': 6},\n",
       "  mean: 0.81853, std: 0.00183, params: {'classifier__max_depth': 13, 'classifier__min_child_weight': 4},\n",
       "  mean: 0.81891, std: 0.00562, params: {'classifier__max_depth': 13, 'classifier__min_child_weight': 5},\n",
       "  mean: 0.81872, std: 0.00638, params: {'classifier__max_depth': 13, 'classifier__min_child_weight': 6}],\n",
       " {'classifier__max_depth': 5, 'classifier__min_child_weight': 4},\n",
       " 0.8236636865132204)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, param_grid=param_grid_2, \n",
    "                   cv = cv, scoring=score_1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "score_1_best_para['max_depth'] = clf.best_params_['classifier__max_depth']\n",
    "score_1_best_para['min_child_weight'] = clf.best_params_['classifier__min_child_weight']\n",
    "print('score_1_max_depth: ', score_1_best_para['max_depth'])\n",
    "print('score_1_min_child_weight: ', score_1_best_para['min_child_weight'])\n",
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T06:21:37.720299Z",
     "start_time": "2018-07-31T06:19:45.340103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9975271067148563\n",
      "accuracy_score:  0.8345164152617569\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.85      0.88      0.86       456\n",
      "          2       0.56      0.38      0.45       100\n",
      "          3       0.66      0.54      0.59        91\n",
      "          4       0.86      0.73      0.79       132\n",
      "          5       0.73      0.76      0.75       181\n",
      "          6       0.79      0.73      0.76        82\n",
      "          7       0.82      0.80      0.81       207\n",
      "          8       0.88      0.93      0.90      1005\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2254\n",
      "\n",
      "confusion_matrix: \n",
      "[[400   7   4   2  13   1   2  27]\n",
      " [ 15  38   4   2   9   2   7  23]\n",
      " [  4   2  49   1   7   2   6  20]\n",
      " [  7   3   0  96   4   0   0  22]\n",
      " [ 13   5   3   1 138   2   7  12]\n",
      " [  0   2   2   0   2  60   6  10]\n",
      " [  6   3   4   0  11   4 165  14]\n",
      " [ 27   8   8  10   5   5   7 935]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('features', combined_features_1), \n",
    "                     ('classifier', XGBClassifier(gamma = 0, \n",
    "                                                  n_estimators = score_1_best_para['n_estimators'],\n",
    "                                                  learning_rate = score_1_best_para['learning_rate'],        \n",
    "                                                  max_depth = score_1_best_para['max_depth'],\n",
    "                                                  min_child_weight = score_1_best_para['min_child_weight'],                                                   \n",
    "                                                  objective='multi:softmax', num_class=8))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.score(X_train, y_train))\n",
    "y_pred_class = pipeline.predict(X_test)\n",
    "print('accuracy_score: ', metrics.accuracy_score(y_test, y_pred_class)) # 指所有分类正确的百分比\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print('confusion_matrix: ')\n",
    "print( metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T09:51:12.095774Z",
     "start_time": "2018-06-13T09:51:11.751754Z"
    },
    "collapsed": true
   },
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T07:12:43.681129Z",
     "start_time": "2018-07-31T07:12:38.472043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/circ_8classifier_0730_1.pkl.z']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"model/circ_8classifier_0730_1.pkl.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T06:53:13.560161Z",
     "start_time": "2018-07-03T06:53:13.537160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    import datetime as dt\n",
    "    \n",
    "    def output_HTML(read_file, output_file):\n",
    "        from nbconvert import HTMLExporter\n",
    "        import codecs\n",
    "        import nbformat\n",
    "        exporter = HTMLExporter()\n",
    "        # read_file is '.ipynb', output_file is '.html'\n",
    "        output_notebook = nbformat.read(read_file, as_version=4)\n",
    "        output, resources = exporter.from_notebook_node(output_notebook)\n",
    "        codecs.open(output_file, 'w', encoding='utf-8').write(output)\n",
    "\n",
    "    html_file_folder = 'html_files'\n",
    "    if not os.path.exists(html_file_folder):\n",
    "        os.makedirs(html_file_folder)\n",
    "\n",
    "    today = dt.datetime.now().strftime('%Y%m%d')\n",
    "    current_file = 'circ_cor_model_2_train.ipynb'\n",
    "    output_file = 'html_files\\%s_%s.html'%(os.path.splitext(current_file)[0], today)\n",
    "    output_HTML(current_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
